{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"","title":"Home"},{"location":"community/get-involved","text":"Welcome to the AWS Copilot Community \u2764\ufe0f This is a place to share your [ applications, articles, videos ] and any other resources related to the AWS Copilot CLI \ud83d\udc69\u200d\u2708\ufe0f. You can get involved with the CLI by: Creating issues in the GitHub repository Joining the chat with fellow Copilots","title":"Get Involved"},{"location":"community/get-involved#welcome-to-the-aws-copilot-community","text":"This is a place to share your [ applications, articles, videos ] and any other resources related to the AWS Copilot CLI \ud83d\udc69\u200d\u2708\ufe0f. You can get involved with the CLI by: Creating issues in the GitHub repository Joining the chat with fellow Copilots","title":"Welcome to the AWS Copilot Community \u2764\ufe0f"},{"location":"community/guides","text":"Share your applications, videos, and blog posts with fellow Copilots! Blog posts Title Description Pilot your containers like a boss with AWS Copilot! by @FlolightC Florian simplifies getting started with AWS Copilot by demoing deployment of a sample app from start to finish, including links to both required and optional resources. Use AWS Copilot CLI to deploy containers on an existing infrastructure - Tutorial by @dannysteenman Danny explains how to use your existing VPC and subnets with AWS Copilot to quickly set up a working container environment. Follow along as he deploys a Django app with an RDS Postgres database and Elasticache Redis cluster. Automatically deploying your container application with AWS Copilot by @nathankpeck Nathan shows how to set up a release pipeline with the CLI that builds, pushes, and deploys an application. Finally, he sets up integration tests for validation before releasing to production. Deploying containers with the AWS Copilot CLI by @maartenbruntink Maarten shows how to use the AWS Copilot CLI to deploy the sample Docker voting app , which showcases how to set up your own Redis and Postgres servers. In the second part , he automates the release process. AWS Copilot: an application-first CLI for containers on AWS by @efekarakus Efe walks through the design tenets of the CLI: why they were chosen, how they map to Copilot features, and the vision for how the CLI will evolve in the future. Introducing AWS Copilot by @nathankpeck Nathan explains how with the AWS Copilot CLI you can go from idea to implementation much faster, with the confidence that the infrastructure you have deployed has production-ready configuration. Videos Title Description Containers from the Couch series by @realadamjkeller , @brentContained , and guests Join Adam and Brent to learn about all the existing features of AWS Copilot with fun demos. From setting up a three-tier application with autoscaling to creating a continuous delivery pipeline with integration tests. AWS re:Invent 2020: AWS Copilot: Simplifying container development by @efekarakus Learn about the motivation behind AWS Copilot, get an overview of the existing commands and a demo of how to deploy a three-tier application. How to Deploy a .NET Application to Amazon Elastic Container Service (ECS) with AWS Copilot by @ignacioafuentes Get a demo on how to build and deploy a .NET application on Amazon ECS and AWS Fargate. AWS What's Next by @nathankpeck and @efekarakus Nathan and Efe discuss what makes AWS Copilot unique compared to other infrastructure provisioning tools and then demo an overview of the existing commands. Code samples Repository Description Key features github.com/copilot-example-voting-app A voting application distributed over three ECS services created with AWS Copilot. Amazon Aurora PostgreSQL database, service discovery, autoscaling #1925 Show and tell explaining how you can do continuous deployments from branches with AWS Copilot pipelines. Branch-based deploys, AWS CodePipeline Workshops Title Description ECS Workshop In this workshop, we deploy a three tier microservices application using the copilot-cli","title":"Guides and resources"},{"location":"community/guides#blog-posts","text":"Title Description Pilot your containers like a boss with AWS Copilot! by @FlolightC Florian simplifies getting started with AWS Copilot by demoing deployment of a sample app from start to finish, including links to both required and optional resources. Use AWS Copilot CLI to deploy containers on an existing infrastructure - Tutorial by @dannysteenman Danny explains how to use your existing VPC and subnets with AWS Copilot to quickly set up a working container environment. Follow along as he deploys a Django app with an RDS Postgres database and Elasticache Redis cluster. Automatically deploying your container application with AWS Copilot by @nathankpeck Nathan shows how to set up a release pipeline with the CLI that builds, pushes, and deploys an application. Finally, he sets up integration tests for validation before releasing to production. Deploying containers with the AWS Copilot CLI by @maartenbruntink Maarten shows how to use the AWS Copilot CLI to deploy the sample Docker voting app , which showcases how to set up your own Redis and Postgres servers. In the second part , he automates the release process. AWS Copilot: an application-first CLI for containers on AWS by @efekarakus Efe walks through the design tenets of the CLI: why they were chosen, how they map to Copilot features, and the vision for how the CLI will evolve in the future. Introducing AWS Copilot by @nathankpeck Nathan explains how with the AWS Copilot CLI you can go from idea to implementation much faster, with the confidence that the infrastructure you have deployed has production-ready configuration.","title":"Blog posts"},{"location":"community/guides#videos","text":"Title Description Containers from the Couch series by @realadamjkeller , @brentContained , and guests Join Adam and Brent to learn about all the existing features of AWS Copilot with fun demos. From setting up a three-tier application with autoscaling to creating a continuous delivery pipeline with integration tests. AWS re:Invent 2020: AWS Copilot: Simplifying container development by @efekarakus Learn about the motivation behind AWS Copilot, get an overview of the existing commands and a demo of how to deploy a three-tier application. How to Deploy a .NET Application to Amazon Elastic Container Service (ECS) with AWS Copilot by @ignacioafuentes Get a demo on how to build and deploy a .NET application on Amazon ECS and AWS Fargate. AWS What's Next by @nathankpeck and @efekarakus Nathan and Efe discuss what makes AWS Copilot unique compared to other infrastructure provisioning tools and then demo an overview of the existing commands.","title":"Videos"},{"location":"community/guides#code-samples","text":"Repository Description Key features github.com/copilot-example-voting-app A voting application distributed over three ECS services created with AWS Copilot. Amazon Aurora PostgreSQL database, service discovery, autoscaling #1925 Show and tell explaining how you can do continuous deployments from branches with AWS Copilot pipelines. Branch-based deploys, AWS CodePipeline","title":"Code samples"},{"location":"community/guides#workshops","text":"Title Description ECS Workshop In this workshop, we deploy a three tier microservices application using the copilot-cli","title":"Workshops"},{"location":"docs/credentials","text":"This section explains our recommendations around credentials to provide the best experience with the AWS Copilot CLI. Application credentials Copilot uses the AWS credentials from the default credential provider chain to store and look up your application's metadata : which services and environments belong to it. Tip We recommend using a named profile to store your application's credentials. The most convenient way is having the [default] profile point to your application's credentials: # ~/.aws/credentials [default] aws_access_key_id = AKIAIOSFODNN7EXAMPLE aws_secret_access_key = wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY # ~/.aws/config [default] region = us-west-2 Alternatively, you can set the AWS_PROFILE environment variable to point to a different named profile. For example, we can have a [my-app] profile that can be used for your Copilot application instead of the [default] profile. Note You cannot use the AWS account root user credentials for your application. Please first create an IAM user instead as described here . # ~/.aws/config [my-app] credential_process = /opt/bin/awscreds-custom --username helen region = us-west-2 # Then you can run your Copilot commands leveraging the alternative profile: $ export AWS_PROFILE = my-app $ copilot deploy Caution We do not recommend using the environment variables: AWS_ACCESS_KEY_ID , AWS_SECRET_ACCESS_KEY , AWS_SESSION_TOKEN directly to look up your application's metadata because if they're overridden or expired, Copilot will not be able to look up your services or environments. To learn more about all the supported config file settings: Configuration and credential file settings . Environment credentials Copilot environments can be created in AWS accounts and regions separate from your application's. While initializing an environment, Copilot will prompt you to enter temporary credentials or a named profile to create your environment: $ copilot env init Name: prod-iad Which credentials would you like to use to create prod-iad? > Enter temporary credentials > [ profile default ] > [ profile test ] > [ profile prod-iad ] > [ profile prod-pdx ] Unlike the Application credentials , the AWS credentials for an environment are only needed for creation or deletion. Therefore, it's safe to use the values from temporary environment variables. Copilot prompts or takes the credentials as flags because the default chain is reserved for your application credentials.","title":"Credentials"},{"location":"docs/credentials#application-credentials","text":"Copilot uses the AWS credentials from the default credential provider chain to store and look up your application's metadata : which services and environments belong to it. Tip We recommend using a named profile to store your application's credentials. The most convenient way is having the [default] profile point to your application's credentials: # ~/.aws/credentials [default] aws_access_key_id = AKIAIOSFODNN7EXAMPLE aws_secret_access_key = wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY # ~/.aws/config [default] region = us-west-2 Alternatively, you can set the AWS_PROFILE environment variable to point to a different named profile. For example, we can have a [my-app] profile that can be used for your Copilot application instead of the [default] profile. Note You cannot use the AWS account root user credentials for your application. Please first create an IAM user instead as described here . # ~/.aws/config [my-app] credential_process = /opt/bin/awscreds-custom --username helen region = us-west-2 # Then you can run your Copilot commands leveraging the alternative profile: $ export AWS_PROFILE = my-app $ copilot deploy Caution We do not recommend using the environment variables: AWS_ACCESS_KEY_ID , AWS_SECRET_ACCESS_KEY , AWS_SESSION_TOKEN directly to look up your application's metadata because if they're overridden or expired, Copilot will not be able to look up your services or environments. To learn more about all the supported config file settings: Configuration and credential file settings .","title":"Application credentials"},{"location":"docs/credentials#environment-credentials","text":"Copilot environments can be created in AWS accounts and regions separate from your application's. While initializing an environment, Copilot will prompt you to enter temporary credentials or a named profile to create your environment: $ copilot env init Name: prod-iad Which credentials would you like to use to create prod-iad? > Enter temporary credentials > [ profile default ] > [ profile test ] > [ profile prod-iad ] > [ profile prod-pdx ] Unlike the Application credentials , the AWS credentials for an environment are only needed for creation or deletion. Therefore, it's safe to use the values from temporary environment variables. Copilot prompts or takes the credentials as flags because the default chain is reserved for your application credentials.","title":"Environment credentials"},{"location":"docs/overview","text":"Welcome to the AWS Copilot CLI \ud83c\udf89 The Copilot CLI is a tool for developers to build, release, and operate production-ready containerized applications on Amazon ECS and AWS Fargate. From getting started, pushing to staging, and releasing to production, Copilot can help manage the entire lifecycle of your application development. Installing You can install AWS Copilot through Homebrew or by downloading the binaries directly. If you don't want to use Homebrew, you can install manually . $ brew install aws/tap/copilot-cli","title":"Overview"},{"location":"docs/overview#installing","text":"You can install AWS Copilot through Homebrew or by downloading the binaries directly. If you don't want to use Homebrew, you can install manually . $ brew install aws/tap/copilot-cli","title":"Installing"},{"location":"docs/commands/app-delete","text":"app delete $ copilot app delete [ flags ] What does it do? copilot app delete deletes all resources associated with an application. What are the flags? -h, --help help for delete --yes Skips confirmation prompt. Examples Force delete the application. $ copilot app delete --yes","title":"app delete"},{"location":"docs/commands/app-delete#app-delete","text":"$ copilot app delete [ flags ]","title":"app delete"},{"location":"docs/commands/app-delete#what-does-it-do","text":"copilot app delete deletes all resources associated with an application.","title":"What does it do?"},{"location":"docs/commands/app-delete#what-are-the-flags","text":"-h, --help help for delete --yes Skips confirmation prompt.","title":"What are the flags?"},{"location":"docs/commands/app-delete#examples","text":"Force delete the application. $ copilot app delete --yes","title":"Examples"},{"location":"docs/commands/app-init","text":"app init $ copilot app init [ name ] [ flags ] What does it do? copilot app init creates a new application within the directory that will contain your service(s). After you answer the questions, the CLI creates AWS Identity and Access Management roles to manage the release infrastructure for your services. You'll also see a new sub-directory created under your working directory: copilot/ . The copilot directory will hold the manifest files and additional infrastructure for your services. Typically, you don't need to run app init ( init does all the same work) unless you want to use a custom domain name or AWS tags. What are the flags? Like all commands in the Copilot CLI, if you don't provide required flags, we'll prompt you for all the information we need to get you going. You can skip the prompts by providing information via flags: --domain string Optional. Your existing custom domain name. -h, --help help for init --resource-tags stringToString Optional. Labels with a key and value separated by commas. Allows you to categorize resources. ( default []) The --domain flag allows you to specify a domain name registered with Amazon Route 53 in your app's account. This will allow all the services in your app to share the same domain name. You'll be able to access your services at: https://{svcName}.{envName}.{appName}.{domain} The --resource-tags flags allows you to add your custom tags to all the resources in your app. For example: copilot app init --resource-tags department=MyDept,team=MyTeam Examples Create a new application named \"my-app\". $ copilot app init my-app Create a new application with an existing domain name in Amazon Route53. $ copilot app init --domain example.com Create a new application with resource tags. $ copilot app init --resource-tags department = MyDept,team = MyTeam What does it look like?","title":"app init"},{"location":"docs/commands/app-init#app-init","text":"$ copilot app init [ name ] [ flags ]","title":"app init"},{"location":"docs/commands/app-init#what-does-it-do","text":"copilot app init creates a new application within the directory that will contain your service(s). After you answer the questions, the CLI creates AWS Identity and Access Management roles to manage the release infrastructure for your services. You'll also see a new sub-directory created under your working directory: copilot/ . The copilot directory will hold the manifest files and additional infrastructure for your services. Typically, you don't need to run app init ( init does all the same work) unless you want to use a custom domain name or AWS tags.","title":"What does it do?"},{"location":"docs/commands/app-init#what-are-the-flags","text":"Like all commands in the Copilot CLI, if you don't provide required flags, we'll prompt you for all the information we need to get you going. You can skip the prompts by providing information via flags: --domain string Optional. Your existing custom domain name. -h, --help help for init --resource-tags stringToString Optional. Labels with a key and value separated by commas. Allows you to categorize resources. ( default []) The --domain flag allows you to specify a domain name registered with Amazon Route 53 in your app's account. This will allow all the services in your app to share the same domain name. You'll be able to access your services at: https://{svcName}.{envName}.{appName}.{domain} The --resource-tags flags allows you to add your custom tags to all the resources in your app. For example: copilot app init --resource-tags department=MyDept,team=MyTeam","title":"What are the flags?"},{"location":"docs/commands/app-init#examples","text":"Create a new application named \"my-app\". $ copilot app init my-app Create a new application with an existing domain name in Amazon Route53. $ copilot app init --domain example.com Create a new application with resource tags. $ copilot app init --resource-tags department = MyDept,team = MyTeam","title":"Examples"},{"location":"docs/commands/app-init#what-does-it-look-like","text":"","title":"What does it look like?"},{"location":"docs/commands/app-ls","text":"app ls $ copilot app ls [ flags ] What does it do? copilot app ls lists all the Copilot applications in your account. What are the flags? -h, --help help for ls Examples List all the applications in your account and region. $ copilot app ls What does it look like?","title":"app ls"},{"location":"docs/commands/app-ls#app-ls","text":"$ copilot app ls [ flags ]","title":"app ls"},{"location":"docs/commands/app-ls#what-does-it-do","text":"copilot app ls lists all the Copilot applications in your account.","title":"What does it do?"},{"location":"docs/commands/app-ls#what-are-the-flags","text":"-h, --help help for ls","title":"What are the flags?"},{"location":"docs/commands/app-ls#examples","text":"List all the applications in your account and region. $ copilot app ls","title":"Examples"},{"location":"docs/commands/app-ls#what-does-it-look-like","text":"","title":"What does it look like?"},{"location":"docs/commands/app-show","text":"app show $ copilot app show [ flags ] What does it do? copilot app show shows configuration, environments and services for an application. What are the flags? -h, --help help for show --json Optional. Outputs in JSON format. -n, --name string Name of the application. Examples Shows info about the application \"my-app\". $ copilot app show -n my-app What does it look like?","title":"app show"},{"location":"docs/commands/app-show#app-show","text":"$ copilot app show [ flags ]","title":"app show"},{"location":"docs/commands/app-show#what-does-it-do","text":"copilot app show shows configuration, environments and services for an application.","title":"What does it do?"},{"location":"docs/commands/app-show#what-are-the-flags","text":"-h, --help help for show --json Optional. Outputs in JSON format. -n, --name string Name of the application.","title":"What are the flags?"},{"location":"docs/commands/app-show#examples","text":"Shows info about the application \"my-app\". $ copilot app show -n my-app","title":"Examples"},{"location":"docs/commands/app-show#what-does-it-look-like","text":"","title":"What does it look like?"},{"location":"docs/commands/completion","text":"completion $ copilot completion [shell] [flags] What does it do? copilot completion prints shell completion code for bash or zsh. The code must be evaluated to provide interactive completion of commands. See the help menu for instructions on how to setup auto-completion for your respective shell. What are the flags? -h, --help help for completion Examples Install zsh completion. $ source < ( copilot completion zsh ) $ copilot completion zsh > \" ${ fpath [1] } /_copilot\" # to autoload on startup Install bash completion on macOS using homebrew. $ brew install bash-completion # if running 3.2 $ brew install bash-completion@2 # if running Bash 4.1+ $ copilot completion bash > /usr/local/etc/bash_completion.d Install bash completion on linux $ source < ( copilot completion bash ) $ copilot completion bash > copilot.sh $ sudo mv copilot.sh /etc/bash_completion.d/copilot","title":"completion"},{"location":"docs/commands/completion#completion","text":"$ copilot completion [shell] [flags]","title":"completion"},{"location":"docs/commands/completion#what-does-it-do","text":"copilot completion prints shell completion code for bash or zsh. The code must be evaluated to provide interactive completion of commands. See the help menu for instructions on how to setup auto-completion for your respective shell.","title":"What does it do?"},{"location":"docs/commands/completion#what-are-the-flags","text":"-h, --help help for completion","title":"What are the flags?"},{"location":"docs/commands/completion#examples","text":"Install zsh completion. $ source < ( copilot completion zsh ) $ copilot completion zsh > \" ${ fpath [1] } /_copilot\" # to autoload on startup Install bash completion on macOS using homebrew. $ brew install bash-completion # if running 3.2 $ brew install bash-completion@2 # if running Bash 4.1+ $ copilot completion bash > /usr/local/etc/bash_completion.d Install bash completion on linux $ source < ( copilot completion bash ) $ copilot completion bash > copilot.sh $ sudo mv copilot.sh /etc/bash_completion.d/copilot","title":"Examples"},{"location":"docs/commands/deploy","text":"copilot deploy $ copilot deploy What does it do? This command is used to run either copilot svc deploy or copilot job deploy under the hood. The steps involved in copilot deploy are the same as those involved in copilot svc deploy and copilot job deploy : Build your local Dockerfile into an image Tag it with the value from --tag or the latest git sha (if you're in a git directory) Push the image to ECR Package your manifest file and addons into CloudFormation Create / update your ECS task definition and job or service. What are the flags? -a, --app string Name of the application. -e, --env string Name of the environment. -h, --help help for deploy -n, --name string Name of the service or job. --resource-tags stringToString Optional. Labels with a key and value separated by commas. Allows you to categorize resources. ( default []) --tag string Optional. The container image tag. Examples Deploys a service named \"frontend\" to a \"test\" environment. $ copilot deploy --name frontend --env test Deploys a job named \"mailer\" with additional resource tags to a \"prod\" environment. $ copilot deploy -n mailer -e prod --resource-tags source/revision = bb133e7,deployment/initiator = manual","title":"deploy"},{"location":"docs/commands/deploy#copilot-deploy","text":"$ copilot deploy","title":"copilot deploy"},{"location":"docs/commands/deploy#what-does-it-do","text":"This command is used to run either copilot svc deploy or copilot job deploy under the hood. The steps involved in copilot deploy are the same as those involved in copilot svc deploy and copilot job deploy : Build your local Dockerfile into an image Tag it with the value from --tag or the latest git sha (if you're in a git directory) Push the image to ECR Package your manifest file and addons into CloudFormation Create / update your ECS task definition and job or service.","title":"What does it do?"},{"location":"docs/commands/deploy#what-are-the-flags","text":"-a, --app string Name of the application. -e, --env string Name of the environment. -h, --help help for deploy -n, --name string Name of the service or job. --resource-tags stringToString Optional. Labels with a key and value separated by commas. Allows you to categorize resources. ( default []) --tag string Optional. The container image tag.","title":"What are the flags?"},{"location":"docs/commands/deploy#examples","text":"Deploys a service named \"frontend\" to a \"test\" environment. $ copilot deploy --name frontend --env test Deploys a job named \"mailer\" with additional resource tags to a \"prod\" environment. $ copilot deploy -n mailer -e prod --resource-tags source/revision = bb133e7,deployment/initiator = manual","title":"Examples"},{"location":"docs/commands/docs","text":"docs $ copilot docs [ flags ] What does it do? copilot docs open the copilot docs in your browser. What are the flags? -h, --help help for docs","title":"docs"},{"location":"docs/commands/docs#docs","text":"$ copilot docs [ flags ]","title":"docs"},{"location":"docs/commands/docs#what-does-it-do","text":"copilot docs open the copilot docs in your browser.","title":"What does it do?"},{"location":"docs/commands/docs#what-are-the-flags","text":"-h, --help help for docs","title":"What are the flags?"},{"location":"docs/commands/env-delete","text":"env delete $ copilot env delete [ flags ] What does it do? copilot env delete deletes an environment from your application. If there are running applications in your environment, you need to first run copilot svc delete . After you answer the questions, you should see that the AWS CloudFormation stack for your environment has been deleted. What are the flags? -h, --help help for delete -n, --name string Name of the environment. --yes Skips confirmation prompt. -a, --app string Name of the application. Examples Delete the \"test\" environment. $ copilot env delete --name test Delete the \"test\" environment without prompting. $ copilot env delete --name test --yes","title":"env delete"},{"location":"docs/commands/env-delete#env-delete","text":"$ copilot env delete [ flags ]","title":"env delete"},{"location":"docs/commands/env-delete#what-does-it-do","text":"copilot env delete deletes an environment from your application. If there are running applications in your environment, you need to first run copilot svc delete . After you answer the questions, you should see that the AWS CloudFormation stack for your environment has been deleted.","title":"What does it do?"},{"location":"docs/commands/env-delete#what-are-the-flags","text":"-h, --help help for delete -n, --name string Name of the environment. --yes Skips confirmation prompt. -a, --app string Name of the application.","title":"What are the flags?"},{"location":"docs/commands/env-delete#examples","text":"Delete the \"test\" environment. $ copilot env delete --name test Delete the \"test\" environment without prompting. $ copilot env delete --name test --yes","title":"Examples"},{"location":"docs/commands/env-init","text":"env init $ copilot env init [ flags ] What does it do? copilot env init creates a new environment where your services will live. After you answer the questions, the CLI creates the common infrastructure that's shared between your services such as a VPC, an Application Load Balancer, and an ECS Cluster. Additionally, you can customize your Copilot environment by either configuring the default environment resources or importing existing resources for your environment. You create environments using a named profile to specify which AWS account and region you'd like the environment to be in. What are the flags? Like all commands in the AWS Copilot CLI, if you don't provide required flags, we'll prompt you for all the information we need to get you going. You can skip the prompts by providing information via flags: Common Flags --aws-access-key-id string Optional. An AWS access key. --aws-secret-access-key string Optional. An AWS secret access key. --aws-session-token string Optional. An AWS session token for temporary credentials. --default-config Optional. Skip prompting and use default environment configuration. -n, --name string Name of the environment. --prod If the environment contains production services. --profile string Name of the profile. --region string Optional. An AWS region where the environment will be created. Import Existing Resources Flags --import-private-subnets strings Optional. Use existing private subnet IDs. --import-public-subnets strings Optional. Use existing public subnet IDs. --import-vpc-id string Optional. Use an existing VPC ID. Configure Default Resources Flags --override-private-cidrs strings Optional. CIDR to use for private subnets (default 10.0.2.0/24,10.0.3.0/24). --override-public-cidrs strings Optional. CIDR to use for public subnets (default 10.0.0.0/24,10.0.1.0/24). --override-vpc-cidr ipNet Optional. Global CIDR to use for VPC (default 10.0.0.0/16). Global Flags -a, --app string Name of the application. Examples Creates a test environment in your \"default\" AWS profile using default config. $ copilot env init --name test --profile default --default-config Creates a prod-iad environment using your \"prod-admin\" AWS profile using existing VPC. $ copilot env init --name prod-iad --profile prod-admin --prod \\ --import-vpc-id vpc-099c32d2b98cdcf47 \\ --import-public-subnets subnet-013e8b691862966cf,subnet-014661ebb7ab8681a \\ --import-private-subnets subnet-055fafef48fb3c547,subnet-00c9e76f288363e7f What does it look like?","title":"env init"},{"location":"docs/commands/env-init#env-init","text":"$ copilot env init [ flags ]","title":"env init"},{"location":"docs/commands/env-init#what-does-it-do","text":"copilot env init creates a new environment where your services will live. After you answer the questions, the CLI creates the common infrastructure that's shared between your services such as a VPC, an Application Load Balancer, and an ECS Cluster. Additionally, you can customize your Copilot environment by either configuring the default environment resources or importing existing resources for your environment. You create environments using a named profile to specify which AWS account and region you'd like the environment to be in.","title":"What does it do?"},{"location":"docs/commands/env-init#what-are-the-flags","text":"Like all commands in the AWS Copilot CLI, if you don't provide required flags, we'll prompt you for all the information we need to get you going. You can skip the prompts by providing information via flags: Common Flags --aws-access-key-id string Optional. An AWS access key. --aws-secret-access-key string Optional. An AWS secret access key. --aws-session-token string Optional. An AWS session token for temporary credentials. --default-config Optional. Skip prompting and use default environment configuration. -n, --name string Name of the environment. --prod If the environment contains production services. --profile string Name of the profile. --region string Optional. An AWS region where the environment will be created. Import Existing Resources Flags --import-private-subnets strings Optional. Use existing private subnet IDs. --import-public-subnets strings Optional. Use existing public subnet IDs. --import-vpc-id string Optional. Use an existing VPC ID. Configure Default Resources Flags --override-private-cidrs strings Optional. CIDR to use for private subnets (default 10.0.2.0/24,10.0.3.0/24). --override-public-cidrs strings Optional. CIDR to use for public subnets (default 10.0.0.0/24,10.0.1.0/24). --override-vpc-cidr ipNet Optional. Global CIDR to use for VPC (default 10.0.0.0/16). Global Flags -a, --app string Name of the application.","title":"What are the flags?"},{"location":"docs/commands/env-init#examples","text":"Creates a test environment in your \"default\" AWS profile using default config. $ copilot env init --name test --profile default --default-config Creates a prod-iad environment using your \"prod-admin\" AWS profile using existing VPC. $ copilot env init --name prod-iad --profile prod-admin --prod \\ --import-vpc-id vpc-099c32d2b98cdcf47 \\ --import-public-subnets subnet-013e8b691862966cf,subnet-014661ebb7ab8681a \\ --import-private-subnets subnet-055fafef48fb3c547,subnet-00c9e76f288363e7f","title":"Examples"},{"location":"docs/commands/env-init#what-does-it-look-like","text":"","title":"What does it look like?"},{"location":"docs/commands/env-ls","text":"env ls $ copilot env ls [ flags ] What does it do? copilot env ls lists all the environments in your application. What are the flags? -h, --help help for ls --json Optional. Outputs in JSON format. -a, --app string Name of the application. You can use the --json flag if you'd like to programmatically parse the results. Examples Lists all the environments for the frontend application. $ copilot env ls -a frontend What does it look like?","title":"env ls"},{"location":"docs/commands/env-ls#env-ls","text":"$ copilot env ls [ flags ]","title":"env ls"},{"location":"docs/commands/env-ls#what-does-it-do","text":"copilot env ls lists all the environments in your application.","title":"What does it do?"},{"location":"docs/commands/env-ls#what-are-the-flags","text":"-h, --help help for ls --json Optional. Outputs in JSON format. -a, --app string Name of the application. You can use the --json flag if you'd like to programmatically parse the results.","title":"What are the flags?"},{"location":"docs/commands/env-ls#examples","text":"Lists all the environments for the frontend application. $ copilot env ls -a frontend","title":"Examples"},{"location":"docs/commands/env-ls#what-does-it-look-like","text":"","title":"What does it look like?"},{"location":"docs/commands/env-show","text":"env show $ copilot env show [ flags ] What does it do? copilot env show shows displays information about a particular environment, including: The region and account the environment is in Whether or not the environment is production The services currently deployed in the environment The tags associated with that environment You can optionally pass in a --resources flag which will include the AWS resources associated specifically with the environment. What are the flags? -h, --help help for show --json Optional. Outputs in JSON format. -n, --name string Name of the environment. --resources Optional. Show the resources in your environment. You can use the --json flag if you'd like to programmatically parse the results. Examples Shows info about the environment \"test\". $ copilot env show -n test","title":"env show"},{"location":"docs/commands/env-show#env-show","text":"$ copilot env show [ flags ]","title":"env show"},{"location":"docs/commands/env-show#what-does-it-do","text":"copilot env show shows displays information about a particular environment, including: The region and account the environment is in Whether or not the environment is production The services currently deployed in the environment The tags associated with that environment You can optionally pass in a --resources flag which will include the AWS resources associated specifically with the environment.","title":"What does it do?"},{"location":"docs/commands/env-show#what-are-the-flags","text":"-h, --help help for show --json Optional. Outputs in JSON format. -n, --name string Name of the environment. --resources Optional. Show the resources in your environment. You can use the --json flag if you'd like to programmatically parse the results.","title":"What are the flags?"},{"location":"docs/commands/env-show#examples","text":"Shows info about the environment \"test\". $ copilot env show -n test","title":"Examples"},{"location":"docs/commands/init","text":"init $ copilot init What does it do? copilot init is your starting point if you want to deploy your container app on Amazon ECS. Run it within a directory with your Dockerfile, and init will ask you questions about your application so we can get it up and running quickly. After you answer all the questions, copilot init will set up an ECR repository for you and ask you if you'd like to deploy. If you opt to deploy, it'll create a new test environment (complete with a networking stack and roles), build your Dockerfile, push it to Amazon ECR, and deploy your service or job. If you have an existing app, and want to add another service or job to that app, you can run copilot init - and you'll be prompted to select an existing app to add your service or job to. What are the flags? Like all commands in the Copilot CLI, if you don't provide required flags, we'll prompt you for all the information we need to get you going. You can skip the prompts by providing information via flags: -a, --app string Name of the application. --deploy Deploy your service or job to a \"test\" environment. -d, --dockerfile string Path to the Dockerfile. Mutually exclusive with -i, --image -h, --help help for init -i, --image string The location of an existing Docker image. Mutually exclusive with -d, --dockerfile -n, --name string Name of the service or job. --port uint16 Optional. The port on which your service listens. --retries int Optional. The number of times to try restarting the job on a failure. --schedule string The schedule on which to run this job. Accepts cron expressions of the format ( M H DoM M DoW ) and schedule definition strings. For example: \"0 * * * *\" , \"@daily\" , \"@weekly\" , \"@every 1h30m\" . AWS Schedule Expressions of the form \"rate(10 minutes)\" or \"cron(0 12 L * ? 2021)\" are also accepted. --tag string Optional. The container image tag. --timeout string Optional. The total execution time for the task, including retries. Accepts valid Go duration strings. For example: \"2h\" , \"1h30m\" , \"900s\" . -t, --type string Type of service to create. Must be one of: \"Request-Driven Web Service\" , \"Load Balanced Web Service\" , \"Backend Service\" , \"Scheduled Job\"","title":"init"},{"location":"docs/commands/init#init","text":"$ copilot init","title":"init"},{"location":"docs/commands/init#what-does-it-do","text":"copilot init is your starting point if you want to deploy your container app on Amazon ECS. Run it within a directory with your Dockerfile, and init will ask you questions about your application so we can get it up and running quickly. After you answer all the questions, copilot init will set up an ECR repository for you and ask you if you'd like to deploy. If you opt to deploy, it'll create a new test environment (complete with a networking stack and roles), build your Dockerfile, push it to Amazon ECR, and deploy your service or job. If you have an existing app, and want to add another service or job to that app, you can run copilot init - and you'll be prompted to select an existing app to add your service or job to.","title":"What does it do?"},{"location":"docs/commands/init#what-are-the-flags","text":"Like all commands in the Copilot CLI, if you don't provide required flags, we'll prompt you for all the information we need to get you going. You can skip the prompts by providing information via flags: -a, --app string Name of the application. --deploy Deploy your service or job to a \"test\" environment. -d, --dockerfile string Path to the Dockerfile. Mutually exclusive with -i, --image -h, --help help for init -i, --image string The location of an existing Docker image. Mutually exclusive with -d, --dockerfile -n, --name string Name of the service or job. --port uint16 Optional. The port on which your service listens. --retries int Optional. The number of times to try restarting the job on a failure. --schedule string The schedule on which to run this job. Accepts cron expressions of the format ( M H DoM M DoW ) and schedule definition strings. For example: \"0 * * * *\" , \"@daily\" , \"@weekly\" , \"@every 1h30m\" . AWS Schedule Expressions of the form \"rate(10 minutes)\" or \"cron(0 12 L * ? 2021)\" are also accepted. --tag string Optional. The container image tag. --timeout string Optional. The total execution time for the task, including retries. Accepts valid Go duration strings. For example: \"2h\" , \"1h30m\" , \"900s\" . -t, --type string Type of service to create. Must be one of: \"Request-Driven Web Service\" , \"Load Balanced Web Service\" , \"Backend Service\" , \"Scheduled Job\"","title":"What are the flags?"},{"location":"docs/commands/job-delete","text":"job delete $ copilot job delete [ flags ] What does it do? copilot job delete deletes all resources associated with your job in a particular environment. What are the flags? -a, --app string Name of the application. -e, --env string Name of the environment. -h, --help help for delete -n, --name string Name of the job. --yes Skips confirmation prompt. Examples Delete the \"report-generator\" job from the my-app application. $ copilot job delete --name report-generator --app my-app Delete the \"report-generator\" job from just the prod environment. $ copilot job delete --name report-generator --env prod Delete the \"report-generator\" job from the my-app application from outside of the workspace. $ copilot job delete --name report-generator --app my-app Delete the \"report-generator\" job without the confirmation prompt. $ copilot job delete --name report-generator --yes","title":"job delete"},{"location":"docs/commands/job-delete#job-delete","text":"$ copilot job delete [ flags ]","title":"job delete"},{"location":"docs/commands/job-delete#what-does-it-do","text":"copilot job delete deletes all resources associated with your job in a particular environment.","title":"What does it do?"},{"location":"docs/commands/job-delete#what-are-the-flags","text":"-a, --app string Name of the application. -e, --env string Name of the environment. -h, --help help for delete -n, --name string Name of the job. --yes Skips confirmation prompt.","title":"What are the flags?"},{"location":"docs/commands/job-delete#examples","text":"Delete the \"report-generator\" job from the my-app application. $ copilot job delete --name report-generator --app my-app Delete the \"report-generator\" job from just the prod environment. $ copilot job delete --name report-generator --env prod Delete the \"report-generator\" job from the my-app application from outside of the workspace. $ copilot job delete --name report-generator --app my-app Delete the \"report-generator\" job without the confirmation prompt. $ copilot job delete --name report-generator --yes","title":"Examples"},{"location":"docs/commands/job-deploy","text":"job deploy $ copilot job deploy What does it do? job deploy takes your local code and configuration and deploys your job. The steps involved in job deploy are: Build your local Dockerfile into an image Tag it with the value from --tag or the latest git sha (if you're in a git directory) Push the image to ECR Package your manifest file and addons into CloudFormation Create / update your ECS task definition and job What are the flags? -a, --app string Name of the application. -e, --env string Name of the environment. -h, --help help for deploy -n, --name string Name of the job. --resource-tags stringToString Optional. Labels with a key and value separated by commas. Allows you to categorize resources. ( default []) --tag string Optional. The container image tag. Examples Deploys a job named \"report-gen\" to a \"test\" environment. $ copilot job deploy --name report-gen --env test Deploys a job with additional resource tags. $ copilot job deploy --resource-tags source/revision = bb133e7,deployment/initiator = manual `","title":"job deploy"},{"location":"docs/commands/job-deploy#job-deploy","text":"$ copilot job deploy","title":"job deploy"},{"location":"docs/commands/job-deploy#what-does-it-do","text":"job deploy takes your local code and configuration and deploys your job. The steps involved in job deploy are: Build your local Dockerfile into an image Tag it with the value from --tag or the latest git sha (if you're in a git directory) Push the image to ECR Package your manifest file and addons into CloudFormation Create / update your ECS task definition and job","title":"What does it do?"},{"location":"docs/commands/job-deploy#what-are-the-flags","text":"-a, --app string Name of the application. -e, --env string Name of the environment. -h, --help help for deploy -n, --name string Name of the job. --resource-tags stringToString Optional. Labels with a key and value separated by commas. Allows you to categorize resources. ( default []) --tag string Optional. The container image tag.","title":"What are the flags?"},{"location":"docs/commands/job-deploy#examples","text":"Deploys a job named \"report-gen\" to a \"test\" environment. $ copilot job deploy --name report-gen --env test Deploys a job with additional resource tags. $ copilot job deploy --resource-tags source/revision = bb133e7,deployment/initiator = manual `","title":"Examples"},{"location":"docs/commands/job-init","text":"job init $ copilot job init What does it do? copilot job init creates a new job to run your code for you. After running this command, the CLI creates sub-directory with your app name in your local copilot directory where you'll find a manifest file . Feel free to update your manifest file to change the default configs for your job. The CLI also sets up an ECR repository with a policy for all environments to be able to pull from it. Then, your job gets registered to AWS System Manager Parameter Store so that the CLI can keep track of your it. After that, if you already have an environment set up, you can run copilot job deploy to deploy your job in that environment. What are the flags? -a, --app string Name of the application. -d, --dockerfile string Path to the Dockerfile. Mutually exclusive with -i, --image -h, --help help for init -i, --image string The location of an existing Docker image. Mutually exclusive with -d, --dockerfile -t, --job-type string Type of job to create. Must be one of: \"Scheduled Job\" -n, --name string Name of the job. --retries int Optional. The number of times to try restarting the job on a failure. -s, --schedule string The schedule on which to run this job. Accepts cron expressions of the format ( M H DoM M DoW ) and schedule definition strings. For example: \"0 * * * *\" , \"@daily\" , \"@weekly\" , \"@every 1h30m\" . AWS Schedule Expressions of the form \"rate(10 minutes)\" or \"cron(0 12 L * ? 2021)\" are also accepted. --timeout string Optional. The total execution time for the task, including retries. Accepts valid Go duration strings. For example: \"2h\" , \"1h30m\" , \"900s\" . Examples Creates a \"reaper\" scheduled task to run once per day. $ copilot job init --name reaper --dockerfile ./frontend/Dockerfile --schedule \"@daily\" Creates a \"report-generator\" scheduled task with retries. $ copilot job init --name report-generator --schedule \"@monthly\" --retries 3 --timeout 900s","title":"job init"},{"location":"docs/commands/job-init#job-init","text":"$ copilot job init","title":"job init"},{"location":"docs/commands/job-init#what-does-it-do","text":"copilot job init creates a new job to run your code for you. After running this command, the CLI creates sub-directory with your app name in your local copilot directory where you'll find a manifest file . Feel free to update your manifest file to change the default configs for your job. The CLI also sets up an ECR repository with a policy for all environments to be able to pull from it. Then, your job gets registered to AWS System Manager Parameter Store so that the CLI can keep track of your it. After that, if you already have an environment set up, you can run copilot job deploy to deploy your job in that environment.","title":"What does it do?"},{"location":"docs/commands/job-init#what-are-the-flags","text":"-a, --app string Name of the application. -d, --dockerfile string Path to the Dockerfile. Mutually exclusive with -i, --image -h, --help help for init -i, --image string The location of an existing Docker image. Mutually exclusive with -d, --dockerfile -t, --job-type string Type of job to create. Must be one of: \"Scheduled Job\" -n, --name string Name of the job. --retries int Optional. The number of times to try restarting the job on a failure. -s, --schedule string The schedule on which to run this job. Accepts cron expressions of the format ( M H DoM M DoW ) and schedule definition strings. For example: \"0 * * * *\" , \"@daily\" , \"@weekly\" , \"@every 1h30m\" . AWS Schedule Expressions of the form \"rate(10 minutes)\" or \"cron(0 12 L * ? 2021)\" are also accepted. --timeout string Optional. The total execution time for the task, including retries. Accepts valid Go duration strings. For example: \"2h\" , \"1h30m\" , \"900s\" .","title":"What are the flags?"},{"location":"docs/commands/job-init#examples","text":"Creates a \"reaper\" scheduled task to run once per day. $ copilot job init --name reaper --dockerfile ./frontend/Dockerfile --schedule \"@daily\" Creates a \"report-generator\" scheduled task with retries. $ copilot job init --name report-generator --schedule \"@monthly\" --retries 3 --timeout 900s","title":"Examples"},{"location":"docs/commands/job-ls","text":"job ls $ copilot job ls What does it do? copilot job ls lists all the Copilot jobs for a particular application. What are the flags? -a, --app string Name of the application. -h, --help help for ls --json Optional. Outputs in JSON format. --local Only show jobs in the workspace. Example Lists all the jobs for the \"myapp\" application. $ copilot job ls --app myapp","title":"job ls"},{"location":"docs/commands/job-ls#job-ls","text":"$ copilot job ls","title":"job ls"},{"location":"docs/commands/job-ls#what-does-it-do","text":"copilot job ls lists all the Copilot jobs for a particular application.","title":"What does it do?"},{"location":"docs/commands/job-ls#what-are-the-flags","text":"-a, --app string Name of the application. -h, --help help for ls --json Optional. Outputs in JSON format. --local Only show jobs in the workspace.","title":"What are the flags?"},{"location":"docs/commands/job-ls#example","text":"Lists all the jobs for the \"myapp\" application. $ copilot job ls --app myapp","title":"Example"},{"location":"docs/commands/job-package","text":"job package $ copilot job package What does it do? copilot job package produces the CloudFormation template(s) used to deploy a job to an environment. What are the flags? -a, --app string Name of the application. -e, --env string Name of the environment. -h, --help help for package -n, --name string Name of the job. --output-dir string Optional. Writes the stack template and template configuration to a directory. --tag string Optional. The container image tag. Examples Prints the CloudFormation template for the \"report-generator\" job parametrized for the \"test\" environment. $ copilot job package -n report-generator -e test Writes the CloudFormation stack and configuration to an \"infrastructure/\" sub-directory instead of printing. $ copilot job package -n report-generator -e test --output-dir ./infrastructure $ ls ./infrastructure report-generator-test.stack.yml report-generator-test.params.yml","title":"job package"},{"location":"docs/commands/job-package#job-package","text":"$ copilot job package","title":"job package"},{"location":"docs/commands/job-package#what-does-it-do","text":"copilot job package produces the CloudFormation template(s) used to deploy a job to an environment.","title":"What does it do?"},{"location":"docs/commands/job-package#what-are-the-flags","text":"-a, --app string Name of the application. -e, --env string Name of the environment. -h, --help help for package -n, --name string Name of the job. --output-dir string Optional. Writes the stack template and template configuration to a directory. --tag string Optional. The container image tag.","title":"What are the flags?"},{"location":"docs/commands/job-package#examples","text":"Prints the CloudFormation template for the \"report-generator\" job parametrized for the \"test\" environment. $ copilot job package -n report-generator -e test Writes the CloudFormation stack and configuration to an \"infrastructure/\" sub-directory instead of printing. $ copilot job package -n report-generator -e test --output-dir ./infrastructure $ ls ./infrastructure report-generator-test.stack.yml report-generator-test.params.yml","title":"Examples"},{"location":"docs/commands/pipeline-delete","text":"pipeline delete $ copilot pipeline delete [ flags ] What does it do? copilot pipeline delete deletes the pipeline associated with your workspace. What are the flags? --delete-secret Deletes AWS Secrets Manager secret associated with a pipeline source repository. -h, --help help for delete --yes Skips confirmation prompt. Examples Delete the pipeline associated with your workspace. $ copilot pipeline delete","title":"pipeline delete"},{"location":"docs/commands/pipeline-delete#pipeline-delete","text":"$ copilot pipeline delete [ flags ]","title":"pipeline delete"},{"location":"docs/commands/pipeline-delete#what-does-it-do","text":"copilot pipeline delete deletes the pipeline associated with your workspace.","title":"What does it do?"},{"location":"docs/commands/pipeline-delete#what-are-the-flags","text":"--delete-secret Deletes AWS Secrets Manager secret associated with a pipeline source repository. -h, --help help for delete --yes Skips confirmation prompt.","title":"What are the flags?"},{"location":"docs/commands/pipeline-delete#examples","text":"Delete the pipeline associated with your workspace. $ copilot pipeline delete","title":"Examples"},{"location":"docs/commands/pipeline-init","text":"pipeline init $ copilot pipeline init [ flags ] What does it do? copilot pipeline init creates a pipeline manifest for the services in your workspace, using the environments associated with the application. What are the flags? -a, --app string Name of the application. -e, --environments strings Environments to add to the pipeline. -b, --git-branch string Branch used to trigger your pipeline. -u, --url string The repository URL to trigger your pipeline. -h, --help help for init Examples Create a pipeline for the services in your workspace. $ copilot pipeline init \\ --url https://github.com/gitHubUserName/myFrontendApp.git \\ --environments \"test,prod\"","title":"pipeline init"},{"location":"docs/commands/pipeline-init#pipeline-init","text":"$ copilot pipeline init [ flags ]","title":"pipeline init"},{"location":"docs/commands/pipeline-init#what-does-it-do","text":"copilot pipeline init creates a pipeline manifest for the services in your workspace, using the environments associated with the application.","title":"What does it do?"},{"location":"docs/commands/pipeline-init#what-are-the-flags","text":"-a, --app string Name of the application. -e, --environments strings Environments to add to the pipeline. -b, --git-branch string Branch used to trigger your pipeline. -u, --url string The repository URL to trigger your pipeline. -h, --help help for init","title":"What are the flags?"},{"location":"docs/commands/pipeline-init#examples","text":"Create a pipeline for the services in your workspace. $ copilot pipeline init \\ --url https://github.com/gitHubUserName/myFrontendApp.git \\ --environments \"test,prod\"","title":"Examples"},{"location":"docs/commands/pipeline-ls","text":"pipeline ls $ copilot pipeline ls [ flags ] What does it do? copilot pipeline ls lists all the deployed pipelines in an application. What are the flags? -a, --app string Name of the application. -h, --help help for ls --json Optional. Outputs in JSON format. Examples Lists all the pipelines for the \"phonetool\" application. $ copilot pipeline ls -a phonetool","title":"pipeline ls"},{"location":"docs/commands/pipeline-ls#pipeline-ls","text":"$ copilot pipeline ls [ flags ]","title":"pipeline ls"},{"location":"docs/commands/pipeline-ls#what-does-it-do","text":"copilot pipeline ls lists all the deployed pipelines in an application.","title":"What does it do?"},{"location":"docs/commands/pipeline-ls#what-are-the-flags","text":"-a, --app string Name of the application. -h, --help help for ls --json Optional. Outputs in JSON format.","title":"What are the flags?"},{"location":"docs/commands/pipeline-ls#examples","text":"Lists all the pipelines for the \"phonetool\" application. $ copilot pipeline ls -a phonetool","title":"Examples"},{"location":"docs/commands/pipeline-show","text":"pipeline show $ copilot pipeline show [ flags ] What does it do? copilot pipeline show shows configuration information about a deployed pipeline for an application, including the account, region, and stages. What are the flags? -a, --app string Name of the application. -h, --help help for show --json Optional. Outputs in JSON format. -n, --name string Name of the pipeline. --resources Optional. Show the resources in your pipeline. Examples Shows info about the pipeline in the \"myapp\" application. $ copilot pipeline show --app myapp --resources What does it look like?","title":"pipeline show"},{"location":"docs/commands/pipeline-show#pipeline-show","text":"$ copilot pipeline show [ flags ]","title":"pipeline show"},{"location":"docs/commands/pipeline-show#what-does-it-do","text":"copilot pipeline show shows configuration information about a deployed pipeline for an application, including the account, region, and stages.","title":"What does it do?"},{"location":"docs/commands/pipeline-show#what-are-the-flags","text":"-a, --app string Name of the application. -h, --help help for show --json Optional. Outputs in JSON format. -n, --name string Name of the pipeline. --resources Optional. Show the resources in your pipeline.","title":"What are the flags?"},{"location":"docs/commands/pipeline-show#examples","text":"Shows info about the pipeline in the \"myapp\" application. $ copilot pipeline show --app myapp --resources","title":"Examples"},{"location":"docs/commands/pipeline-show#what-does-it-look-like","text":"","title":"What does it look like?"},{"location":"docs/commands/pipeline-status","text":"pipeline status $ copilot pipeline status [ flags ] What does it do? copilot pipeline status shows the status of the stages in a deployed pipeline. What are the flags? -a, --app string Name of the application. -h, --help help for status --json Optional. Outputs in JSON format. -n, --name string Name of the pipeline. Examples Shows status of the pipeline \"pipeline-myapp-myrepo\". $ copilot pipeline status -n pipeline-myapp-myrepo What does it look like?","title":"pipeline status"},{"location":"docs/commands/pipeline-status#pipeline-status","text":"$ copilot pipeline status [ flags ]","title":"pipeline status"},{"location":"docs/commands/pipeline-status#what-does-it-do","text":"copilot pipeline status shows the status of the stages in a deployed pipeline.","title":"What does it do?"},{"location":"docs/commands/pipeline-status#what-are-the-flags","text":"-a, --app string Name of the application. -h, --help help for status --json Optional. Outputs in JSON format. -n, --name string Name of the pipeline.","title":"What are the flags?"},{"location":"docs/commands/pipeline-status#examples","text":"Shows status of the pipeline \"pipeline-myapp-myrepo\". $ copilot pipeline status -n pipeline-myapp-myrepo","title":"Examples"},{"location":"docs/commands/pipeline-status#what-does-it-look-like","text":"","title":"What does it look like?"},{"location":"docs/commands/pipeline-update","text":"pipeline update $ copilot pipeline update [ flags ] What does it do? copilot pipeline update deploys a pipeline for the services in your workspace, using the environments associated with the application from a pipeline manifest. What are the flags? -h, --help help for update --yes Skips confirmation prompt. Examples Deploys an updated pipeline for the services in your workspace. $ copilot pipeline update","title":"pipeline update"},{"location":"docs/commands/pipeline-update#pipeline-update","text":"$ copilot pipeline update [ flags ]","title":"pipeline update"},{"location":"docs/commands/pipeline-update#what-does-it-do","text":"copilot pipeline update deploys a pipeline for the services in your workspace, using the environments associated with the application from a pipeline manifest.","title":"What does it do?"},{"location":"docs/commands/pipeline-update#what-are-the-flags","text":"-h, --help help for update --yes Skips confirmation prompt.","title":"What are the flags?"},{"location":"docs/commands/pipeline-update#examples","text":"Deploys an updated pipeline for the services in your workspace. $ copilot pipeline update","title":"Examples"},{"location":"docs/commands/storage-init","text":"storage init $ copilot storage init What does it do? copilot storage init creates a new storage resource attached to one of your workloads, accessible from inside your service container via a friendly environment variable. You can specify either S3 , DynamoDB or Aurora as the resource type. After running this command, the CLI creates an addons subdirectory inside your copilot/service directory if it does not exist. When you run copilot svc deploy , your newly initialized storage resource is created in the environment you're deploying to. By default, only the service you specify during storage init will have access to that storage resource. What are the flags? Required Flags -n, --name string Name of the storage resource to create. -t, --storage-type string Type of storage to add. Must be one of: \"DynamoDB\" , \"S3\" , \"Aurora\" -w, --workload string Name of the service or job to associate with storage. DynamoDB Flags --lsi stringArray Optional. Attribute to use as an alternate sort key. May be specified up to 5 times. Must be of the format '<keyName>:<dataType>' . --no-lsi Optional. Don 't ask about configuring alternate sort keys. --no-sort Optional. Skip configuring sort keys. --partition-key string Partition key for the DDB table. Must be of the format ' <keyName>:<dataType> '. --sort-key string Optional. Sort key for the DDB table. Must be of the format ' <keyName>:<dataType> ' . Aurora Serverless Flags --engine string The database engine used in the cluster. Must be either \"MySQL\" or \"PostgreSQL\" . --parameter-group string Optional. The name of the parameter group to associate with the cluster. --initial-db string The initial database to create in the cluster. How can I use it? Create an S3 bucket named \"my-bucket\" attached to the \"frontend\" service. $ copilot storage init -n my-bucket -t S3 -w frontend Create a basic DynamoDB table named \"my-table\" attached to the \"frontend\" service with a sort key specified. $ copilot storage init -n my-table -t DynamoDB -w frontend --partition-key Email:S --sort-key UserId:N --no-lsi Create a DynamoDB table with multiple alternate sort keys. $ copilot storage init \\ -n my-table -t DynamoDB -w frontend \\ --partition-key Email:S \\ --sort-key UserId:N \\ --lsi Points:N \\ --lsi Goodness:N Create an RDS Aurora Serverless cluster using PostgreSQL as the database engine. $ copilot storage init \\ -n my-cluster -t Aurora -w frontend --engine PostgreSQL What happens under the hood? Copilot writes a Cloudformation template specifying the S3 bucket or DDB table to the addons dir. When you run copilot svc deploy , the CLI merges this template with all the other templates in the addons directory to create a nested stack associated with your service. This nested stack describes all the additional resources you've associated with that service and is deployed wherever your service is deployed. This means that after running $ copilot storage init -n bucket -t S3 -w fe $ copilot svc deploy -n fe -e test $ copilot svc deploy -n fe -e prod there will be two buckets deployed, one in the \"test\" env and one in the \"prod\" env, accessible only to the \"fe\" service in its respective environment.","title":"storage init"},{"location":"docs/commands/storage-init#storage-init","text":"$ copilot storage init","title":"storage init"},{"location":"docs/commands/storage-init#what-does-it-do","text":"copilot storage init creates a new storage resource attached to one of your workloads, accessible from inside your service container via a friendly environment variable. You can specify either S3 , DynamoDB or Aurora as the resource type. After running this command, the CLI creates an addons subdirectory inside your copilot/service directory if it does not exist. When you run copilot svc deploy , your newly initialized storage resource is created in the environment you're deploying to. By default, only the service you specify during storage init will have access to that storage resource.","title":"What does it do?"},{"location":"docs/commands/storage-init#what-are-the-flags","text":"Required Flags -n, --name string Name of the storage resource to create. -t, --storage-type string Type of storage to add. Must be one of: \"DynamoDB\" , \"S3\" , \"Aurora\" -w, --workload string Name of the service or job to associate with storage. DynamoDB Flags --lsi stringArray Optional. Attribute to use as an alternate sort key. May be specified up to 5 times. Must be of the format '<keyName>:<dataType>' . --no-lsi Optional. Don 't ask about configuring alternate sort keys. --no-sort Optional. Skip configuring sort keys. --partition-key string Partition key for the DDB table. Must be of the format ' <keyName>:<dataType> '. --sort-key string Optional. Sort key for the DDB table. Must be of the format ' <keyName>:<dataType> ' . Aurora Serverless Flags --engine string The database engine used in the cluster. Must be either \"MySQL\" or \"PostgreSQL\" . --parameter-group string Optional. The name of the parameter group to associate with the cluster. --initial-db string The initial database to create in the cluster.","title":"What are the flags?"},{"location":"docs/commands/storage-init#how-can-i-use-it","text":"Create an S3 bucket named \"my-bucket\" attached to the \"frontend\" service. $ copilot storage init -n my-bucket -t S3 -w frontend Create a basic DynamoDB table named \"my-table\" attached to the \"frontend\" service with a sort key specified. $ copilot storage init -n my-table -t DynamoDB -w frontend --partition-key Email:S --sort-key UserId:N --no-lsi Create a DynamoDB table with multiple alternate sort keys. $ copilot storage init \\ -n my-table -t DynamoDB -w frontend \\ --partition-key Email:S \\ --sort-key UserId:N \\ --lsi Points:N \\ --lsi Goodness:N Create an RDS Aurora Serverless cluster using PostgreSQL as the database engine. $ copilot storage init \\ -n my-cluster -t Aurora -w frontend --engine PostgreSQL","title":"How can I use it?"},{"location":"docs/commands/storage-init#what-happens-under-the-hood","text":"Copilot writes a Cloudformation template specifying the S3 bucket or DDB table to the addons dir. When you run copilot svc deploy , the CLI merges this template with all the other templates in the addons directory to create a nested stack associated with your service. This nested stack describes all the additional resources you've associated with that service and is deployed wherever your service is deployed. This means that after running $ copilot storage init -n bucket -t S3 -w fe $ copilot svc deploy -n fe -e test $ copilot svc deploy -n fe -e prod there will be two buckets deployed, one in the \"test\" env and one in the \"prod\" env, accessible only to the \"fe\" service in its respective environment.","title":"What happens under the hood?"},{"location":"docs/commands/svc-delete","text":"svc delete $ copilot svc delete [ flags ] What does it do? copilot svc delete deletes all resources associated with your service in a particular environment. What are the flags? -e, --env string Name of the environment. -h, --help help for delete -n, --name string Name of the service. --yes Skips confirmation prompt. Examples Force delete the application with environments \"test\" and \"prod\". $ copilot svc delete --name test --yes","title":"svc delete"},{"location":"docs/commands/svc-delete#svc-delete","text":"$ copilot svc delete [ flags ]","title":"svc delete"},{"location":"docs/commands/svc-delete#what-does-it-do","text":"copilot svc delete deletes all resources associated with your service in a particular environment.","title":"What does it do?"},{"location":"docs/commands/svc-delete#what-are-the-flags","text":"-e, --env string Name of the environment. -h, --help help for delete -n, --name string Name of the service. --yes Skips confirmation prompt.","title":"What are the flags?"},{"location":"docs/commands/svc-delete#examples","text":"Force delete the application with environments \"test\" and \"prod\". $ copilot svc delete --name test --yes","title":"Examples"},{"location":"docs/commands/svc-deploy","text":"svc deploy $ copilot svc deploy What does it do? copilot svc deploy takes your local code and configuration and deploys it. The steps involved in service deploy are: Build your local Dockerfile into an image Tag it with the value from --tag or the latest git sha (if you're in a git directory) Push the image to ECR Package your manifest file and addons into CloudFormation Create / update your ECS task definition and service What are the flags? -e, --env string Name of the environment. -h, --help help for deploy -n, --name string Name of the service. --resource-tags stringToString Optional. Labels with a key and value separated by commas. Allows you to categorize resources. ( default []) --tag string Optional. The service ' s image tag.","title":"svc deploy"},{"location":"docs/commands/svc-deploy#svc-deploy","text":"$ copilot svc deploy","title":"svc deploy"},{"location":"docs/commands/svc-deploy#what-does-it-do","text":"copilot svc deploy takes your local code and configuration and deploys it. The steps involved in service deploy are: Build your local Dockerfile into an image Tag it with the value from --tag or the latest git sha (if you're in a git directory) Push the image to ECR Package your manifest file and addons into CloudFormation Create / update your ECS task definition and service","title":"What does it do?"},{"location":"docs/commands/svc-deploy#what-are-the-flags","text":"-e, --env string Name of the environment. -h, --help help for deploy -n, --name string Name of the service. --resource-tags stringToString Optional. Labels with a key and value separated by commas. Allows you to categorize resources. ( default []) --tag string Optional. The service ' s image tag.","title":"What are the flags?"},{"location":"docs/commands/svc-exec","text":"svc exec $ copilot svc exec What does it do? copilot svc exec executes a command in a running container part of a service. What are the flags? -a, --app string Name of the application. -c, --command string Optional. The command that is passed to a running container. (default \"/bin/bash\") --container string Optional. The specific container you want to exec in. By default the first essential container will be used. -e, --env string Name of the environment. -h, --help help for exec -n, --name string Name of the service, job, or task group. --task-id string Optional. ID of the task you want to exec in. Examples Start an interactive bash session with a task part of the \"frontend\" service. $ copilot svc exec -a my-app -e test -n frontend Runs the 'ls' command in the task prefixed with ID \"8c38184\" within the \"backend\" service. $ copilot svc exec -a my-app -e test --name backend --task-id 8c38184 --command \"ls\" What does it look like? Info Please make sure exec: true is set in your manifest before deploying the service. Please note that this will update the service's Fargate Platform Version to 1.4.0. Updating the Platform Version results in replacing your service which will result in downtime for your service.","title":"svc exec"},{"location":"docs/commands/svc-exec#svc-exec","text":"$ copilot svc exec","title":"svc exec"},{"location":"docs/commands/svc-exec#what-does-it-do","text":"copilot svc exec executes a command in a running container part of a service.","title":"What does it do?"},{"location":"docs/commands/svc-exec#what-are-the-flags","text":"-a, --app string Name of the application. -c, --command string Optional. The command that is passed to a running container. (default \"/bin/bash\") --container string Optional. The specific container you want to exec in. By default the first essential container will be used. -e, --env string Name of the environment. -h, --help help for exec -n, --name string Name of the service, job, or task group. --task-id string Optional. ID of the task you want to exec in.","title":"What are the flags?"},{"location":"docs/commands/svc-exec#examples","text":"Start an interactive bash session with a task part of the \"frontend\" service. $ copilot svc exec -a my-app -e test -n frontend Runs the 'ls' command in the task prefixed with ID \"8c38184\" within the \"backend\" service. $ copilot svc exec -a my-app -e test --name backend --task-id 8c38184 --command \"ls\"","title":"Examples"},{"location":"docs/commands/svc-exec#what-does-it-look-like","text":"Info Please make sure exec: true is set in your manifest before deploying the service. Please note that this will update the service's Fargate Platform Version to 1.4.0. Updating the Platform Version results in replacing your service which will result in downtime for your service.","title":"What does it look like?"},{"location":"docs/commands/svc-init","text":"svc init $ copilot svc init What does it do? copilot svc init creates a new service to run your code for you. After running this command, the CLI creates sub-directory with your app name in your local copilot directory where you'll find a manifest file . Feel free to update your manifest file to change the default configs for your service. The CLI also sets up an ECR repository with a policy for all environments to be able to pull from it. Then, your service gets registered to AWS System Manager Parameter Store so that the CLI can keep track of it. After that, if you already have an environment set up, you can run copilot deploy to deploy your service in that environment. What are the flags? Flags -a, --app string Name of the application. -d, --dockerfile string Path to the Dockerfile. Mutually exclusive with -i, --image -i, --image string The location of an existing Docker image. Mutually exclusive with -d, --dockerfile -n, --name string Name of the service. --port uint16 The port on which your service listens. -t, --svc-type string Type of service to create. Must be one of: \"Request-Driven Web Service\" , \"Load Balanced Web Service\" , \"Backend Service\" To create a \"frontend\" load balanced web service you could run: $ copilot svc init --name frontend --app-type \"Load Balanced Web Service\" --dockerfile ./frontend/Dockerfile What does it look like?","title":"svc init"},{"location":"docs/commands/svc-init#svc-init","text":"$ copilot svc init","title":"svc init"},{"location":"docs/commands/svc-init#what-does-it-do","text":"copilot svc init creates a new service to run your code for you. After running this command, the CLI creates sub-directory with your app name in your local copilot directory where you'll find a manifest file . Feel free to update your manifest file to change the default configs for your service. The CLI also sets up an ECR repository with a policy for all environments to be able to pull from it. Then, your service gets registered to AWS System Manager Parameter Store so that the CLI can keep track of it. After that, if you already have an environment set up, you can run copilot deploy to deploy your service in that environment.","title":"What does it do?"},{"location":"docs/commands/svc-init#what-are-the-flags","text":"Flags -a, --app string Name of the application. -d, --dockerfile string Path to the Dockerfile. Mutually exclusive with -i, --image -i, --image string The location of an existing Docker image. Mutually exclusive with -d, --dockerfile -n, --name string Name of the service. --port uint16 The port on which your service listens. -t, --svc-type string Type of service to create. Must be one of: \"Request-Driven Web Service\" , \"Load Balanced Web Service\" , \"Backend Service\" To create a \"frontend\" load balanced web service you could run: $ copilot svc init --name frontend --app-type \"Load Balanced Web Service\" --dockerfile ./frontend/Dockerfile","title":"What are the flags?"},{"location":"docs/commands/svc-init#what-does-it-look-like","text":"","title":"What does it look like?"},{"location":"docs/commands/svc-logs","text":"svc logs $ copilot svc logs What does it do? copilot svc logs displays the logs of a deployed service. What are the flags? -a, --app string Name of the application. --end-time string Optional. Only return logs before a specific date ( RFC3339 ) . Defaults to all logs. Only one of end-time / follow may be used. -e, --env string Name of the environment. --follow Optional. Specifies if the logs should be streamed. -h, --help help for logs --json Optional. Outputs in JSON format. --limit int Optional. The maximum number of log events returned. ( default 10 ) -n, --name string Name of the service. --since duration Optional. Only return logs newer than a relative duration like 5s, 2m, or 3h. Defaults to all logs. Only one of start-time / since may be used. --start-time string Optional. Only return logs after a specific date ( RFC3339 ) . Defaults to all logs. Only one of start-time / since may be used. --tasks strings Optional. Only return logs from specific task IDs. Examples Displays logs of the service \"my-svc\" in environment \"test\". $ copilot svc logs -n my-svc -e test Displays logs in the last hour. $ copilot svc logs --since 1h Displays logs from 2006-01-02T15:04:05 to 2006-01-02T15:05:05. $ copilot svc logs --start-time 2006 -01-02T15:04:05+00:00 --end-time 2006 -01-02T15:05:05+00:00","title":"svc logs"},{"location":"docs/commands/svc-logs#svc-logs","text":"$ copilot svc logs","title":"svc logs"},{"location":"docs/commands/svc-logs#what-does-it-do","text":"copilot svc logs displays the logs of a deployed service.","title":"What does it do?"},{"location":"docs/commands/svc-logs#what-are-the-flags","text":"-a, --app string Name of the application. --end-time string Optional. Only return logs before a specific date ( RFC3339 ) . Defaults to all logs. Only one of end-time / follow may be used. -e, --env string Name of the environment. --follow Optional. Specifies if the logs should be streamed. -h, --help help for logs --json Optional. Outputs in JSON format. --limit int Optional. The maximum number of log events returned. ( default 10 ) -n, --name string Name of the service. --since duration Optional. Only return logs newer than a relative duration like 5s, 2m, or 3h. Defaults to all logs. Only one of start-time / since may be used. --start-time string Optional. Only return logs after a specific date ( RFC3339 ) . Defaults to all logs. Only one of start-time / since may be used. --tasks strings Optional. Only return logs from specific task IDs.","title":"What are the flags?"},{"location":"docs/commands/svc-logs#examples","text":"Displays logs of the service \"my-svc\" in environment \"test\". $ copilot svc logs -n my-svc -e test Displays logs in the last hour. $ copilot svc logs --since 1h Displays logs from 2006-01-02T15:04:05 to 2006-01-02T15:05:05. $ copilot svc logs --start-time 2006 -01-02T15:04:05+00:00 --end-time 2006 -01-02T15:05:05+00:00","title":"Examples"},{"location":"docs/commands/svc-ls","text":"svc ls $ copilot svc ls What does it do? copilot svc ls lists all the Copilot services for a particular application. What are the flags? -a, --app string Name of the application. -h, --help help for ls --json Optional. Outputs in JSON format. --local Only show services in the workspace. What does it look like?","title":"svc ls"},{"location":"docs/commands/svc-ls#svc-ls","text":"$ copilot svc ls","title":"svc ls"},{"location":"docs/commands/svc-ls#what-does-it-do","text":"copilot svc ls lists all the Copilot services for a particular application.","title":"What does it do?"},{"location":"docs/commands/svc-ls#what-are-the-flags","text":"-a, --app string Name of the application. -h, --help help for ls --json Optional. Outputs in JSON format. --local Only show services in the workspace.","title":"What are the flags?"},{"location":"docs/commands/svc-ls#what-does-it-look-like","text":"","title":"What does it look like?"},{"location":"docs/commands/svc-package","text":"svc package $ copilot svc package What does it do? copilot svc package produces the CloudFormation template(s) used to deploy a service to an environment. What are the flags? -e, --env string Name of the environment. -h, --help help for package -n, --name string Name of the service. --output-dir string Optional. Writes the stack template and template configuration to a directory. --tag string Optional. The service ' s image tag. Example Write the CloudFormation stack and configuration to a \"infrastructure/\" sub-directory instead of printing. $ copilot svc package -n frontend -e test --output-dir ./infrastructure $ ls ./infrastructure frontend.stack.yml frontend-test.config.yml","title":"svc package"},{"location":"docs/commands/svc-package#svc-package","text":"$ copilot svc package","title":"svc package"},{"location":"docs/commands/svc-package#what-does-it-do","text":"copilot svc package produces the CloudFormation template(s) used to deploy a service to an environment.","title":"What does it do?"},{"location":"docs/commands/svc-package#what-are-the-flags","text":"-e, --env string Name of the environment. -h, --help help for package -n, --name string Name of the service. --output-dir string Optional. Writes the stack template and template configuration to a directory. --tag string Optional. The service ' s image tag.","title":"What are the flags?"},{"location":"docs/commands/svc-package#example","text":"Write the CloudFormation stack and configuration to a \"infrastructure/\" sub-directory instead of printing. $ copilot svc package -n frontend -e test --output-dir ./infrastructure $ ls ./infrastructure frontend.stack.yml frontend-test.config.yml","title":"Example"},{"location":"docs/commands/svc-pause","text":"svc pause $ copilot svc pause [ flags ] What does it do? Note svc pause is only supported by services of type \"Request-Driven Web Service\". copilot svc pause pauses the App Runner Service associated with your service within a specific environment. What are the flags? -a, --app string Name of the application. -e, --env string Name of the environment. -h, --help help for pause -n, --name string Name of the service. --yes Skips confirmation prompt. Examples Pause running App Runner service \"my-svc\". $ copilot svc pause -n my-svc","title":"svc pause"},{"location":"docs/commands/svc-pause#svc-pause","text":"$ copilot svc pause [ flags ]","title":"svc pause"},{"location":"docs/commands/svc-pause#what-does-it-do","text":"Note svc pause is only supported by services of type \"Request-Driven Web Service\". copilot svc pause pauses the App Runner Service associated with your service within a specific environment.","title":"What does it do?"},{"location":"docs/commands/svc-pause#what-are-the-flags","text":"-a, --app string Name of the application. -e, --env string Name of the environment. -h, --help help for pause -n, --name string Name of the service. --yes Skips confirmation prompt.","title":"What are the flags?"},{"location":"docs/commands/svc-pause#examples","text":"Pause running App Runner service \"my-svc\". $ copilot svc pause -n my-svc","title":"Examples"},{"location":"docs/commands/svc-resume","text":"svc resume $ copilot svc resume [ flags ] What does it do? Note svc resume is only supported by services of type \"Request-Driven Web Service\". copilot svc resume resumes the App Runner Service associated with your service within a specific environment. What are the flags? -a, --app string Name of the application. -e, --env string Name of the environment. -h, --help help for resume -n, --name string Name of the service. Examples Resume paused App Runner service \"my-svc\". $ copilot svc resume -n my-svc","title":"svc resume"},{"location":"docs/commands/svc-resume#svc-resume","text":"$ copilot svc resume [ flags ]","title":"svc resume"},{"location":"docs/commands/svc-resume#what-does-it-do","text":"Note svc resume is only supported by services of type \"Request-Driven Web Service\". copilot svc resume resumes the App Runner Service associated with your service within a specific environment.","title":"What does it do?"},{"location":"docs/commands/svc-resume#what-are-the-flags","text":"-a, --app string Name of the application. -e, --env string Name of the environment. -h, --help help for resume -n, --name string Name of the service.","title":"What are the flags?"},{"location":"docs/commands/svc-resume#examples","text":"Resume paused App Runner service \"my-svc\". $ copilot svc resume -n my-svc","title":"Examples"},{"location":"docs/commands/svc-show","text":"svc show $ copilot svc show What does it do? copilot svc show shows info about a deployed service, including endpoints, capacity and related resources per environment. What are the flags? -a, --app string Name of the application. -h, --help help for show --json Optional. Outputs in JSON format. -n, --name string Name of the service. --resources Optional. Show the resources in your service. What does it look like?","title":"svc show"},{"location":"docs/commands/svc-show#svc-show","text":"$ copilot svc show","title":"svc show"},{"location":"docs/commands/svc-show#what-does-it-do","text":"copilot svc show shows info about a deployed service, including endpoints, capacity and related resources per environment.","title":"What does it do?"},{"location":"docs/commands/svc-show#what-are-the-flags","text":"-a, --app string Name of the application. -h, --help help for show --json Optional. Outputs in JSON format. -n, --name string Name of the service. --resources Optional. Show the resources in your service.","title":"What are the flags?"},{"location":"docs/commands/svc-show#what-does-it-look-like","text":"","title":"What does it look like?"},{"location":"docs/commands/svc-status","text":"svc status $ copilot svc status What does it do? copilot svc status shows the health status of a deployed service, including service status, task status, and related CloudWatch alarms. What are the flags? -a, --app string Name of the application. -e, --env string Name of the environment. -h, --help help for status --json Optional. Outputs in JSON format. -n, --name string Name of the service. What does it look like?","title":"svc status"},{"location":"docs/commands/svc-status#svc-status","text":"$ copilot svc status","title":"svc status"},{"location":"docs/commands/svc-status#what-does-it-do","text":"copilot svc status shows the health status of a deployed service, including service status, task status, and related CloudWatch alarms.","title":"What does it do?"},{"location":"docs/commands/svc-status#what-are-the-flags","text":"-a, --app string Name of the application. -e, --env string Name of the environment. -h, --help help for status --json Optional. Outputs in JSON format. -n, --name string Name of the service.","title":"What are the flags?"},{"location":"docs/commands/svc-status#what-does-it-look-like","text":"","title":"What does it look like?"},{"location":"docs/commands/task-delete","text":"task delete $ copilot task delete What does it do? copilot task delete stops running instances of the task, and deletes associated resources. Info Tasks created with versions of Copilot earlier than v1.2.0 cannot be stopped by copilot task delete . Customers using tasks launched with earlier versions should manually stop any running tasks via the ECS console after running the command. What are the flags? -a, --app string Name of the application. --default Optional. Delete a task which was launched in the default cluster and subnets. Cannot be specified with 'app' or 'env' -e, --env string Name of the environment. -h, --help help for delete -n, --name string Name of the service. --yes Optional. Skips confirmation prompt. Example Delete the \"test\" task from the default cluster. $ copilot task delete --name test --default Delete the \"db-migrate\" task from the prod environment. $ copilot task delete --name db-migrate --env prod Delete the \"test\" task without confirmation prompt. $ copilot task delete --name test --yes","title":"task delete"},{"location":"docs/commands/task-delete#task-delete","text":"$ copilot task delete","title":"task delete"},{"location":"docs/commands/task-delete#what-does-it-do","text":"copilot task delete stops running instances of the task, and deletes associated resources. Info Tasks created with versions of Copilot earlier than v1.2.0 cannot be stopped by copilot task delete . Customers using tasks launched with earlier versions should manually stop any running tasks via the ECS console after running the command.","title":"What does it do?"},{"location":"docs/commands/task-delete#what-are-the-flags","text":"-a, --app string Name of the application. --default Optional. Delete a task which was launched in the default cluster and subnets. Cannot be specified with 'app' or 'env' -e, --env string Name of the environment. -h, --help help for delete -n, --name string Name of the service. --yes Optional. Skips confirmation prompt.","title":"What are the flags?"},{"location":"docs/commands/task-delete#example","text":"Delete the \"test\" task from the default cluster. $ copilot task delete --name test --default Delete the \"db-migrate\" task from the prod environment. $ copilot task delete --name db-migrate --env prod Delete the \"test\" task without confirmation prompt. $ copilot task delete --name test --yes","title":"Example"},{"location":"docs/commands/task-exec","text":"task exec $ copilot task exec What does it do? copilot task exec executes a command in a running container part of a task. What are the flags? -a, --app string Name of the application. -c, --command string Optional. The command that is passed to a running container. (default \"/bin/bash\") --default Optional. Execute commands in running tasks in default cluster and default subnets. Cannot be specified with 'app' or 'env'. -e, --env string Name of the environment. -h, --help help for exec -n, --name string Name of the service, job, or task group. --task-id string Optional. ID of the task you want to exec in. Examples Start an interactive bash session with a task in task group \"db-migrate\" in the \"test\" environment under the current workspace. $ copilot task exec -e test -n db-migrate Runs the 'cat progress.csv' command in the task prefixed with ID \"1848c38\" part of the \"db-migrate\" task group. $ copilot task exec --name db-migrate --task-id 1848c38 --command \"cat progress.csv\" Start an interactive bash session with a task prefixed with ID \"38c3818\" in the default cluster. $ copilot task exec --default --task-id 38c3818 Info copilot task exec cannot be performed without certain task role permissions. If you are using existing task role to run the tasks, please make sure it has the following permissions in order to make copilot task exec work. { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Action\" : [ \"ssmmessages:CreateControlChannel\" , \"ssmmessages:OpenControlChannel\" , \"ssmmessages:CreateDataChannel\" , \"ssmmessages:OpenDataChannel\" ], \"Resource\" : \"*\" , \"Effect\" : \"Allow\" }, { \"Action\" : [ \"logs:CreateLogStream\" , \"logs:DescribeLogGroups\" , \"logs:DescribeLogStreams\" , \"logs:PutLogEvents\" ], \"Resource\" : \"*\" , \"Effect\" : \"Allow\" } ] }","title":"task exec"},{"location":"docs/commands/task-exec#task-exec","text":"$ copilot task exec","title":"task exec"},{"location":"docs/commands/task-exec#what-does-it-do","text":"copilot task exec executes a command in a running container part of a task.","title":"What does it do?"},{"location":"docs/commands/task-exec#what-are-the-flags","text":"-a, --app string Name of the application. -c, --command string Optional. The command that is passed to a running container. (default \"/bin/bash\") --default Optional. Execute commands in running tasks in default cluster and default subnets. Cannot be specified with 'app' or 'env'. -e, --env string Name of the environment. -h, --help help for exec -n, --name string Name of the service, job, or task group. --task-id string Optional. ID of the task you want to exec in.","title":"What are the flags?"},{"location":"docs/commands/task-exec#examples","text":"Start an interactive bash session with a task in task group \"db-migrate\" in the \"test\" environment under the current workspace. $ copilot task exec -e test -n db-migrate Runs the 'cat progress.csv' command in the task prefixed with ID \"1848c38\" part of the \"db-migrate\" task group. $ copilot task exec --name db-migrate --task-id 1848c38 --command \"cat progress.csv\" Start an interactive bash session with a task prefixed with ID \"38c3818\" in the default cluster. $ copilot task exec --default --task-id 38c3818 Info copilot task exec cannot be performed without certain task role permissions. If you are using existing task role to run the tasks, please make sure it has the following permissions in order to make copilot task exec work. { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Action\" : [ \"ssmmessages:CreateControlChannel\" , \"ssmmessages:OpenControlChannel\" , \"ssmmessages:CreateDataChannel\" , \"ssmmessages:OpenDataChannel\" ], \"Resource\" : \"*\" , \"Effect\" : \"Allow\" }, { \"Action\" : [ \"logs:CreateLogStream\" , \"logs:DescribeLogGroups\" , \"logs:DescribeLogStreams\" , \"logs:PutLogEvents\" ], \"Resource\" : \"*\" , \"Effect\" : \"Allow\" } ] }","title":"Examples"},{"location":"docs/commands/task-run","text":"task run $ copilot task run What does it do? copilot task run deploys and runs one-off tasks. Generally, the steps involved in task run are: Create an ECR repository and a log group for your task Build and push the image to ECR Create or update your ECS task definition Run and wait for the tasks to start Info Tasks with the same group name share the same set of resources, including the CloudFormation stack, ECR repository, CloudWatch log group and task definition. If the tasks are deployed to a Copilot environment (i.e. by specifying --env ), only public subnets that are created by that environment will be used. If you are using the --default flag and get an error saying there's no default cluster, run aws ecs create-cluster and then re-run the Copilot command. What are the flags? --app string Optional. Name of the application. Cannot be specified with 'default', 'subnets' or 'security-groups' --cluster string Optional. The short name or full ARN of the cluster to run the task in. --command string Optional. The command that is passed to \"docker run\" to override the default command. --count int Optional. The number of tasks to set up. (default 1) --cpu int Optional. The number of CPU units to reserve for each task. (default 256) --default Optional. Run tasks in default cluster and default subnets. Cannot be specified with 'app', 'env' or 'subnets'. --dockerfile string Path to the Dockerfile. (default \"Dockerfile\") --entrypoint string Optional. The entrypoint that is passed to \"docker run\" to override the default entrypoint. --env string Optional. Name of the environment. Cannot be specified with 'default', 'subnets' or 'security-groups' --env-vars stringToString Optional. Environment variables specified by key=value separated by commas. (default []) --execution-role string Optional. The role that grants the container agent permission to make AWS API calls. --follow Optional. Specifies if the logs should be streamed. --generate-cmd string Optional. Generate a command with a pre-filled value for each flag. To use it for an ECS service, specify --generate-cmd <cluster name>/<service name>. Alternatively, if the service or job is created with Copilot, specify --generate-cmd <application>/<environment>/<service or job name>. Cannot be specified with any other flags. -h, --help help for run --image string Optional. The image to run instead of building a Dockerfile. --memory int Optional. The amount of memory to reserve in MiB for each task. (default 512) --resource-tags stringToString Optional. Labels with a key and value separated by commas. Allows you to categorize resources. (default []) --secrets stringToString Optional. Secrets to inject into the container. Specified by key=value separated by commas. (default []) --security-groups strings Optional. The security group IDs for the task to use. Can be specified multiple times. Cannot be specified with 'app' or 'env'. --subnets strings Optional. The subnet IDs for the task to use. Can be specified multiple times. Cannot be specified with 'app', 'env' or 'default'. --tag string Optional. The container image tag in addition to \"latest\". -n, --task-group-name string Optional. The group name of the task. Tasks with the same group name share the same set of resources. --task-role string Optional. The role for the task to use. Example Run a task using your local Dockerfile and display log streams after the task is running. You will be prompted to specify an environment for the tasks to run in. $ copilot task run --follow Run a task named \"db-migrate\" in the \"test\" environment under the current workspace. $ copilot task run -n db-migrate --env test --follow Run 4 tasks with 2GB memory, an existing image, and a custom task role. $ copilot task run --count 4 --memory 2048 --image=rds-migrate --task-role migrate-role --follow Run a task with environment variables. $ copilot task run --env-vars name=myName,user=myUser Run a task using the current workspace with specific subnets and security groups. $ copilot task run --subnets subnet-123,subnet-456 --security-groups sg-123,sg-456 Run a task with a command. $ copilot task run --command \"python migrate-script.py\"","title":"task run"},{"location":"docs/commands/task-run#task-run","text":"$ copilot task run","title":"task run"},{"location":"docs/commands/task-run#what-does-it-do","text":"copilot task run deploys and runs one-off tasks. Generally, the steps involved in task run are: Create an ECR repository and a log group for your task Build and push the image to ECR Create or update your ECS task definition Run and wait for the tasks to start Info Tasks with the same group name share the same set of resources, including the CloudFormation stack, ECR repository, CloudWatch log group and task definition. If the tasks are deployed to a Copilot environment (i.e. by specifying --env ), only public subnets that are created by that environment will be used. If you are using the --default flag and get an error saying there's no default cluster, run aws ecs create-cluster and then re-run the Copilot command.","title":"What does it do?"},{"location":"docs/commands/task-run#what-are-the-flags","text":"--app string Optional. Name of the application. Cannot be specified with 'default', 'subnets' or 'security-groups' --cluster string Optional. The short name or full ARN of the cluster to run the task in. --command string Optional. The command that is passed to \"docker run\" to override the default command. --count int Optional. The number of tasks to set up. (default 1) --cpu int Optional. The number of CPU units to reserve for each task. (default 256) --default Optional. Run tasks in default cluster and default subnets. Cannot be specified with 'app', 'env' or 'subnets'. --dockerfile string Path to the Dockerfile. (default \"Dockerfile\") --entrypoint string Optional. The entrypoint that is passed to \"docker run\" to override the default entrypoint. --env string Optional. Name of the environment. Cannot be specified with 'default', 'subnets' or 'security-groups' --env-vars stringToString Optional. Environment variables specified by key=value separated by commas. (default []) --execution-role string Optional. The role that grants the container agent permission to make AWS API calls. --follow Optional. Specifies if the logs should be streamed. --generate-cmd string Optional. Generate a command with a pre-filled value for each flag. To use it for an ECS service, specify --generate-cmd <cluster name>/<service name>. Alternatively, if the service or job is created with Copilot, specify --generate-cmd <application>/<environment>/<service or job name>. Cannot be specified with any other flags. -h, --help help for run --image string Optional. The image to run instead of building a Dockerfile. --memory int Optional. The amount of memory to reserve in MiB for each task. (default 512) --resource-tags stringToString Optional. Labels with a key and value separated by commas. Allows you to categorize resources. (default []) --secrets stringToString Optional. Secrets to inject into the container. Specified by key=value separated by commas. (default []) --security-groups strings Optional. The security group IDs for the task to use. Can be specified multiple times. Cannot be specified with 'app' or 'env'. --subnets strings Optional. The subnet IDs for the task to use. Can be specified multiple times. Cannot be specified with 'app', 'env' or 'default'. --tag string Optional. The container image tag in addition to \"latest\". -n, --task-group-name string Optional. The group name of the task. Tasks with the same group name share the same set of resources. --task-role string Optional. The role for the task to use.","title":"What are the flags?"},{"location":"docs/commands/task-run#example","text":"Run a task using your local Dockerfile and display log streams after the task is running. You will be prompted to specify an environment for the tasks to run in. $ copilot task run --follow Run a task named \"db-migrate\" in the \"test\" environment under the current workspace. $ copilot task run -n db-migrate --env test --follow Run 4 tasks with 2GB memory, an existing image, and a custom task role. $ copilot task run --count 4 --memory 2048 --image=rds-migrate --task-role migrate-role --follow Run a task with environment variables. $ copilot task run --env-vars name=myName,user=myUser Run a task using the current workspace with specific subnets and security groups. $ copilot task run --subnets subnet-123,subnet-456 --security-groups sg-123,sg-456 Run a task with a command. $ copilot task run --command \"python migrate-script.py\"","title":"Example"},{"location":"docs/commands/version","text":"version $ copilot version [flags] What does it do? copilot version prints the version of the CLI along with the target operating system it was built for. What are the flags? -h, --help help for version","title":"version"},{"location":"docs/commands/version#version","text":"$ copilot version [flags]","title":"version"},{"location":"docs/commands/version#what-does-it-do","text":"copilot version prints the version of the CLI along with the target operating system it was built for.","title":"What does it do?"},{"location":"docs/commands/version#what-are-the-flags","text":"-h, --help help for version","title":"What are the flags?"},{"location":"docs/concepts/applications","text":"An application is a group of related services, environments, and pipelines. Whether you have one service that does everything or a constellation of micro-services, Copilot organizes them and the environments they're deployed to into an \"application\". Let's walk through an example. We want to build a voting app which needs to collect votes and aggregate the results. To set up our vote app with two services, we can run copilot init twice. The first time we run copilot init , we'll be asked what we should call the application this service will belong to. Since we're trying to build a voting system, we can call our application \"vote\" and our first service \"collector\". The next time we run init , we'll be asked if we want to add our new service to the existing \u201cvote\u201d app, and we\u2019ll name the new service \"aggregator\". Your application configuration (which services and environments belong to it) is stored in your AWS account, so any other users in your account will be able to develop on the \u201cvote\" app as well. This means that you can have a teammate work on one service while you develop the other. Creating an App To set up an application, you can just run copilot init . You'll be asked if you want to set up an app or choose an use app. copilot init Once you've created an application, Copilot stores that application in SSM Parameter store in your AWS account. The account used to set up your application is known as the \"application account\". This is where your app's configuration lives, and anyone who has access to this account can use this app. All resources created within this application will be tagged with the copilot-app aws resource tag . This helps you know which app resources in your account belong to. The name of your application has to be unique within your account (even across region). Additional App Configurations You can also provide more granular configuration for your application by running copilot app init . This includes options to: Tag all application, service and environment resources with an additional set of aws resource tags Use a custom domain name for Load Balanced services $ copilot app init \\ --domain my-awesome-app.aws \\ --resource-tags department = MyDept,team = MyTeam App Infrastructure While the bulk of the infrastructure Copilot provisions is specific to an environment and service, there are some application-wide resources as well. ECR Repositories ECR Repositories are regional resources which store your service images. Each service has its own ECR Repository per region in your app. In the above diagram, the app has several environments spread across three regions. Each of those regions has its own ECR repository for every service in your app. In this case, there are three services. Every time you add a service, we create an ECR Repository in every region. We do this to maintain region isolation (if one region goes down, environments in other region won't be affected) and to reduce cross-region data transfer costs. These ECR Repositories all live within your app's account (not the environment accounts) - and have policies which allow your environment accounts to pull from them. Release Infrastructure For every region represented in your app, we create a KMS Key and an S3 bucket. These resources are used by CodePipeline to enable cross-region and cross-account deployments. All pipelines in your app share these same resources. Similar to the ECR Repositories, the S3 bucket and KMS keys have policies which allow for all of your environments, even in other accounts, to read encrypted deployment artifacts. This makes your cross-account, cross-region CodePipelines possible. Digging into your App Now that we've set up an app, we can check on it using Copilot. Below are a few common ways to check in on your app. What applications are in my account? To see all the apps in your current account and region you can run copilot app ls . $ copilot app ls vote ecs-kudos What's in my application? Running copilot app show will show you a summary of your application, including all the services and environments in your app. $ copilot app show About Name vote URI vote-app.aws Environments Name AccountID Region test 000000000000 us-east-1 Services Name Type collector Load Balanced Web Service aggregator Backend Service","title":"Applications"},{"location":"docs/concepts/applications#creating-an-app","text":"To set up an application, you can just run copilot init . You'll be asked if you want to set up an app or choose an use app. copilot init Once you've created an application, Copilot stores that application in SSM Parameter store in your AWS account. The account used to set up your application is known as the \"application account\". This is where your app's configuration lives, and anyone who has access to this account can use this app. All resources created within this application will be tagged with the copilot-app aws resource tag . This helps you know which app resources in your account belong to. The name of your application has to be unique within your account (even across region).","title":"Creating an App"},{"location":"docs/concepts/applications#additional-app-configurations","text":"You can also provide more granular configuration for your application by running copilot app init . This includes options to: Tag all application, service and environment resources with an additional set of aws resource tags Use a custom domain name for Load Balanced services $ copilot app init \\ --domain my-awesome-app.aws \\ --resource-tags department = MyDept,team = MyTeam","title":"Additional App Configurations"},{"location":"docs/concepts/applications#app-infrastructure","text":"While the bulk of the infrastructure Copilot provisions is specific to an environment and service, there are some application-wide resources as well.","title":"App Infrastructure"},{"location":"docs/concepts/applications#ecr-repositories","text":"ECR Repositories are regional resources which store your service images. Each service has its own ECR Repository per region in your app. In the above diagram, the app has several environments spread across three regions. Each of those regions has its own ECR repository for every service in your app. In this case, there are three services. Every time you add a service, we create an ECR Repository in every region. We do this to maintain region isolation (if one region goes down, environments in other region won't be affected) and to reduce cross-region data transfer costs. These ECR Repositories all live within your app's account (not the environment accounts) - and have policies which allow your environment accounts to pull from them.","title":"ECR Repositories"},{"location":"docs/concepts/applications#release-infrastructure","text":"For every region represented in your app, we create a KMS Key and an S3 bucket. These resources are used by CodePipeline to enable cross-region and cross-account deployments. All pipelines in your app share these same resources. Similar to the ECR Repositories, the S3 bucket and KMS keys have policies which allow for all of your environments, even in other accounts, to read encrypted deployment artifacts. This makes your cross-account, cross-region CodePipelines possible.","title":"Release Infrastructure"},{"location":"docs/concepts/applications#digging-into-your-app","text":"Now that we've set up an app, we can check on it using Copilot. Below are a few common ways to check in on your app.","title":"Digging into your App"},{"location":"docs/concepts/applications#what-applications-are-in-my-account","text":"To see all the apps in your current account and region you can run copilot app ls . $ copilot app ls vote ecs-kudos","title":"What applications are in my account?"},{"location":"docs/concepts/applications#whats-in-my-application","text":"Running copilot app show will show you a summary of your application, including all the services and environments in your app. $ copilot app show About Name vote URI vote-app.aws Environments Name AccountID Region test 000000000000 us-east-1 Services Name Type collector Load Balanced Web Service aggregator Backend Service","title":"What's in my application?"},{"location":"docs/concepts/environments","text":"When you first run copilot init , you're asked if you want to create a test environment. This test environment contains all the AWS resources to provision a secure network (VPC, subnets, security groups, and more), as well as other resources that are meant to be shared between multiple services like an Application Load Balancer or an ECS Cluster. When you deploy your service into your test environment, your service will use the test environment's network and resources. Your application can have multiple environments, and each will have its own networking and shared resources infrastructure. While Copilot creates a test environment for you when you get started, it's common to create a new, separate environment for production. This production environment will be completely independent from the test environment, with its own networking stack and its own copy of services. By having both a test environment and a production environment, you can deploy changes to your test environment, validate them, then promote them to the production environment. In the diagram below we have an application called MyApp with two services, API and Backend . Those two services are deployed to the two environments, test and prod . You can see that in the test environment, both services are running only one container while the prod services have more containers running. Services can use different configurations depending on the environment they're deployed in. For more, check out the using environment variables guide. Creating an Environment To create a new environment in your app, you can run copilot env init from within your workspace. Copilot will ask you what you want to name this environment and what profile you'd like to use to bootstrap the environment. These profiles are AWS named profiles which are associated with a particular account and region. When you select one of these profiles, your environment will be created in whichever account and region that profile is associated with. $ copilot env init After you run copilot env init you can watch as Copilot sets up all the environment resources, which can take a few minutes. Once all those resources are created, the environment will be linked back to the application account. This allows actors in the application account to manage the environment, even without access to the environment account. This linking process also creates and configures new regional ECR repositories, if necessary. Deploying a Service When you first create a new environment, no services are deployed to it. To deploy a service run copilot deploy from that service's directory, and you'll be prompted to select which environment to deploy to. Environment Infrastructure VPC and Networking Each environment gets its own multi-AZ VPC. Your VPC is the network boundary of your environment, allowing the traffic you expect in and out, and blocking the rest. The VPCs Copilot creates are spread across two availability zones to help balance availability and cost - with each AZ getting a public and private subnet. Your services are launched in the public subnets but can be reached only through your load balancer. Load Balancers and DNS If you set up any service using one of the Load Balanced Service types, Copilot will set up an Application Load Balancer. All Load Balanced Web Services within an environment will share a load balancer by creating app-specific listeners on it. Your load balancer is allowed to communicate with services in your VPC. Optionally, when you set up an application, you can provide a domain name that you own and is registered in Route 53. If you provide a domain name, each time you spin up an environment, Copilot will create a subdomain environment-name.app-name.your-domain.com, provision an ACM cert, and bind it to your Application Load Balancer so it can use HTTPS. Customize your Environment Optionally, you can customize your environment interactively by using flags to import your existing resources, or configure the default environment resources. Currently, only VPC resources are customizable. However, if you want to customize more types of resources, feel free to bring your use cases and cut an issue! Digging into your Environment Now that we've spun up an environment, we can check on it using Copilot. Below are a few common ways to check in on your environment. What environments are part of my app? To see all the environments in your application you can run copilot env ls . $ copilot env ls test production What's in your environment? Running copilot env show will show you a summary of your environment. Here's an example of the output you might see for our test environment. This output includes the account and region the environment is in, the services deployed to that environment, and the tag that all resources created in this environment will have. You can also provide an optional --resources flag to see all AWS resources associated with this environment. $ copilot env show --name test About Name test Production false Region us-west-2 Account ID 693652174720 Services Name Type ---- ---- api Load Balanced Web Service backend Backend Service Tags Key Value --- ----- copilot-application my-app copilot-environment test","title":"Environments"},{"location":"docs/concepts/environments#creating-an-environment","text":"To create a new environment in your app, you can run copilot env init from within your workspace. Copilot will ask you what you want to name this environment and what profile you'd like to use to bootstrap the environment. These profiles are AWS named profiles which are associated with a particular account and region. When you select one of these profiles, your environment will be created in whichever account and region that profile is associated with. $ copilot env init After you run copilot env init you can watch as Copilot sets up all the environment resources, which can take a few minutes. Once all those resources are created, the environment will be linked back to the application account. This allows actors in the application account to manage the environment, even without access to the environment account. This linking process also creates and configures new regional ECR repositories, if necessary.","title":"Creating an Environment"},{"location":"docs/concepts/environments#deploying-a-service","text":"When you first create a new environment, no services are deployed to it. To deploy a service run copilot deploy from that service's directory, and you'll be prompted to select which environment to deploy to.","title":"Deploying a Service"},{"location":"docs/concepts/environments#environment-infrastructure","text":"","title":"Environment Infrastructure"},{"location":"docs/concepts/environments#vpc-and-networking","text":"Each environment gets its own multi-AZ VPC. Your VPC is the network boundary of your environment, allowing the traffic you expect in and out, and blocking the rest. The VPCs Copilot creates are spread across two availability zones to help balance availability and cost - with each AZ getting a public and private subnet. Your services are launched in the public subnets but can be reached only through your load balancer.","title":"VPC and Networking"},{"location":"docs/concepts/environments#load-balancers-and-dns","text":"If you set up any service using one of the Load Balanced Service types, Copilot will set up an Application Load Balancer. All Load Balanced Web Services within an environment will share a load balancer by creating app-specific listeners on it. Your load balancer is allowed to communicate with services in your VPC. Optionally, when you set up an application, you can provide a domain name that you own and is registered in Route 53. If you provide a domain name, each time you spin up an environment, Copilot will create a subdomain environment-name.app-name.your-domain.com, provision an ACM cert, and bind it to your Application Load Balancer so it can use HTTPS.","title":"Load Balancers and DNS"},{"location":"docs/concepts/environments#customize-your-environment","text":"Optionally, you can customize your environment interactively by using flags to import your existing resources, or configure the default environment resources. Currently, only VPC resources are customizable. However, if you want to customize more types of resources, feel free to bring your use cases and cut an issue!","title":"Customize your Environment"},{"location":"docs/concepts/environments#digging-into-your-environment","text":"Now that we've spun up an environment, we can check on it using Copilot. Below are a few common ways to check in on your environment.","title":"Digging into your Environment"},{"location":"docs/concepts/environments#what-environments-are-part-of-my-app","text":"To see all the environments in your application you can run copilot env ls . $ copilot env ls test production","title":"What environments are part of my app?"},{"location":"docs/concepts/environments#whats-in-your-environment","text":"Running copilot env show will show you a summary of your environment. Here's an example of the output you might see for our test environment. This output includes the account and region the environment is in, the services deployed to that environment, and the tag that all resources created in this environment will have. You can also provide an optional --resources flag to see all AWS resources associated with this environment. $ copilot env show --name test About Name test Production false Region us-west-2 Account ID 693652174720 Services Name Type ---- ---- api Load Balanced Web Service backend Backend Service Tags Key Value --- ----- copilot-application my-app copilot-environment test","title":"What's in your environment?"},{"location":"docs/concepts/jobs","text":"Jobs are Amazon ECS tasks that are triggered by an event. Currently, Copilot supports only \"Scheduled Jobs\". These are tasks that can be triggered either on a fixed schedule or periodically by providing a rate. Creating a Job The easiest way to create a job is to run the init command from the same directory as your Dockerfile. $ copilot init Once you select which application the job should be part of, Copilot will ask you the type of job you'd like to create. Currently, Copilot only supports \"Scheduled Job\". Config and the Manifest After you've run copilot init , the CLI will create a file called manifest.yml . The scheduled job manifest is a simple declarative file that contains the most common configuration for a task that's triggered by a scheduled event. For example, you can configure when you'd like to trigger the job, the container size, the timeout for the task, as well as how many times to retry in case of failures. Deploying a Job Once you've configured your manifest file to satisfy your requirements, you can deploy the changes with the deploy command: $ copilot deploy Running this command will: Build your image locally Push to your job's ECR repository Convert the manifest file to a CloudFormation template Package any additional infrastructure into the CloudFormation template Deploy your resources What's in a Job? Since Copilot uses CloudFormation under the hood, all the resources created are visible and tagged by Copilot. Scheduled Jobs are composed of an AmazonECS Task Definition, Task Role, Task Execution Role, a Step Function State Machine for retrying on failures, and finally an Event Rule to trigger the state machine.","title":"Jobs"},{"location":"docs/concepts/jobs#creating-a-job","text":"The easiest way to create a job is to run the init command from the same directory as your Dockerfile. $ copilot init Once you select which application the job should be part of, Copilot will ask you the type of job you'd like to create. Currently, Copilot only supports \"Scheduled Job\".","title":"Creating a Job"},{"location":"docs/concepts/jobs#config-and-the-manifest","text":"After you've run copilot init , the CLI will create a file called manifest.yml . The scheduled job manifest is a simple declarative file that contains the most common configuration for a task that's triggered by a scheduled event. For example, you can configure when you'd like to trigger the job, the container size, the timeout for the task, as well as how many times to retry in case of failures.","title":"Config and the Manifest"},{"location":"docs/concepts/jobs#deploying-a-job","text":"Once you've configured your manifest file to satisfy your requirements, you can deploy the changes with the deploy command: $ copilot deploy Running this command will: Build your image locally Push to your job's ECR repository Convert the manifest file to a CloudFormation template Package any additional infrastructure into the CloudFormation template Deploy your resources","title":"Deploying a Job"},{"location":"docs/concepts/jobs#whats-in-a-job","text":"Since Copilot uses CloudFormation under the hood, all the resources created are visible and tagged by Copilot. Scheduled Jobs are composed of an AmazonECS Task Definition, Task Role, Task Execution Role, a Step Function State Machine for retrying on failures, and finally an Event Rule to trigger the state machine.","title":"What's in a Job?"},{"location":"docs/concepts/overview","text":"Concepts Copilot makes it super easy to set up and deploy your containers on AWS - but getting started is only the first step of the journey. What happens when you want to have one copy of your service running only for testing and another copy serving production traffic? What happens when you want to add another service? How do you manage deploying to all of these services? Copilot wants to help you with all of these things so let's jump into some of Copilot's core concepts to understand how they can help. Applications An Application is a collection of services and environments. When you get started with Copilot, the first thing you'll be asked to do is choose an application name. This can be a high level description of the product you're trying to build. An example might be an application named \"chat\" which has two services \"frontend\" and \"api\" . These two services could then be deployed to a \"test\" and \"production\" environment. Environments Rumor has it, there are people out there that can write perfect code on the first go without any bugs. While we tip our hats to those folks, we believe it's important to be able to test new code on a non-customer facing version of your service before promoting to production. In Copilot we do this by using environments . Each environment can have its own version of a service running allowing you to create a \"test\" and \"production\" environment. You can deploy your service to the test environment, make sure everything looks good, then deploy to your production environment. Since each environment is independent, if you deploy a bug to your test environment, customers using a service deployed to your production environment will be fine. Until now we've been talking about just one service, but what happens when you want to add another service? Perhaps you want to add a backend service to complement your frontend service. Each environment contains a set of resources shared between all the deployed services - these resources include the network (VPC, Subnets, Security Groups, etc...), the ECS Cluster, and the load balancer. If you deploy both your frontend and backend service to your test environment, both services will share the same network and cluster. Services A service is your code and all of the supporting infrastructure needed to get it up and running on AWS. When you first get started setting up a service, Copilot will ask you what type of service you want to create. The type of service determines the infrastructure that'll be created to support your code. If you want your code to serve traffic from the internet, for example, Copilot can set up an Application Load Balancer and an Amazon ECS Service running on AWS Fargate. Once you've told Copilot what type of service you're building, Copilot will take care of building your code's Dockerfile and storing the images securely in an Amazon ECR repository. Copilot will also create a simple file called the manifest which contains all the knobs and toggles for your service. This includes things like how much memory and CPU should be allocated to each copy of your service, how many copies of your service you want running, and more. Jobs Jobs are ephemeral Amazon ECS tasks that are triggered by an event. Once their work is done, the task terminates. Just like services, Copilot will ask you all the necessary information to quickly get going with a scheduled task on AWS. The manifest file can always be used to adjust the configuration and provide more advanced settings. Pipelines Now that you've got an application with a few services deployed to a couple of environments, staying on top of those deployments can become tricky. Copilot can help by setting up a release pipeline that deploys your service whenever you push to your git repository. (At this time, Copilot supports GitHub, Bitbucket, and CodeCommit repositories.) When a push is detected, your pipeline will build your service, push the image to ECR, and deploy to your environments. A common pattern is to set up a pipeline for a particular service that deploys to a test environment, runs automated testing, then deploys to the production environment.","title":"Overview"},{"location":"docs/concepts/overview#concepts","text":"Copilot makes it super easy to set up and deploy your containers on AWS - but getting started is only the first step of the journey. What happens when you want to have one copy of your service running only for testing and another copy serving production traffic? What happens when you want to add another service? How do you manage deploying to all of these services? Copilot wants to help you with all of these things so let's jump into some of Copilot's core concepts to understand how they can help.","title":"Concepts"},{"location":"docs/concepts/overview#applications","text":"An Application is a collection of services and environments. When you get started with Copilot, the first thing you'll be asked to do is choose an application name. This can be a high level description of the product you're trying to build. An example might be an application named \"chat\" which has two services \"frontend\" and \"api\" . These two services could then be deployed to a \"test\" and \"production\" environment.","title":"Applications"},{"location":"docs/concepts/overview#environments","text":"Rumor has it, there are people out there that can write perfect code on the first go without any bugs. While we tip our hats to those folks, we believe it's important to be able to test new code on a non-customer facing version of your service before promoting to production. In Copilot we do this by using environments . Each environment can have its own version of a service running allowing you to create a \"test\" and \"production\" environment. You can deploy your service to the test environment, make sure everything looks good, then deploy to your production environment. Since each environment is independent, if you deploy a bug to your test environment, customers using a service deployed to your production environment will be fine. Until now we've been talking about just one service, but what happens when you want to add another service? Perhaps you want to add a backend service to complement your frontend service. Each environment contains a set of resources shared between all the deployed services - these resources include the network (VPC, Subnets, Security Groups, etc...), the ECS Cluster, and the load balancer. If you deploy both your frontend and backend service to your test environment, both services will share the same network and cluster.","title":"Environments"},{"location":"docs/concepts/overview#services","text":"A service is your code and all of the supporting infrastructure needed to get it up and running on AWS. When you first get started setting up a service, Copilot will ask you what type of service you want to create. The type of service determines the infrastructure that'll be created to support your code. If you want your code to serve traffic from the internet, for example, Copilot can set up an Application Load Balancer and an Amazon ECS Service running on AWS Fargate. Once you've told Copilot what type of service you're building, Copilot will take care of building your code's Dockerfile and storing the images securely in an Amazon ECR repository. Copilot will also create a simple file called the manifest which contains all the knobs and toggles for your service. This includes things like how much memory and CPU should be allocated to each copy of your service, how many copies of your service you want running, and more.","title":"Services"},{"location":"docs/concepts/overview#jobs","text":"Jobs are ephemeral Amazon ECS tasks that are triggered by an event. Once their work is done, the task terminates. Just like services, Copilot will ask you all the necessary information to quickly get going with a scheduled task on AWS. The manifest file can always be used to adjust the configuration and provide more advanced settings.","title":"Jobs"},{"location":"docs/concepts/overview#pipelines","text":"Now that you've got an application with a few services deployed to a couple of environments, staying on top of those deployments can become tricky. Copilot can help by setting up a release pipeline that deploys your service whenever you push to your git repository. (At this time, Copilot supports GitHub, Bitbucket, and CodeCommit repositories.) When a push is detected, your pipeline will build your service, push the image to ECR, and deploy to your environments. A common pattern is to set up a pipeline for a particular service that deploys to a test environment, runs automated testing, then deploys to the production environment.","title":"Pipelines"},{"location":"docs/concepts/pipelines","text":"Having an automated release process is one of the most important parts of software delivery, so Copilot wants to make setting up that process as easy as possible \ud83d\ude80. In this section, we'll talk about using Copilot to set up a CodePipeline that automatically builds your service code when you push to your GitHub, Bitbucket or AWS CodeCommit repository, deploys to your environments, and runs automated testing. Why? We won't get too philosophical about releasing software, but what's the point of having a release pipeline? With copilot deploy you can deploy your service directly from your computer to ECS, so why add a middleman? That's a great question. For some apps, manually using deploy is enough, but as your release process gets more complicated (as you add more environments or add automated testing, for example) you want to offload the boring work of repeatedly orchestrating that process to a service. With two services, each having two environments (test and production, say), running integration tests after you deploy to your test environment becomes surprisingly cumbersome to do by hand. Using an automated release tool like CodePipeline helps make your release manageable. Even if your release isn't particularly complicated, knowing that you can just git push to deploy your change always feels a little magical \ud83c\udf08. Pipeline structure Copilot can set up a CodePipeline for you with a few commands - but before we jump into that, let's talk a little bit about the structure of the pipeline we'll be generating. Our pipeline will have the following basic structure: Source Stage - when you push to a configured GitHub, Bitbucket, or CodeCommit repository branch, a new pipeline execution is triggered. Build Stage - after your source code is pulled from your repository host, your service's container image is built and published to every environment's ECR repository. Deploy Stages - after your code is built, you can deploy to any or all of your environments, with optional post-deployment tests or manual approvals. Once you've set up a CodePipeline using Copilot, all you'll have to do is push to your GitHub, Bitbucket, or CodeCommit repository, and CodePipeline will orchestrate the deployments. Want to learn more about CodePipeline? Check out their getting started docs . Creating a Pipeline in 3 steps Creating a Pipeline requires only three steps: Preparing the pipeline structure. Committing and pushing the files generated in the copilot/ directory. Creating the actual CodePipeline. Follow the three steps below, from your workspace root: $ copilot pipeline init $ git add copilot/pipeline.yml copilot/buildspec.yml copilot/.workspace && git commit -m \"Adding pipeline artifacts\" && git push $ copilot pipeline update \u2728 And you'll have a new pipeline configured in your application account. Want to understand a little bit more what's going on? Read on! Setting up a Pipeline, step by step Step 1: Configuring your Pipeline Pipeline configurations are created at a workspace level. If your workspace has a single service, then your pipeline will be triggered only for that service. However, if you have multiple services in a workspace, then the pipeline will build all the services in the workspace. To start setting up a pipeline, cd into your service(s)'s workspace and run: copilot pipeline init This won't create your pipeline, but it will create some local files that will be used when creating your pipeline. Release order : You'll be prompted for environments you want to deploy to - select them based on the order you want them to be deployed in your pipeline (deployments happen one environment at a time). You may, for example, want to deploy to your test environment first, and then your prod environment. Tracking repository : After you've selected the environments you want to deploy to, you'll be prompted to select which repository you want your CodePipeline to track. This is the repository that, when pushed to, will trigger a pipeline execution. (If the repository you're interested in doesn't show up, you can pass it in using the --url flag.) Step 2: Updating the Pipeline manifest (optional) Just like your service has a simple manifest file, so does your pipeline. After you run pipeline init , two files are created: pipeline.yml and buildspec.yml , both in your copilot/ directory. If you poke in, you'll see that the pipeline.yml looks something like this (for a service called \"api-frontend\" with two environments, \"test\" and \"prod\"): # This YAML file defines the relationship and deployment ordering of your environments. # The name of the pipeline name : pipeline-ecs-kudos-kohidave-demo-api-frontend # The version of the schema used in this template version : 1 # This section defines the source artifacts. source : # The name of the provider that is used to store the source artifacts. provider : GitHub # Additional properties that further specifies the exact location # the artifacts should be sourced from. For example, the GitHub provider # has the following properties: repository, branch. properties : branch : main repository : https://github.com/kohidave/demo-api-frontend # Optional: specify the name of an existing CodeStar Connections connection. # connection_name: a-connection # The deployment section defines the order the pipeline will deploy # to your environments. stages : - # The name of the environment to deploy to. name : test test_commands : - make test - echo \"woo! Tests passed\" - # The name of the environment to deploy to. name : prod # requires_approval: true You can see every available configuration option for pipeline.yml on the pipeline manifest page. There are 3 main parts of this file: the name field, which is the name of your CodePipeline, the source section, which details the repository and branch to track, and the stages section, which lists the environments you want this pipeline to deploy to. You can update this anytime, but you must run copilot pipeline update afterwards. Typically, you'll update this file if you add new environments you want to deploy to, or want to track a different branch. If you are using CodeStar Connections to connect to your repository and would like to utilize an existing connection rather than let Copilot generate one for you, you may add the connection name here. The pipeline manifest is also where you may add a manual approval step before deployment or commands to run tests (see \"Adding Tests,\" below) after deployment. Step 3: Updating the Buildspec (optional) Along with pipeline.yml , the pipeline init command also generated a buildspec.yml file in the copilot/ directory. This contains the instructions for building and publishing your service. If you want to run any additional commands, besides docker build , such as unit tests or style checkers, feel free to add them to the buildspec's build phase. When this buildspec runs, it pulls down the version of Copilot which was used when you ran pipeline init , to ensure backwards compatibility. Step 4: Pushing New Files to your Repository Now that your pipeline.yml , buildspec.yml , and .workspace files have been created, add them to your repository. These files in your copilot/ directory are required for your pipeline's build stage to run successfully. Step 5: Creating your Pipeline Here's the fun part! Run: copilot pipeline update This parses your pipeline.yml , creates a CodePipeline in the same account and region as your application and kicks off a pipeline execution. Log into the AWS Console to watch your pipeline go, or run copilot pipeline status to check in on its execution. Info If you have selected a GitHub or Bitbucket repository, Copilot will help you connect to your source code with CodeStar Connections . You will need to install the AWS authentication app on your third-party account and update the connection status. Copilot and the AWS Management Console will guide you through these steps. Adding Tests Of course, one of the most important parts of a pipeline is the automated testing. To add tests, such as integration or end-to-end tests, that run after a deployment stage, include those commands in the test_commands section. If all the tests succeed, your change is promoted to the next stage. Adding test_commands generates a CodeBuild project with the aws/codebuild/amazonlinux2-x86_64-standard:3.0 image - so most commands from Amazon Linux 2 (including make ) are available for use. Are your tests configured to run inside a Docker container? Copilot's test commands CodeBuild project supports Docker, so docker build commands are available as well. In the example below, the pipeline will run the make test command (in your source code directory) and only promote the change to the prod stage if that command exits successfully. name : pipeline-ecs-kudos-kohidave-demo-api-frontend version : 1 source : provider : GitHub properties : branch : main repository : https://github.com/kohidave/demo-api-frontend stages : - name : test # A change will only deploy to the production stage if the # make test and echo commands exit successfully. test_commands : - make test - echo \"woo! Tests passed\" - name : prod","title":"Pipelines"},{"location":"docs/concepts/pipelines#why","text":"We won't get too philosophical about releasing software, but what's the point of having a release pipeline? With copilot deploy you can deploy your service directly from your computer to ECS, so why add a middleman? That's a great question. For some apps, manually using deploy is enough, but as your release process gets more complicated (as you add more environments or add automated testing, for example) you want to offload the boring work of repeatedly orchestrating that process to a service. With two services, each having two environments (test and production, say), running integration tests after you deploy to your test environment becomes surprisingly cumbersome to do by hand. Using an automated release tool like CodePipeline helps make your release manageable. Even if your release isn't particularly complicated, knowing that you can just git push to deploy your change always feels a little magical \ud83c\udf08.","title":"Why?"},{"location":"docs/concepts/pipelines#pipeline-structure","text":"Copilot can set up a CodePipeline for you with a few commands - but before we jump into that, let's talk a little bit about the structure of the pipeline we'll be generating. Our pipeline will have the following basic structure: Source Stage - when you push to a configured GitHub, Bitbucket, or CodeCommit repository branch, a new pipeline execution is triggered. Build Stage - after your source code is pulled from your repository host, your service's container image is built and published to every environment's ECR repository. Deploy Stages - after your code is built, you can deploy to any or all of your environments, with optional post-deployment tests or manual approvals. Once you've set up a CodePipeline using Copilot, all you'll have to do is push to your GitHub, Bitbucket, or CodeCommit repository, and CodePipeline will orchestrate the deployments. Want to learn more about CodePipeline? Check out their getting started docs .","title":"Pipeline structure"},{"location":"docs/concepts/pipelines#creating-a-pipeline-in-3-steps","text":"Creating a Pipeline requires only three steps: Preparing the pipeline structure. Committing and pushing the files generated in the copilot/ directory. Creating the actual CodePipeline. Follow the three steps below, from your workspace root: $ copilot pipeline init $ git add copilot/pipeline.yml copilot/buildspec.yml copilot/.workspace && git commit -m \"Adding pipeline artifacts\" && git push $ copilot pipeline update \u2728 And you'll have a new pipeline configured in your application account. Want to understand a little bit more what's going on? Read on!","title":"Creating a Pipeline in 3 steps"},{"location":"docs/concepts/pipelines#setting-up-a-pipeline-step-by-step","text":"","title":"Setting up a Pipeline, step by step"},{"location":"docs/concepts/pipelines#step-1-configuring-your-pipeline","text":"Pipeline configurations are created at a workspace level. If your workspace has a single service, then your pipeline will be triggered only for that service. However, if you have multiple services in a workspace, then the pipeline will build all the services in the workspace. To start setting up a pipeline, cd into your service(s)'s workspace and run: copilot pipeline init This won't create your pipeline, but it will create some local files that will be used when creating your pipeline. Release order : You'll be prompted for environments you want to deploy to - select them based on the order you want them to be deployed in your pipeline (deployments happen one environment at a time). You may, for example, want to deploy to your test environment first, and then your prod environment. Tracking repository : After you've selected the environments you want to deploy to, you'll be prompted to select which repository you want your CodePipeline to track. This is the repository that, when pushed to, will trigger a pipeline execution. (If the repository you're interested in doesn't show up, you can pass it in using the --url flag.)","title":"Step 1: Configuring your Pipeline"},{"location":"docs/concepts/pipelines#step-2-updating-the-pipeline-manifest-optional","text":"Just like your service has a simple manifest file, so does your pipeline. After you run pipeline init , two files are created: pipeline.yml and buildspec.yml , both in your copilot/ directory. If you poke in, you'll see that the pipeline.yml looks something like this (for a service called \"api-frontend\" with two environments, \"test\" and \"prod\"): # This YAML file defines the relationship and deployment ordering of your environments. # The name of the pipeline name : pipeline-ecs-kudos-kohidave-demo-api-frontend # The version of the schema used in this template version : 1 # This section defines the source artifacts. source : # The name of the provider that is used to store the source artifacts. provider : GitHub # Additional properties that further specifies the exact location # the artifacts should be sourced from. For example, the GitHub provider # has the following properties: repository, branch. properties : branch : main repository : https://github.com/kohidave/demo-api-frontend # Optional: specify the name of an existing CodeStar Connections connection. # connection_name: a-connection # The deployment section defines the order the pipeline will deploy # to your environments. stages : - # The name of the environment to deploy to. name : test test_commands : - make test - echo \"woo! Tests passed\" - # The name of the environment to deploy to. name : prod # requires_approval: true You can see every available configuration option for pipeline.yml on the pipeline manifest page. There are 3 main parts of this file: the name field, which is the name of your CodePipeline, the source section, which details the repository and branch to track, and the stages section, which lists the environments you want this pipeline to deploy to. You can update this anytime, but you must run copilot pipeline update afterwards. Typically, you'll update this file if you add new environments you want to deploy to, or want to track a different branch. If you are using CodeStar Connections to connect to your repository and would like to utilize an existing connection rather than let Copilot generate one for you, you may add the connection name here. The pipeline manifest is also where you may add a manual approval step before deployment or commands to run tests (see \"Adding Tests,\" below) after deployment.","title":"Step 2: Updating the Pipeline manifest (optional)"},{"location":"docs/concepts/pipelines#step-3-updating-the-buildspec-optional","text":"Along with pipeline.yml , the pipeline init command also generated a buildspec.yml file in the copilot/ directory. This contains the instructions for building and publishing your service. If you want to run any additional commands, besides docker build , such as unit tests or style checkers, feel free to add them to the buildspec's build phase. When this buildspec runs, it pulls down the version of Copilot which was used when you ran pipeline init , to ensure backwards compatibility.","title":"Step 3: Updating the Buildspec (optional)"},{"location":"docs/concepts/pipelines#step-4-pushing-new-files-to-your-repository","text":"Now that your pipeline.yml , buildspec.yml , and .workspace files have been created, add them to your repository. These files in your copilot/ directory are required for your pipeline's build stage to run successfully.","title":"Step 4: Pushing New Files to your Repository"},{"location":"docs/concepts/pipelines#step-5-creating-your-pipeline","text":"Here's the fun part! Run: copilot pipeline update This parses your pipeline.yml , creates a CodePipeline in the same account and region as your application and kicks off a pipeline execution. Log into the AWS Console to watch your pipeline go, or run copilot pipeline status to check in on its execution. Info If you have selected a GitHub or Bitbucket repository, Copilot will help you connect to your source code with CodeStar Connections . You will need to install the AWS authentication app on your third-party account and update the connection status. Copilot and the AWS Management Console will guide you through these steps.","title":"Step 5: Creating your Pipeline"},{"location":"docs/concepts/pipelines#adding-tests","text":"Of course, one of the most important parts of a pipeline is the automated testing. To add tests, such as integration or end-to-end tests, that run after a deployment stage, include those commands in the test_commands section. If all the tests succeed, your change is promoted to the next stage. Adding test_commands generates a CodeBuild project with the aws/codebuild/amazonlinux2-x86_64-standard:3.0 image - so most commands from Amazon Linux 2 (including make ) are available for use. Are your tests configured to run inside a Docker container? Copilot's test commands CodeBuild project supports Docker, so docker build commands are available as well. In the example below, the pipeline will run the make test command (in your source code directory) and only promote the change to the prod stage if that command exits successfully. name : pipeline-ecs-kudos-kohidave-demo-api-frontend version : 1 source : provider : GitHub properties : branch : main repository : https://github.com/kohidave/demo-api-frontend stages : - name : test # A change will only deploy to the production stage if the # make test and echo commands exit successfully. test_commands : - make test - echo \"woo! Tests passed\" - name : prod","title":"Adding Tests"},{"location":"docs/concepts/services","text":"One of the awesome things about containers is that once you've written your code, running it locally is as easy as typing docker run . Copilot makes running those same containers on AWS as easy as typing copilot init . Copilot will build your image, push it to Amazon ECR and set up all the infrastructure to run your service in a scalable and secure way. Creating a Service Creating a service to run your containers on AWS can be done in a few ways. The easiest way is by running the init command from the same directory as your Dockerfile. $ copilot init You'll be asked which application you want this service to be a part of (or asked to create an application if there isn't one). Copilot will then ask about the type of service you're trying to build. After selecting a service type, Copilot will detect any health checks or exposed ports from your Dockerfile and ask if you'd like to deploy. Choosing a Service Type We mentioned before that Copilot will set up all the infrastructure your service needs to run. But how does it know what kind of infrastructure to use? When you're setting up a service, Copilot will ask you about what kind of service you want to build. Internet-facing services If you want your service to serve internet traffic then you have two options: * \"Request-Driven Web Service\" will provision an AWS App Runner Service to run your service. * \"Load Balanced Web Service\" will provision an Application Load Balancer, security groups, an ECS service on Fargate to run your service. Request-Driven Web Service An AWS App Runner service that autoscales your services based on incoming traffic and scales down to a baseline instance when there's no traffic. This option is more cost effective for HTTP services with sudden bursts in request volumes or low request volumes. Load Balanced Web Service An ECS Service running tasks on Fargate with an Application Load Balancer as ingress. This option is suitable for HTTP services with steady request volumes that need to access resources in a VPC or require advanced configuration. Backend Service If you want a service that can't be accessed externally, but only from other services within your application, you can create a Backend Service . Copilot will provision an ECS Service running on AWS Fargate, but won't set up any internet-facing endpoints. Config and the Manifest After you've run copilot init you might have noticed that Copilot created a file called manifest.yml in the copilot directory. This manifest file contains common configuration options for your service. While the exact set of options depends on the type of service you're running, common ones include the resources allocated to your service (like memory and CPU), health checks, and environment variables. Let's take a look at the manifest for a Load Balanced Web Service called front-end . name : front-end type : Load Balanced Web Service image : # Path to your service's Dockerfile. build : ./Dockerfile # Port exposed through your container to route traffic to it. port : 8080 http : # Requests to this path will be forwarded to your service. # To match all requests you can use the \"/\" path. path : '/' # You can specify a custom health check path. The default is \"/\" # healthcheck: '/' # Number of CPU units for the task. cpu : 256 # Amount of memory in MiB used by the task. memory : 512 # Number of tasks that should be running in your service. count : 1 # Optional fields for more advanced use-cases. # variables : # Pass environment variables as key value pairs. LOG_LEVEL : info #secrets: # Pass secrets from AWS Systems Manager (SSM) Parameter Store. # GITHUB_TOKEN: GH_SECRET_TOKEN # The key is the name of the environment variable, # the value is the name of the SSM parameter. # You can override any of the values defined above by environment. environments : prod : count : 2 # Number of tasks to run for the \"prod\" environment. To learn about the specification of manifest files, see the manifest page. Deploying a Service Once you've set up your service, you can deploy it (and any changes to your manifest) by running the deploy command: $ copilot deploy Running this command will: Build your image locally Push to your service's ECR repository Convert your manifest file to CloudFormation Package any additional infrastructure into CloudFormation Deploy your updated service and resources to CloudFormation If you have multiple environments, you'll be prompted to select which environment you want to deploy to. Digging into your Service Now that we've got a service up and running, we can check on it using Copilot. Below are a few common ways to check in on your deployed service. What's in your service? Running copilot svc show will show you a summary of your service. Here's an example of the output you might see for a load balanced web application. This output includes the configuration of your service for each environment, all the endpoints for your service, and the environment variables passed into your service. You can also provide an optional --resources flag to see all AWS resources associated with your service. $ copilot svc show About Application my-app Name front-end Type Load Balanced Web Service Configurations Environment Tasks CPU ( vCPU ) Memory ( MiB ) Port test 1 0 .25 512 80 Routes Environment URL test http://my-ap-Publi-1RV8QEBNTEQCW-1762184596.ca-central-1.elb.amazonaws.com Service Discovery Environment Namespace test front-end.my-app.local:8080 Variables Name Environment Value COPILOT_APPLICATION_NAME test my-app COPILOT_ENVIRONMENT_NAME test test COPILOT_LB_DNS test my-ap-Publi-1RV8QEBNTEQCW-1762184596.ca-central-1.elb.amazonaws.com COPILOT_SERVICE_DISCOVERY_ENDPOINT test my-app.local COPILOT_SERVICE_NAME test front-end What's your service status? Often it's handy to be able to check on the status of your service. Are all the instances of my service healthy? Are there any alarms firing? To do that, you can run copilot svc status to get a summary of your service's status. $ copilot svc status Service Status ACTIVE 1 / 1 running tasks ( 0 pending ) Last Deployment Updated At 12 minutes ago Task Definition arn:aws:ecs:ca-central-1:693652174720:task-definition/my-app-test-front-end:1 Task Status ID Image Digest Last Status Health Status Started At Stopped At 37236ed3 da3cfcdd RUNNING HEALTHY 12 minutes ago - Alarms Name Health Last Updated Reason CPU-Utilization OK 5 minutes ago - Where are my service logs? Checking the your service logs is easy as well. Running copilot svc logs will show the most recent logs of your service. You can follow your logs live with the --follow flag. $ copilot svc logs 37236ed 10 .0.0.30 \ud83d\ude91 Health-check ok! 37236ed 10 .0.0.30 \ud83d\ude91 Health-check ok! 37236ed 10 .0.0.30 \ud83d\ude91 Health-check ok! 37236ed 10 .0.0.30 \ud83d\ude91 Health-check ok! 37236ed 10 .0.0.30 \ud83d\ude91 Health-check ok!","title":"Services"},{"location":"docs/concepts/services#creating-a-service","text":"Creating a service to run your containers on AWS can be done in a few ways. The easiest way is by running the init command from the same directory as your Dockerfile. $ copilot init You'll be asked which application you want this service to be a part of (or asked to create an application if there isn't one). Copilot will then ask about the type of service you're trying to build. After selecting a service type, Copilot will detect any health checks or exposed ports from your Dockerfile and ask if you'd like to deploy.","title":"Creating a Service"},{"location":"docs/concepts/services#choosing-a-service-type","text":"We mentioned before that Copilot will set up all the infrastructure your service needs to run. But how does it know what kind of infrastructure to use? When you're setting up a service, Copilot will ask you about what kind of service you want to build.","title":"Choosing a Service Type"},{"location":"docs/concepts/services#internet-facing-services","text":"If you want your service to serve internet traffic then you have two options: * \"Request-Driven Web Service\" will provision an AWS App Runner Service to run your service. * \"Load Balanced Web Service\" will provision an Application Load Balancer, security groups, an ECS service on Fargate to run your service.","title":"Internet-facing services"},{"location":"docs/concepts/services#request-driven-web-service","text":"An AWS App Runner service that autoscales your services based on incoming traffic and scales down to a baseline instance when there's no traffic. This option is more cost effective for HTTP services with sudden bursts in request volumes or low request volumes.","title":"Request-Driven Web Service"},{"location":"docs/concepts/services#load-balanced-web-service","text":"An ECS Service running tasks on Fargate with an Application Load Balancer as ingress. This option is suitable for HTTP services with steady request volumes that need to access resources in a VPC or require advanced configuration.","title":"Load Balanced Web Service"},{"location":"docs/concepts/services#backend-service","text":"If you want a service that can't be accessed externally, but only from other services within your application, you can create a Backend Service . Copilot will provision an ECS Service running on AWS Fargate, but won't set up any internet-facing endpoints.","title":"Backend Service"},{"location":"docs/concepts/services#config-and-the-manifest","text":"After you've run copilot init you might have noticed that Copilot created a file called manifest.yml in the copilot directory. This manifest file contains common configuration options for your service. While the exact set of options depends on the type of service you're running, common ones include the resources allocated to your service (like memory and CPU), health checks, and environment variables. Let's take a look at the manifest for a Load Balanced Web Service called front-end . name : front-end type : Load Balanced Web Service image : # Path to your service's Dockerfile. build : ./Dockerfile # Port exposed through your container to route traffic to it. port : 8080 http : # Requests to this path will be forwarded to your service. # To match all requests you can use the \"/\" path. path : '/' # You can specify a custom health check path. The default is \"/\" # healthcheck: '/' # Number of CPU units for the task. cpu : 256 # Amount of memory in MiB used by the task. memory : 512 # Number of tasks that should be running in your service. count : 1 # Optional fields for more advanced use-cases. # variables : # Pass environment variables as key value pairs. LOG_LEVEL : info #secrets: # Pass secrets from AWS Systems Manager (SSM) Parameter Store. # GITHUB_TOKEN: GH_SECRET_TOKEN # The key is the name of the environment variable, # the value is the name of the SSM parameter. # You can override any of the values defined above by environment. environments : prod : count : 2 # Number of tasks to run for the \"prod\" environment. To learn about the specification of manifest files, see the manifest page.","title":"Config and the Manifest"},{"location":"docs/concepts/services#deploying-a-service","text":"Once you've set up your service, you can deploy it (and any changes to your manifest) by running the deploy command: $ copilot deploy Running this command will: Build your image locally Push to your service's ECR repository Convert your manifest file to CloudFormation Package any additional infrastructure into CloudFormation Deploy your updated service and resources to CloudFormation If you have multiple environments, you'll be prompted to select which environment you want to deploy to.","title":"Deploying a Service"},{"location":"docs/concepts/services#digging-into-your-service","text":"Now that we've got a service up and running, we can check on it using Copilot. Below are a few common ways to check in on your deployed service.","title":"Digging into your Service"},{"location":"docs/concepts/services#whats-in-your-service","text":"Running copilot svc show will show you a summary of your service. Here's an example of the output you might see for a load balanced web application. This output includes the configuration of your service for each environment, all the endpoints for your service, and the environment variables passed into your service. You can also provide an optional --resources flag to see all AWS resources associated with your service. $ copilot svc show About Application my-app Name front-end Type Load Balanced Web Service Configurations Environment Tasks CPU ( vCPU ) Memory ( MiB ) Port test 1 0 .25 512 80 Routes Environment URL test http://my-ap-Publi-1RV8QEBNTEQCW-1762184596.ca-central-1.elb.amazonaws.com Service Discovery Environment Namespace test front-end.my-app.local:8080 Variables Name Environment Value COPILOT_APPLICATION_NAME test my-app COPILOT_ENVIRONMENT_NAME test test COPILOT_LB_DNS test my-ap-Publi-1RV8QEBNTEQCW-1762184596.ca-central-1.elb.amazonaws.com COPILOT_SERVICE_DISCOVERY_ENDPOINT test my-app.local COPILOT_SERVICE_NAME test front-end","title":"What's in your service?"},{"location":"docs/concepts/services#whats-your-service-status","text":"Often it's handy to be able to check on the status of your service. Are all the instances of my service healthy? Are there any alarms firing? To do that, you can run copilot svc status to get a summary of your service's status. $ copilot svc status Service Status ACTIVE 1 / 1 running tasks ( 0 pending ) Last Deployment Updated At 12 minutes ago Task Definition arn:aws:ecs:ca-central-1:693652174720:task-definition/my-app-test-front-end:1 Task Status ID Image Digest Last Status Health Status Started At Stopped At 37236ed3 da3cfcdd RUNNING HEALTHY 12 minutes ago - Alarms Name Health Last Updated Reason CPU-Utilization OK 5 minutes ago -","title":"What's your service status?"},{"location":"docs/concepts/services#where-are-my-service-logs","text":"Checking the your service logs is easy as well. Running copilot svc logs will show the most recent logs of your service. You can follow your logs live with the --follow flag. $ copilot svc logs 37236ed 10 .0.0.30 \ud83d\ude91 Health-check ok! 37236ed 10 .0.0.30 \ud83d\ude91 Health-check ok! 37236ed 10 .0.0.30 \ud83d\ude91 Health-check ok! 37236ed 10 .0.0.30 \ud83d\ude91 Health-check ok! 37236ed 10 .0.0.30 \ud83d\ude91 Health-check ok!","title":"Where are my service logs?"},{"location":"docs/developing/additional-aws-resources","text":"Additional AWS Resources Additional AWS resources, referred to as \"addons\" in the CLI, are any additional AWS services that a service manifest does not integrate by default. For example, an addon can be a DynamoDB table, an S3 bucket, or an RDS Aurora Serverless cluster that your service needs to read or write to. How do I add an S3 bucket, a DDB Table, or an Aurora Serverless cluster? Copilot provides the following commands to help you create certain kinds of addons: storage init will create a DynamoDB table, an S3 bucket, or an Aurora Serverless cluster. You can run copilot storage init from your workspace and be guided through some questions to help you set up these resources. How to do I add other resources? For other types of addons, you can add your own custom CloudFormation templates according to the following instructions. Let's say you have a service named webhook in your workspace: . \u2514\u2500\u2500 copilot \u2514\u2500\u2500 webhook \u2514\u2500\u2500 manifest.yml And you want to add a custom DynamoDB table to webhook . Then under the webhook/ directory, create a new addons/ directory and add a CloudFormation template for your instance. . \u2514\u2500\u2500 copilot \u2514\u2500\u2500 webhook \u251c\u2500\u2500 addons \u2502 \u2514\u2500\u2500 mytable-ddb.yaml \u2514\u2500\u2500 manifest.yaml Typically each file under the addons/ directory represents a separate addon and is represented as an AWS CloudFormation (CFN) template . For example, if we want to also add an S3 bucket addon to our service then we could either run storage init or create our own custom, separate mybucket-s3.yaml file. When your service gets deployed, Copilot merges all these files into a single AWS CloudFormation template and creates a nested stack under your service's stack. What does an addon template look like? An addon template can be any valid CloudFormation template. However, by default, Copilot will pass the App , Env , and Name Parameters ; you can customize your resource properties with Conditions or Mappings if you wish to. Here are several possible ways to access Resources from your ECS task: * If you need to add additional policies to your ECS task role, you can define an IAM ManagedPolicy resource in your template that holds the permissions for your task, and then add it as an Output . The permission will be injected into your ECS Task Role. * If you need to add a security group to your ECS service, you can define a Security Group in your template, and then add it as an Output . The security group will be automatically attached to your ECS service. * If you'd like to inject a secret, you can define a Secret in your template, and then add it as an Output . The secret will be injected into your container and can be accessed as an environment variable as capital SNAKE_CASE. * If you'd like to inject any environment variable, you can create an Output for any value that you want to be injected as an environment variable to your ECS tasks. It will be injected into your container and accessed as an environment variable as capital SNAKE_CASE. Here is an example template layout for a DynamoDB table addon: # You can use any of these parameters to create conditions or mappings in your template. Parameters : App : Type : String Description : Your application's name. Env : Type : String Description : The environment name your service, job, or workflow is being deployed to. Name : Type : String Description : The name of the service, job, or workflow being deployed. Resources : # Create your resource here, such as an AWS::DynamoDB::Table: # MyTable: # Type: AWS::DynamoDB::Table # Properties: # ... # 1. In addition to your resource, if you need to access the resource from your ECS task # then you need to create an AWS::IAM::ManagedPolicy that holds the permissions for your resource. # # For example, below is a sample policy for MyTable: MyTableAccessPolicy : Type : AWS::IAM::ManagedPolicy Properties : PolicyDocument : Version : 2012-10-17 Statement : - Sid : DDBActions Effect : Allow Action : - dynamodb:BatchGet* - dynamodb:DescribeStream - dynamodb:DescribeTable - dynamodb:Get* - dynamodb:Query - dynamodb:Scan - dynamodb:BatchWrite* - dynamodb:Create* - dynamodb:Delete* - dynamodb:Update* - dynamodb:PutItem Resource : !Sub ${ MyTable.Arn} Outputs : # 2. If you want to inject a property of your resource as an environment variable to your ECS task, # then you need to define an output for it. # # For example, the output MyTableName will be injected in capital snake case, MY_TABLE_NAME, to your task. MyTableName : Description : \"The name of this DynamoDB.\" Value : !Ref MyTable # 1. You also need to output the IAM ManagedPolicy so that Copilot can inject it to your ECS task role. MyTableAccessPolicyArn : Description : \"The ARN of the ManagedPolicy to attach to the task role.\" Value : !Ref MyTableAccessPolicy On your next release, Copilot will include this template as a nested stack under your service! Info We recommend following Amazon IAM best practices while defining AWS Managed Policies for the additional resources, including: Grant least privilege to the policies defined in your addons/ directory. Use policy conditions for extra security to restrict your policies to access only the resources defined in your addons/ directory.","title":"Additional AWS Resources"},{"location":"docs/developing/additional-aws-resources#additional-aws-resources","text":"Additional AWS resources, referred to as \"addons\" in the CLI, are any additional AWS services that a service manifest does not integrate by default. For example, an addon can be a DynamoDB table, an S3 bucket, or an RDS Aurora Serverless cluster that your service needs to read or write to.","title":"Additional AWS Resources"},{"location":"docs/developing/additional-aws-resources#how-do-i-add-an-s3-bucket-a-ddb-table-or-an-aurora-serverless-cluster","text":"Copilot provides the following commands to help you create certain kinds of addons: storage init will create a DynamoDB table, an S3 bucket, or an Aurora Serverless cluster. You can run copilot storage init from your workspace and be guided through some questions to help you set up these resources.","title":"How do I add an S3 bucket, a DDB Table, or an Aurora Serverless cluster?"},{"location":"docs/developing/additional-aws-resources#how-to-do-i-add-other-resources","text":"For other types of addons, you can add your own custom CloudFormation templates according to the following instructions. Let's say you have a service named webhook in your workspace: . \u2514\u2500\u2500 copilot \u2514\u2500\u2500 webhook \u2514\u2500\u2500 manifest.yml And you want to add a custom DynamoDB table to webhook . Then under the webhook/ directory, create a new addons/ directory and add a CloudFormation template for your instance. . \u2514\u2500\u2500 copilot \u2514\u2500\u2500 webhook \u251c\u2500\u2500 addons \u2502 \u2514\u2500\u2500 mytable-ddb.yaml \u2514\u2500\u2500 manifest.yaml Typically each file under the addons/ directory represents a separate addon and is represented as an AWS CloudFormation (CFN) template . For example, if we want to also add an S3 bucket addon to our service then we could either run storage init or create our own custom, separate mybucket-s3.yaml file. When your service gets deployed, Copilot merges all these files into a single AWS CloudFormation template and creates a nested stack under your service's stack.","title":"How to do I add other resources?"},{"location":"docs/developing/additional-aws-resources#what-does-an-addon-template-look-like","text":"An addon template can be any valid CloudFormation template. However, by default, Copilot will pass the App , Env , and Name Parameters ; you can customize your resource properties with Conditions or Mappings if you wish to. Here are several possible ways to access Resources from your ECS task: * If you need to add additional policies to your ECS task role, you can define an IAM ManagedPolicy resource in your template that holds the permissions for your task, and then add it as an Output . The permission will be injected into your ECS Task Role. * If you need to add a security group to your ECS service, you can define a Security Group in your template, and then add it as an Output . The security group will be automatically attached to your ECS service. * If you'd like to inject a secret, you can define a Secret in your template, and then add it as an Output . The secret will be injected into your container and can be accessed as an environment variable as capital SNAKE_CASE. * If you'd like to inject any environment variable, you can create an Output for any value that you want to be injected as an environment variable to your ECS tasks. It will be injected into your container and accessed as an environment variable as capital SNAKE_CASE. Here is an example template layout for a DynamoDB table addon: # You can use any of these parameters to create conditions or mappings in your template. Parameters : App : Type : String Description : Your application's name. Env : Type : String Description : The environment name your service, job, or workflow is being deployed to. Name : Type : String Description : The name of the service, job, or workflow being deployed. Resources : # Create your resource here, such as an AWS::DynamoDB::Table: # MyTable: # Type: AWS::DynamoDB::Table # Properties: # ... # 1. In addition to your resource, if you need to access the resource from your ECS task # then you need to create an AWS::IAM::ManagedPolicy that holds the permissions for your resource. # # For example, below is a sample policy for MyTable: MyTableAccessPolicy : Type : AWS::IAM::ManagedPolicy Properties : PolicyDocument : Version : 2012-10-17 Statement : - Sid : DDBActions Effect : Allow Action : - dynamodb:BatchGet* - dynamodb:DescribeStream - dynamodb:DescribeTable - dynamodb:Get* - dynamodb:Query - dynamodb:Scan - dynamodb:BatchWrite* - dynamodb:Create* - dynamodb:Delete* - dynamodb:Update* - dynamodb:PutItem Resource : !Sub ${ MyTable.Arn} Outputs : # 2. If you want to inject a property of your resource as an environment variable to your ECS task, # then you need to define an output for it. # # For example, the output MyTableName will be injected in capital snake case, MY_TABLE_NAME, to your task. MyTableName : Description : \"The name of this DynamoDB.\" Value : !Ref MyTable # 1. You also need to output the IAM ManagedPolicy so that Copilot can inject it to your ECS task role. MyTableAccessPolicyArn : Description : \"The ARN of the ManagedPolicy to attach to the task role.\" Value : !Ref MyTableAccessPolicy On your next release, Copilot will include this template as a nested stack under your service! Info We recommend following Amazon IAM best practices while defining AWS Managed Policies for the additional resources, including: Grant least privilege to the policies defined in your addons/ directory. Use policy conditions for extra security to restrict your policies to access only the resources defined in your addons/ directory.","title":"What does an addon template look like?"},{"location":"docs/developing/environment-variables","text":"Environment Variables Environment variables are variables that are available to your service, based on the environment they're running in. Your service can reference them without having to define them. Environment variables are useful for when you want to pass in data to your service that's specific to a particular environment. For example, your test database name versus your production database name. Accessing environment variables is usually simply based on the language you're using. Here are some examples of getting an environment variable called DATABASE_NAME in a few different languages. Go dbName := os . Getenv ( \"DATABASE_NAME\" ) Javascript var dbName = process . env . DATABASE_NAME ; Python database_name = os . getenv ( 'DATABASE_NAME' ) What are the Default Environment Variables? By default, the AWS Copilot CLI passes in some default environment variables for your service to use. COPILOT_APPLICATION_NAME - this is the name of the application this service is running in. COPILOT_ENVIRONMENT_NAME - this is the name of the environment the service is running in (test vs prod, for example) COPILOT_SERVICE_NAME - this is the name of the current service. COPILOT_LB_DNS - this is the DNS name of the Load Balancer (if it exists) such as kudos-Publi-MC2WNHAIOAVS-588300247.us-west-2.elb.amazonaws.com . Note: if you're using a custom domain name, this value will still be the Load Balancer's DNS name. COPILOT_SERVICE_DISCOVERY_ENDPOINT - this is the endpoint to add after a service name to talk to another service in your environment via service discovery. The value is {app name}.local . For more information about service discovery, check out our Service Discovery guide . How do I add my own Environment Variables? Adding your own environment variable is easy. You can add them directly to your manifest in the variables section. The following snippet will pass a environment variable called LOG_LEVEL to your service, with the value set to debug . # in copilot/{service name}/manifest.yml variables : LOG_LEVEL : debug You can also pass in a specific value for an environment variable based on the environment. We'll follow the same example as above, by setting the log level, but overriding the value to be info in our production environment. Changes to your manifest take effect when you deploy them, so changing them locally is safe. # in copilot/{service name}/manifest.yml variables : LOG_LEVEL : debug environments : production : variables : LOG_LEVEL : info Here's a quick guide showing you how to add environment variables to your app by editing the manifest \ud83d\udc47 How do I know the name of my DynamoDB table, S3 bucket, RDS database, etc? When using the Copilot CLI to provision additional AWS resources such as DynamoDB tables, S3 buckets, databases, etc., any output values will be passed in as environment variables to your app. For more information, check out the additional resources guide .","title":"Environment Variables"},{"location":"docs/developing/environment-variables#environment-variables","text":"Environment variables are variables that are available to your service, based on the environment they're running in. Your service can reference them without having to define them. Environment variables are useful for when you want to pass in data to your service that's specific to a particular environment. For example, your test database name versus your production database name. Accessing environment variables is usually simply based on the language you're using. Here are some examples of getting an environment variable called DATABASE_NAME in a few different languages. Go dbName := os . Getenv ( \"DATABASE_NAME\" ) Javascript var dbName = process . env . DATABASE_NAME ; Python database_name = os . getenv ( 'DATABASE_NAME' )","title":"Environment Variables"},{"location":"docs/developing/environment-variables#what-are-the-default-environment-variables","text":"By default, the AWS Copilot CLI passes in some default environment variables for your service to use. COPILOT_APPLICATION_NAME - this is the name of the application this service is running in. COPILOT_ENVIRONMENT_NAME - this is the name of the environment the service is running in (test vs prod, for example) COPILOT_SERVICE_NAME - this is the name of the current service. COPILOT_LB_DNS - this is the DNS name of the Load Balancer (if it exists) such as kudos-Publi-MC2WNHAIOAVS-588300247.us-west-2.elb.amazonaws.com . Note: if you're using a custom domain name, this value will still be the Load Balancer's DNS name. COPILOT_SERVICE_DISCOVERY_ENDPOINT - this is the endpoint to add after a service name to talk to another service in your environment via service discovery. The value is {app name}.local . For more information about service discovery, check out our Service Discovery guide .","title":"What are the Default Environment Variables?"},{"location":"docs/developing/environment-variables#how-do-i-add-my-own-environment-variables","text":"Adding your own environment variable is easy. You can add them directly to your manifest in the variables section. The following snippet will pass a environment variable called LOG_LEVEL to your service, with the value set to debug . # in copilot/{service name}/manifest.yml variables : LOG_LEVEL : debug You can also pass in a specific value for an environment variable based on the environment. We'll follow the same example as above, by setting the log level, but overriding the value to be info in our production environment. Changes to your manifest take effect when you deploy them, so changing them locally is safe. # in copilot/{service name}/manifest.yml variables : LOG_LEVEL : debug environments : production : variables : LOG_LEVEL : info Here's a quick guide showing you how to add environment variables to your app by editing the manifest \ud83d\udc47","title":"How do I add my own Environment Variables?"},{"location":"docs/developing/environment-variables#how-do-i-know-the-name-of-my-dynamodb-table-s3-bucket-rds-database-etc","text":"When using the Copilot CLI to provision additional AWS resources such as DynamoDB tables, S3 buckets, databases, etc., any output values will be passed in as environment variables to your app. For more information, check out the additional resources guide .","title":"How do I know the name of my DynamoDB table, S3 bucket, RDS database, etc?"},{"location":"docs/developing/secrets","text":"Secrets Secrets are sensitive bits of information like OAuth tokens, secret keys or API keys - information that you need in your application code, but shouldn't commit to your source code. In the AWS Copilot CLI, secrets are passed in as environment variables (read more about developing with environment variables ), but they're treated differently due to their sensitive nature. How do I add Secrets? Adding secrets currently requires you to store your secret as a secure string in AWS Systems Manager Parameter Store (SSM), then add a reference to the SSM parameter to your manifest . We'll walk through an example where we want to store a secret called GH_WEBHOOK_SECRET with the value secretvalue1234 . First, store the secret in SSM like so: aws ssm put-parameter --name GH_WEBHOOK_SECRET --value secretvalue1234 --type SecureString \\ --tags Key = copilot-environment,Value = ${ ENVIRONMENT_NAME } Key = copilot-application,Value = ${ APP_NAME } This will store the value secretvalue1234 into the SSM parameter GH_WEBHOOK_SECRET as a secret. Attention Copilot requires the copilot-application and copilot-environment tags to limit access to this secret. It's important to replace the ${ENVIRONMENT_NAME} and ${APP_NAME} with the Copilot application and environment you want to have access to this secret. Next, we'll modify our manifest file to pass in this value: secrets : GITHUB_WEBHOOK_SECRET : GH_WEBHOOK_SECRET Once we deploy this update to our manifest, we'll be able to access the environment variable GITHUB_WEBHOOK_SECRET which will have the value of the SSM parameter GH_WEBHOOK_SECRET , secretvalue1234 . This works because ECS Agent will resolve the SSM parameter when it starts up your task, and set the environment variable for you. Attention Secrets are not supported for Request-Driven Web Services. Info We're going to make this easier! There are a couple of caveats - you have to store the secret in the same environment as your application. Some of our next work is to add a secrets command that lets you add a secret without having to worry about which environment you're in, or how SSM works.","title":"Secrets"},{"location":"docs/developing/secrets#secrets","text":"Secrets are sensitive bits of information like OAuth tokens, secret keys or API keys - information that you need in your application code, but shouldn't commit to your source code. In the AWS Copilot CLI, secrets are passed in as environment variables (read more about developing with environment variables ), but they're treated differently due to their sensitive nature.","title":"Secrets"},{"location":"docs/developing/secrets#how-do-i-add-secrets","text":"Adding secrets currently requires you to store your secret as a secure string in AWS Systems Manager Parameter Store (SSM), then add a reference to the SSM parameter to your manifest . We'll walk through an example where we want to store a secret called GH_WEBHOOK_SECRET with the value secretvalue1234 . First, store the secret in SSM like so: aws ssm put-parameter --name GH_WEBHOOK_SECRET --value secretvalue1234 --type SecureString \\ --tags Key = copilot-environment,Value = ${ ENVIRONMENT_NAME } Key = copilot-application,Value = ${ APP_NAME } This will store the value secretvalue1234 into the SSM parameter GH_WEBHOOK_SECRET as a secret. Attention Copilot requires the copilot-application and copilot-environment tags to limit access to this secret. It's important to replace the ${ENVIRONMENT_NAME} and ${APP_NAME} with the Copilot application and environment you want to have access to this secret. Next, we'll modify our manifest file to pass in this value: secrets : GITHUB_WEBHOOK_SECRET : GH_WEBHOOK_SECRET Once we deploy this update to our manifest, we'll be able to access the environment variable GITHUB_WEBHOOK_SECRET which will have the value of the SSM parameter GH_WEBHOOK_SECRET , secretvalue1234 . This works because ECS Agent will resolve the SSM parameter when it starts up your task, and set the environment variable for you. Attention Secrets are not supported for Request-Driven Web Services. Info We're going to make this easier! There are a couple of caveats - you have to store the secret in the same environment as your application. Some of our next work is to add a secrets command that lets you add a secret without having to worry about which environment you're in, or how SSM works.","title":"How do I add Secrets?"},{"location":"docs/developing/service-discovery","text":"Service Discovery Service Discovery is a way of letting services discover and connect with each other. Typically, services can only talk to each other if they expose a public endpoint - and even then, requests will have to go over the internet. With ECS Service Discovery , each service you create is given a private address and DNS name - meaning each service can talk to another without ever leaving the local network (VPC) and without exposing a public endpoint. How do I use Service Discovery? Service Discovery is enabled for all services set up using the Copilot CLI. We'll show you how to use it by using an example. Imagine we have an app called kudos and two services: api and front-end . Attention Service Discovery is not supported for Request-Driven Web Services. In this example we'll imagine our front-end service has a public endpoint and wants to call our api service using its service discovery endpoint. // Calling our api service from the front-end service using Service Discovery func ServiceDiscoveryGet ( w http . ResponseWriter , req * http . Request , ps httprouter . Params ) { endpoint := fmt . Sprintf ( \"http://api.%s/some-request\" , os . Getenv ( \"COPILOT_SERVICE_DISCOVERY_ENDPOINT\" )) resp , err := http . Get ( endpoint /* http://api.kudos.local/some-request */ ) if err != nil { http . Error ( w , err . Error (), http . StatusInternalServerError ) return } defer resp . Body . Close () body , _ := ioutil . ReadAll ( resp . Body ) w . WriteHeader ( http . StatusOK ) w . Write ( body ) } The important part is that our front-end service is making a request to our api service through a special endpoint: endpoint := fmt . Sprintf ( \"http://api.%s/some-request\" , os . Getenv ( \"COPILOT_SERVICE_DISCOVERY_ENDPOINT\" )) COPILOT_SERVICE_DISCOVERY_ENDPOINT is a special environment variable that the Copilot CLI sets for you when it creates your service. It's of the format {app name}.local - so in this case in our kudos app, the request would be to http://api.kudos.local/some-request . Since our api service is running on port 80, we're not specifying the port in the URL. However, if it was running on another port, say 8080, we'd need to include the port in the request, as well http://api.kudos.local:8080/some-request . When our front-end makes this request, the endpoint api.kudos.local resolves to a private IP address and is routed privately within your VPC.","title":"Service Discovery"},{"location":"docs/developing/service-discovery#service-discovery","text":"Service Discovery is a way of letting services discover and connect with each other. Typically, services can only talk to each other if they expose a public endpoint - and even then, requests will have to go over the internet. With ECS Service Discovery , each service you create is given a private address and DNS name - meaning each service can talk to another without ever leaving the local network (VPC) and without exposing a public endpoint.","title":"Service Discovery"},{"location":"docs/developing/service-discovery#how-do-i-use-service-discovery","text":"Service Discovery is enabled for all services set up using the Copilot CLI. We'll show you how to use it by using an example. Imagine we have an app called kudos and two services: api and front-end . Attention Service Discovery is not supported for Request-Driven Web Services. In this example we'll imagine our front-end service has a public endpoint and wants to call our api service using its service discovery endpoint. // Calling our api service from the front-end service using Service Discovery func ServiceDiscoveryGet ( w http . ResponseWriter , req * http . Request , ps httprouter . Params ) { endpoint := fmt . Sprintf ( \"http://api.%s/some-request\" , os . Getenv ( \"COPILOT_SERVICE_DISCOVERY_ENDPOINT\" )) resp , err := http . Get ( endpoint /* http://api.kudos.local/some-request */ ) if err != nil { http . Error ( w , err . Error (), http . StatusInternalServerError ) return } defer resp . Body . Close () body , _ := ioutil . ReadAll ( resp . Body ) w . WriteHeader ( http . StatusOK ) w . Write ( body ) } The important part is that our front-end service is making a request to our api service through a special endpoint: endpoint := fmt . Sprintf ( \"http://api.%s/some-request\" , os . Getenv ( \"COPILOT_SERVICE_DISCOVERY_ENDPOINT\" )) COPILOT_SERVICE_DISCOVERY_ENDPOINT is a special environment variable that the Copilot CLI sets for you when it creates your service. It's of the format {app name}.local - so in this case in our kudos app, the request would be to http://api.kudos.local/some-request . Since our api service is running on port 80, we're not specifying the port in the URL. However, if it was running on another port, say 8080, we'd need to include the port in the request, as well http://api.kudos.local:8080/some-request . When our front-end makes this request, the endpoint api.kudos.local resolves to a private IP address and is routed privately within your VPC.","title":"How do I use Service Discovery?"},{"location":"docs/developing/sidecars","text":"Sidecars Sidecars are additional containers that run along side the main container. They are usually used to perform peripheral tasks such as logging, configuration, or proxying requests. AWS also provides some plugin options that can be seamlessly incorporated with your ECS service, including but not limited to FireLens , AWS X-Ray , and AWS App Mesh . If you have defined an EFS volume for your main container through the storage field in the manifest, you can also mount that volume in any sidecar containers you have defined. How to add sidecars with Copilot? There are two ways of adding sidecars using the Copilot manifest: by specifying general sidecars or by using sidecar patterns . Attention Sidecars are not supported for Request-Driven Web Services General sidecars You'll need to provide the URL for the sidecar image. Optionally, you can specify the port you'd like to expose and the credential parameter for private registry . sidecars : <sidecar name> : # Port of the container to expose. (Optional) port : <port number> # Image URL for the sidecar container. (Required) image : <image url> # ARN of the secret containing the private repository credentials. (Optional) credentialsParameter : <credential> # Environment variables for the sidecar container. variables : <env var> # Secrets to expose to the sidecar container. secrets : <secret> # Mount paths for EFS volumes specified at the service level. (Optional) mount_points : - # Source volume to mount in this sidecar. (Required) source_volume : <named volume> # The path inside the sidecar container at which to mount the volume. (Required) path : <path> # Whether to allow the sidecar read-only access to the volume. (Default true) read_only : <bool> # Optional Docker labels to apply to this container. labels : { label key } : <label value> Below is an example of specifying the nginx sidecar container in a load balanced web service manifest. name : api type : Load Balanced Web Service image : build : api/Dockerfile port : 3000 http : path : 'api' healthcheck : '/api/health-check' # Target container for Load Balancer is our sidecar 'nginx', instead of the service container. targetContainer : 'nginx' cpu : 256 memory : 512 count : 1 sidecars : nginx : port : 80 image : 1234567890.dkr.ecr.us-west-2.amazonaws.com/reverse-proxy:revision_1 variables : NGINX_PORT : 80 Below is a fragment of a manifest including an EFS volume in both the service and sidecar container. storage : volumes : myEFSVolume : path : '/etc/mount1' read_only : false efs : id : fs-1234567 sidecars : nginx : port : 80 image : 1234567890.dkr.ecr.us-west-2.amazonaws.com/reverse-proxy:revision_1 variables : NGINX_PORT : 80 mount_points : - source_volume : myEFSVolume path : '/etc/mount1' Sidecar patterns Sidecar patterns are predefined Copilot sidecar configurations. For now, the only supported pattern is FireLens, but we'll add more in the future! # In the manifest. logging : # The Fluent Bit image. (Optional, we'll use \"amazon/aws-for-fluent-bit:latest\" by default) image : <image URL> # The configuration options to send to the FireLens log driver. (Optional) destination : <config key> : <config value> # Whether to include ECS metadata in logs. (Optional, default to true) enableMetadata : <true|false> # Secret to pass to the log configuration. (Optional) secretOptions : <key> : <value> # The full config file path in your custom Fluent Bit image. configFilePath : <config file path> For example: logging : destination : Name : cloudwatch region : us-west-2 log_group_name : /copilot/sidecar-test-hello log_stream_prefix : copilot/ You might need to add necessary permissions to the task role so that FireLens can forward your data. You can add permissions by specifying them in your addons . For example: Resources : FireLensPolicy : Type : AWS::IAM::ManagedPolicy Properties : PolicyDocument : Version : 2012-10-17 Statement : - Effect : Allow Action : - logs:CreateLogStream - logs:CreateLogGroup - logs:DescribeLogStreams - logs:PutLogEvents Resource : \"<resource ARN>\" Outputs : FireLensPolicyArn : Description : An addon ManagedPolicy gets used by the ECS task role Value : !Ref FireLensPolicy Info Since the FireLens log driver can route your main container's logs to various destinations, the svc logs command can track them only when they are sent to the log group we create for your Copilot service in CloudWatch. Info We're going to make this easier and more powerful! Currently, we only support using remote images for sidecars, which means users need to build and push their local sidecar images. But we are planning to support using local images or Dockerfiles. Additionally, FireLens will be able to route logs for the other sidecars (not just the main container).","title":"Sidecars"},{"location":"docs/developing/sidecars#sidecars","text":"Sidecars are additional containers that run along side the main container. They are usually used to perform peripheral tasks such as logging, configuration, or proxying requests. AWS also provides some plugin options that can be seamlessly incorporated with your ECS service, including but not limited to FireLens , AWS X-Ray , and AWS App Mesh . If you have defined an EFS volume for your main container through the storage field in the manifest, you can also mount that volume in any sidecar containers you have defined.","title":"Sidecars"},{"location":"docs/developing/sidecars#how-to-add-sidecars-with-copilot","text":"There are two ways of adding sidecars using the Copilot manifest: by specifying general sidecars or by using sidecar patterns . Attention Sidecars are not supported for Request-Driven Web Services","title":"How to add sidecars with Copilot?"},{"location":"docs/developing/sidecars#general-sidecars","text":"You'll need to provide the URL for the sidecar image. Optionally, you can specify the port you'd like to expose and the credential parameter for private registry . sidecars : <sidecar name> : # Port of the container to expose. (Optional) port : <port number> # Image URL for the sidecar container. (Required) image : <image url> # ARN of the secret containing the private repository credentials. (Optional) credentialsParameter : <credential> # Environment variables for the sidecar container. variables : <env var> # Secrets to expose to the sidecar container. secrets : <secret> # Mount paths for EFS volumes specified at the service level. (Optional) mount_points : - # Source volume to mount in this sidecar. (Required) source_volume : <named volume> # The path inside the sidecar container at which to mount the volume. (Required) path : <path> # Whether to allow the sidecar read-only access to the volume. (Default true) read_only : <bool> # Optional Docker labels to apply to this container. labels : { label key } : <label value> Below is an example of specifying the nginx sidecar container in a load balanced web service manifest. name : api type : Load Balanced Web Service image : build : api/Dockerfile port : 3000 http : path : 'api' healthcheck : '/api/health-check' # Target container for Load Balancer is our sidecar 'nginx', instead of the service container. targetContainer : 'nginx' cpu : 256 memory : 512 count : 1 sidecars : nginx : port : 80 image : 1234567890.dkr.ecr.us-west-2.amazonaws.com/reverse-proxy:revision_1 variables : NGINX_PORT : 80 Below is a fragment of a manifest including an EFS volume in both the service and sidecar container. storage : volumes : myEFSVolume : path : '/etc/mount1' read_only : false efs : id : fs-1234567 sidecars : nginx : port : 80 image : 1234567890.dkr.ecr.us-west-2.amazonaws.com/reverse-proxy:revision_1 variables : NGINX_PORT : 80 mount_points : - source_volume : myEFSVolume path : '/etc/mount1'","title":"General sidecars"},{"location":"docs/developing/sidecars#sidecar-patterns","text":"Sidecar patterns are predefined Copilot sidecar configurations. For now, the only supported pattern is FireLens, but we'll add more in the future! # In the manifest. logging : # The Fluent Bit image. (Optional, we'll use \"amazon/aws-for-fluent-bit:latest\" by default) image : <image URL> # The configuration options to send to the FireLens log driver. (Optional) destination : <config key> : <config value> # Whether to include ECS metadata in logs. (Optional, default to true) enableMetadata : <true|false> # Secret to pass to the log configuration. (Optional) secretOptions : <key> : <value> # The full config file path in your custom Fluent Bit image. configFilePath : <config file path> For example: logging : destination : Name : cloudwatch region : us-west-2 log_group_name : /copilot/sidecar-test-hello log_stream_prefix : copilot/ You might need to add necessary permissions to the task role so that FireLens can forward your data. You can add permissions by specifying them in your addons . For example: Resources : FireLensPolicy : Type : AWS::IAM::ManagedPolicy Properties : PolicyDocument : Version : 2012-10-17 Statement : - Effect : Allow Action : - logs:CreateLogStream - logs:CreateLogGroup - logs:DescribeLogStreams - logs:PutLogEvents Resource : \"<resource ARN>\" Outputs : FireLensPolicyArn : Description : An addon ManagedPolicy gets used by the ECS task role Value : !Ref FireLensPolicy Info Since the FireLens log driver can route your main container's logs to various destinations, the svc logs command can track them only when they are sent to the log group we create for your Copilot service in CloudWatch. Info We're going to make this easier and more powerful! Currently, we only support using remote images for sidecars, which means users need to build and push their local sidecar images. But we are planning to support using local images or Dockerfiles. Additionally, FireLens will be able to route logs for the other sidecars (not just the main container).","title":"Sidecar patterns"},{"location":"docs/developing/storage","text":"Storage There are two ways to add persistence to Copilot workloads: using copilot storage init to create databases and S3 buckets; and attaching an existing EFS filesystem using the storage field in the manifest. Database and Artifacts To add a database or S3 bucket to your job or service, simply run copilot storage init . # For a guided experience. $ copilot storage init -t S3 # To create a bucket named \"my-bucket\" accessible by the \"api\" service. $ copilot storage init -n my-bucket -t S3 -w api The above command will create the Cloudformation template for an S3 bucket in the addons directory for the \"api\" service. The next time you run copilot deploy -n api , the bucket will be created, permission to access it will be added to the api task role, and the name of the bucket will be injected into the api container under the environment variable MY_BUCKET_NAME . Info All names are converted into SCREAMING_SNAKE_CASE based on their use of hyphens or underscores. You can view the environment variables for a given service by running copilot svc show . You can also create a DynamoDB table using copilot storage init . For example, to create the Cloudformation template for a table with a sort key and a local secondary index, you could run the following command. # For a guided experience. $ copilot storage init -t DynamoDB # Or skip the prompts by providing flags. $ copilot storage init -n users -t DynamoDB -w api --partition-key id:N --sort-key email:S --lsi post-count:N This will create a DynamoDB table called ${app}-${env}-${svc}-users . Its partition key will be id , a Number attribute; its sort key will be email , a String attribute; and it will have a local secondary index (essentially an alternate sort key) on the Number attribute post-count . It is also possible to create an RDS Aurora Serverless cluster using copilot storage init . # For a guided experience. $ copilot storage init -t Aurora # Or skip the prompts by providing flags. $ copilot storage init -n my-cluster -t Aurora -w api --engine PostgreSQL --initial-db my_db This will create an RDS Aurora Serverless cluster that uses PostgreSQL engine with a database named my_db . An environment variable named MYCLUSTER_SECRET is injected into your workload as a JSON string. The fields are 'host' , 'port' , 'dbname' , 'username' , 'password' , 'dbClusterIdentifier' and 'engine' . File Systems There are two ways to use an EFS file system with Copilot: using managed EFS, and importing your own filesystem. Managed EFS The easiest way to get started using EFS for service- or job-level storage is via Copilot's built-in managed EFS capability. To get started, simply enable the efs key in the manifest under your volume's name. name : frontend storage : volumes : myManagedEFSVolume : efs : true path : /var/efs read_only : false This manifest will result in an EFS volume being created at the environment level, with an Access Point and dedicated directory at the path /frontend in the EFS filesystem created specifically for your service. Your container will be able to access this directory and all its subdirectories at the /var/efs path in its own filesystem. The /frontend directory and EFS filesystem will persist until you delete your environment. The use of an access point for each service ensures that no two services can access each other's data unless you specifically intend for them to do so by specifying the full advanced configuration. You can read more in Advanced Use Cases . You can also customize the UID and GID used for the access point by specifying the uid and gid fields in advanced EFS configuration. If you do not specify a UID or GID, Copilot picks a pseudorandom UID and GID for the access point based on the CRC32 checksum of the service's name. storage : volumes : myManagedEFSVolume : efs : uid : 1000 gid : 10000 path : /var/efs read_only : false uid and gid may not be specified with any other advanced EFS configuration. Under the Hood When you enable managed EFS, Copilot creates the following resources at the environment level: An EFS file system . Mount targets in each of your environment's private subnets Security group rules allowing the Environment Security Group to access the mount targets. At the service level, Copilot creates: An EFS Access Point . The Access Point refers to a directory created by CFN named after the service or job you wish to use EFS with. You can see the environment-level resources created by calling copilot env show --json --resources and parsing the output with your favorite command line JSON processor. For example: > copilot env show -n test --json --resources | jq '.resources[] | select( .type | contains(\"EFS\") )' Advanced Use Cases Hydrating a Managed EFS Volume Sometimes, you may wish to populate the created EFS volume with data before your service begins accepting traffic. There are several ways you can do this, depending on your main container's requirements and whether it requires this data for startup. Using a Sidecar You can mount the created EFS volume in a sidecar using the mount_points field, and use your sidecar's COMMAND or ENTRYPOINT directives to copy data from the sidecar's filesystem or pull data down from S3 or another cloud service. If you mark the sidecar as nonessential with essential:false , it will start, do its work, and exit as the service containers come up and stabilize. This may not be suitable for workloads which depend on the correct data being present in the EFS volume. Using copilot svc exec For workloads where data must be present prior to your task containers coming up, we recommend using a placeholder container first. For example, deploy your service with the following values in the manifest: image : location : amazon/amazon-ecs-sample exec : true storage : volumes : myVolume : efs : true path : /var/efs read_only : false Then, when your service is stable, run: $ copilot svc exec This will open an interactive shell from which you can add packages like curl or wget , download data from the internet, create a directory structure, etc. Info This method of configuring containers is not recommended for production environments; containers are ephemeral and if you wish for a piece of software to be present in your service containers, be sure to add it using the RUN directive in a Dockerfile. When you have populated the directory, modify your manifest to remove the exec directive and update the build field to your desired Docker build config or image location. image : build : ./Dockerfile storage : volumes : myVolume : efs : true path : /var/efs read_only : false External EFS Mounting an externally-managed EFS volume in Copilot tasks requires two things: That you create an EFS file system in the desired environment's region. That you create an EFS Mount Target using the Copilot environment security group in each subnet of your environment. When those prerequisites are satisfied, you can enable EFS storage using simple syntax in your manifest. You'll need the filesystem ID and, if using, the access point configuration for the filesystem. Info You can only use a given EFS file system in a single environment at a time. Mount targets are limited to one per availability zone; therefore, you must delete any existing mount targets before bringing the file system to Copilot if you have used it in another VPC. Manifest Syntax The simplest possible EFS volume can be specified with the following syntax: storage : volumes : myEFSVolume : # This is a variable key and can be set to an arbitrary string. path : '/etc/mount1' efs : id : fs-1234567 This will create a read-only mounted volume in your service's or job's container using the filesystem fs-1234567 . If mount targets are not created in the subnets of the environment, the task will fail to launch. Full syntax for external EFS volumes follows. storage : volumes : <volume name> : path : <mount path> # Required. The path inside the container. read_only : <boolean> # Default: true efs : id : <filesystem ID> # Required. root_dir : <filesystem root> # Optional. Defaults to \"/\". Must not be # specified if using access points. auth : iam : <boolean> # Optional. Whether to use IAM authorization when # mounting this filesystem. access_point_id : <access point ID> # Optional. The ID of the EFS Access Point # to use when mounting this filesystem. uid : <uint32> # Optional. UID for managed EFS access point. gid : <uint32> # Optional. GID for managed EFS access point. Cannot be specified # with `id`, `root_dir`, or `auth`. Creating Mount Targets There are several ways to create mount targets for an existing EFS filesystem: using the AWS CLI and using CloudFormation . With the AWS CLI To create mount targets for an existing filesystem, you'll need the ID of that filesystem. a Copilot environment deployed in the same account and region. To retrieve the filesystem ID, you can use the AWS CLI: $ EFS_FILESYSTEMS = $( aws efs describe-file-systems | \\ jq '.FileSystems[] | {ID: .FileSystemId, CreationTime: .CreationTime, Size: .SizeInBytes.Value}' ) If you echo this variable you should be able to find which filesystem you need. Assign it to the variable $EFS_ID and continue. You'll also need the public subnets of the Copilot environment and the Environment Security Group. This jq command will filter the output of the describe-stacks call down to simply the desired output value. Info The filesystem you use MUST be in the same region as your Copilot environment! $ SUBNETS = $( aws cloudformation describe-stacks --stack-name ${ YOUR_APP } - ${ YOUR_ENV } \\ | jq '.Stacks[] | .Outputs[] | select(.OutputKey == \"PublicSubnets\") | .OutputValue' ) $ SUBNET1 = $( echo $SUBNETS | jq -r 'split(\",\") | .[0]' ) $ SUBNET2 = $( echo $SUBNETS | jq -r 'split(\",\") | .[1]' ) $ ENV_SG = $( aws cloudformation describe-stacks --stack-name ${ YOUR_APP } - ${ YOUR_ENV } \\ | jq -r '.Stacks[] | .Outputs[] | select(.OutputKey == \"EnvironmentSecurityGroup\") | .OutputValue' ) Once you have these, create the mount targets. $ MOUNT_TARGET_1_ID = $( aws efs create-mount-target \\ --subnet-id $SUBNET_1 \\ --security-groups $ENV_SG \\ --file-system-id $EFS_ID | jq -r .MountTargetID ) $ MOUNT_TARGET_2_ID = $( aws efs create-mount-target \\ --subnet-id $SUBNET_2 \\ --security-groups $ENV_SG \\ --file-system-id $EFS_ID | jq -r .MountTargetID ) Once you've done this, you can specify the storage configuration in the manifest as above. Cleanup Delete the mount targets using the AWS CLI. $ aws efs delete-mount-target --mount-target-id $MOUNT_TARGET_1 $ aws efs delete-mount-target --mount-target-id $MOUNT_TARGET_2 CloudFormation Here's an example of how you might create the appropriate EFS infrastructure for an external file system using a CloudFormation stack. After creating an environment, deploy the following CloudFormation template into the same account and region as the environment. Place the following CloudFormation template into a file called efs.yml . Parameters : App : Type : String Description : Your application's name. Env : Type : String Description : The environment name your service, job, or workflow is being deployed to. Resources : EFSFileSystem : Metadata : 'aws:copilot:description' : 'An EFS File System for persistent backing storage for tasks and services' Type : AWS::EFS::FileSystem Properties : PerformanceMode : generalPurpose ThroughputMode : bursting Encrypted : true MountTargetPublicSubnet1 : Type : AWS::EFS::MountTarget Properties : FileSystemId : !Ref EFSFileSystem SecurityGroups : - Fn::ImportValue : !Sub \"${App}-${Env}-EnvironmentSecurityGroup\" SubnetId : !Select - 0 - !Split - \",\" - Fn::ImportValue : !Sub \"${App}-${Env}-PublicSubnets\" MountTargetPublicSubnet2 : Type : AWS::EFS::MountTarget Properties : FileSystemId : !Ref EFSFileSystem SecurityGroups : - Fn::ImportValue : !Sub \"${App}-${Env}-EnvironmentSecurityGroup\" SubnetId : !Select - 1 - !Split - \",\" - Fn::ImportValue : !Sub \"${App}-${Env}-PublicSubnets\" Outputs : EFSVolumeID : Value : !Ref EFSFileSystem Export : Name : !Sub ${App}-${Env}-FilesystemID Then run: $ aws cloudformation deploy --stack-name efs-cfn \\ --template-file ecs.yml --parameter-overrides App = ${ YOUR_APP } Env = ${ YOUR_ENV } This will create an EFS file system and the mount targets your tasks need using outputs from the Copilot environment stack. To get the EFS filesystem ID, you can run a describe-stacks call: $ aws cloudformation describe-stacks --stack-name efs-cfn | \\ jq -r '.Stacks[] | .Outputs[] | .OutputValue' Then, in the manifest of the service which you would like to have access to the EFS filesystem, add the following configuration. storage : volumes : copilotVolume : # This is a variable key and can be set to arbitrary strings. path : '/etc/mount1' read_only : true # Set to false if your service needs write access. efs : id : <your filesystem ID> Finally, run copilot svc deploy to reconfigure your service to mount the filesystem at /etc/mount1 . Cleanup To clean this up, remove the storage configuration from the manifest and redeploy the service: $ copilot svc deploy Then, delete the stack. $ aws cloudformation delete-stack --stack-name efs-cfn","title":"Storage"},{"location":"docs/developing/storage#storage","text":"There are two ways to add persistence to Copilot workloads: using copilot storage init to create databases and S3 buckets; and attaching an existing EFS filesystem using the storage field in the manifest.","title":"Storage"},{"location":"docs/developing/storage#database-and-artifacts","text":"To add a database or S3 bucket to your job or service, simply run copilot storage init . # For a guided experience. $ copilot storage init -t S3 # To create a bucket named \"my-bucket\" accessible by the \"api\" service. $ copilot storage init -n my-bucket -t S3 -w api The above command will create the Cloudformation template for an S3 bucket in the addons directory for the \"api\" service. The next time you run copilot deploy -n api , the bucket will be created, permission to access it will be added to the api task role, and the name of the bucket will be injected into the api container under the environment variable MY_BUCKET_NAME . Info All names are converted into SCREAMING_SNAKE_CASE based on their use of hyphens or underscores. You can view the environment variables for a given service by running copilot svc show . You can also create a DynamoDB table using copilot storage init . For example, to create the Cloudformation template for a table with a sort key and a local secondary index, you could run the following command. # For a guided experience. $ copilot storage init -t DynamoDB # Or skip the prompts by providing flags. $ copilot storage init -n users -t DynamoDB -w api --partition-key id:N --sort-key email:S --lsi post-count:N This will create a DynamoDB table called ${app}-${env}-${svc}-users . Its partition key will be id , a Number attribute; its sort key will be email , a String attribute; and it will have a local secondary index (essentially an alternate sort key) on the Number attribute post-count . It is also possible to create an RDS Aurora Serverless cluster using copilot storage init . # For a guided experience. $ copilot storage init -t Aurora # Or skip the prompts by providing flags. $ copilot storage init -n my-cluster -t Aurora -w api --engine PostgreSQL --initial-db my_db This will create an RDS Aurora Serverless cluster that uses PostgreSQL engine with a database named my_db . An environment variable named MYCLUSTER_SECRET is injected into your workload as a JSON string. The fields are 'host' , 'port' , 'dbname' , 'username' , 'password' , 'dbClusterIdentifier' and 'engine' .","title":"Database and Artifacts"},{"location":"docs/developing/storage#file-systems","text":"There are two ways to use an EFS file system with Copilot: using managed EFS, and importing your own filesystem.","title":"File Systems"},{"location":"docs/developing/storage#managed-efs","text":"The easiest way to get started using EFS for service- or job-level storage is via Copilot's built-in managed EFS capability. To get started, simply enable the efs key in the manifest under your volume's name. name : frontend storage : volumes : myManagedEFSVolume : efs : true path : /var/efs read_only : false This manifest will result in an EFS volume being created at the environment level, with an Access Point and dedicated directory at the path /frontend in the EFS filesystem created specifically for your service. Your container will be able to access this directory and all its subdirectories at the /var/efs path in its own filesystem. The /frontend directory and EFS filesystem will persist until you delete your environment. The use of an access point for each service ensures that no two services can access each other's data unless you specifically intend for them to do so by specifying the full advanced configuration. You can read more in Advanced Use Cases . You can also customize the UID and GID used for the access point by specifying the uid and gid fields in advanced EFS configuration. If you do not specify a UID or GID, Copilot picks a pseudorandom UID and GID for the access point based on the CRC32 checksum of the service's name. storage : volumes : myManagedEFSVolume : efs : uid : 1000 gid : 10000 path : /var/efs read_only : false uid and gid may not be specified with any other advanced EFS configuration.","title":"Managed EFS"},{"location":"docs/developing/storage#under-the-hood","text":"When you enable managed EFS, Copilot creates the following resources at the environment level: An EFS file system . Mount targets in each of your environment's private subnets Security group rules allowing the Environment Security Group to access the mount targets. At the service level, Copilot creates: An EFS Access Point . The Access Point refers to a directory created by CFN named after the service or job you wish to use EFS with. You can see the environment-level resources created by calling copilot env show --json --resources and parsing the output with your favorite command line JSON processor. For example: > copilot env show -n test --json --resources | jq '.resources[] | select( .type | contains(\"EFS\") )'","title":"Under the Hood"},{"location":"docs/developing/storage#advanced-use-cases","text":"","title":"Advanced Use Cases"},{"location":"docs/developing/storage#hydrating-a-managed-efs-volume","text":"Sometimes, you may wish to populate the created EFS volume with data before your service begins accepting traffic. There are several ways you can do this, depending on your main container's requirements and whether it requires this data for startup.","title":"Hydrating a Managed EFS Volume"},{"location":"docs/developing/storage#using-a-sidecar","text":"You can mount the created EFS volume in a sidecar using the mount_points field, and use your sidecar's COMMAND or ENTRYPOINT directives to copy data from the sidecar's filesystem or pull data down from S3 or another cloud service. If you mark the sidecar as nonessential with essential:false , it will start, do its work, and exit as the service containers come up and stabilize. This may not be suitable for workloads which depend on the correct data being present in the EFS volume.","title":"Using a Sidecar"},{"location":"docs/developing/storage#using-copilot-svc-exec","text":"For workloads where data must be present prior to your task containers coming up, we recommend using a placeholder container first. For example, deploy your service with the following values in the manifest: image : location : amazon/amazon-ecs-sample exec : true storage : volumes : myVolume : efs : true path : /var/efs read_only : false Then, when your service is stable, run: $ copilot svc exec This will open an interactive shell from which you can add packages like curl or wget , download data from the internet, create a directory structure, etc. Info This method of configuring containers is not recommended for production environments; containers are ephemeral and if you wish for a piece of software to be present in your service containers, be sure to add it using the RUN directive in a Dockerfile. When you have populated the directory, modify your manifest to remove the exec directive and update the build field to your desired Docker build config or image location. image : build : ./Dockerfile storage : volumes : myVolume : efs : true path : /var/efs read_only : false","title":"Using copilot svc exec"},{"location":"docs/developing/storage#external-efs","text":"Mounting an externally-managed EFS volume in Copilot tasks requires two things: That you create an EFS file system in the desired environment's region. That you create an EFS Mount Target using the Copilot environment security group in each subnet of your environment. When those prerequisites are satisfied, you can enable EFS storage using simple syntax in your manifest. You'll need the filesystem ID and, if using, the access point configuration for the filesystem. Info You can only use a given EFS file system in a single environment at a time. Mount targets are limited to one per availability zone; therefore, you must delete any existing mount targets before bringing the file system to Copilot if you have used it in another VPC.","title":"External EFS"},{"location":"docs/developing/storage#manifest-syntax","text":"The simplest possible EFS volume can be specified with the following syntax: storage : volumes : myEFSVolume : # This is a variable key and can be set to an arbitrary string. path : '/etc/mount1' efs : id : fs-1234567 This will create a read-only mounted volume in your service's or job's container using the filesystem fs-1234567 . If mount targets are not created in the subnets of the environment, the task will fail to launch. Full syntax for external EFS volumes follows. storage : volumes : <volume name> : path : <mount path> # Required. The path inside the container. read_only : <boolean> # Default: true efs : id : <filesystem ID> # Required. root_dir : <filesystem root> # Optional. Defaults to \"/\". Must not be # specified if using access points. auth : iam : <boolean> # Optional. Whether to use IAM authorization when # mounting this filesystem. access_point_id : <access point ID> # Optional. The ID of the EFS Access Point # to use when mounting this filesystem. uid : <uint32> # Optional. UID for managed EFS access point. gid : <uint32> # Optional. GID for managed EFS access point. Cannot be specified # with `id`, `root_dir`, or `auth`.","title":"Manifest Syntax"},{"location":"docs/developing/storage#creating-mount-targets","text":"There are several ways to create mount targets for an existing EFS filesystem: using the AWS CLI and using CloudFormation .","title":"Creating Mount Targets"},{"location":"docs/developing/storage#with-the-aws-cli","text":"To create mount targets for an existing filesystem, you'll need the ID of that filesystem. a Copilot environment deployed in the same account and region. To retrieve the filesystem ID, you can use the AWS CLI: $ EFS_FILESYSTEMS = $( aws efs describe-file-systems | \\ jq '.FileSystems[] | {ID: .FileSystemId, CreationTime: .CreationTime, Size: .SizeInBytes.Value}' ) If you echo this variable you should be able to find which filesystem you need. Assign it to the variable $EFS_ID and continue. You'll also need the public subnets of the Copilot environment and the Environment Security Group. This jq command will filter the output of the describe-stacks call down to simply the desired output value. Info The filesystem you use MUST be in the same region as your Copilot environment! $ SUBNETS = $( aws cloudformation describe-stacks --stack-name ${ YOUR_APP } - ${ YOUR_ENV } \\ | jq '.Stacks[] | .Outputs[] | select(.OutputKey == \"PublicSubnets\") | .OutputValue' ) $ SUBNET1 = $( echo $SUBNETS | jq -r 'split(\",\") | .[0]' ) $ SUBNET2 = $( echo $SUBNETS | jq -r 'split(\",\") | .[1]' ) $ ENV_SG = $( aws cloudformation describe-stacks --stack-name ${ YOUR_APP } - ${ YOUR_ENV } \\ | jq -r '.Stacks[] | .Outputs[] | select(.OutputKey == \"EnvironmentSecurityGroup\") | .OutputValue' ) Once you have these, create the mount targets. $ MOUNT_TARGET_1_ID = $( aws efs create-mount-target \\ --subnet-id $SUBNET_1 \\ --security-groups $ENV_SG \\ --file-system-id $EFS_ID | jq -r .MountTargetID ) $ MOUNT_TARGET_2_ID = $( aws efs create-mount-target \\ --subnet-id $SUBNET_2 \\ --security-groups $ENV_SG \\ --file-system-id $EFS_ID | jq -r .MountTargetID ) Once you've done this, you can specify the storage configuration in the manifest as above.","title":"With the AWS CLI"},{"location":"docs/developing/storage#cleanup","text":"Delete the mount targets using the AWS CLI. $ aws efs delete-mount-target --mount-target-id $MOUNT_TARGET_1 $ aws efs delete-mount-target --mount-target-id $MOUNT_TARGET_2","title":"Cleanup"},{"location":"docs/developing/storage#cloudformation","text":"Here's an example of how you might create the appropriate EFS infrastructure for an external file system using a CloudFormation stack. After creating an environment, deploy the following CloudFormation template into the same account and region as the environment. Place the following CloudFormation template into a file called efs.yml . Parameters : App : Type : String Description : Your application's name. Env : Type : String Description : The environment name your service, job, or workflow is being deployed to. Resources : EFSFileSystem : Metadata : 'aws:copilot:description' : 'An EFS File System for persistent backing storage for tasks and services' Type : AWS::EFS::FileSystem Properties : PerformanceMode : generalPurpose ThroughputMode : bursting Encrypted : true MountTargetPublicSubnet1 : Type : AWS::EFS::MountTarget Properties : FileSystemId : !Ref EFSFileSystem SecurityGroups : - Fn::ImportValue : !Sub \"${App}-${Env}-EnvironmentSecurityGroup\" SubnetId : !Select - 0 - !Split - \",\" - Fn::ImportValue : !Sub \"${App}-${Env}-PublicSubnets\" MountTargetPublicSubnet2 : Type : AWS::EFS::MountTarget Properties : FileSystemId : !Ref EFSFileSystem SecurityGroups : - Fn::ImportValue : !Sub \"${App}-${Env}-EnvironmentSecurityGroup\" SubnetId : !Select - 1 - !Split - \",\" - Fn::ImportValue : !Sub \"${App}-${Env}-PublicSubnets\" Outputs : EFSVolumeID : Value : !Ref EFSFileSystem Export : Name : !Sub ${App}-${Env}-FilesystemID Then run: $ aws cloudformation deploy --stack-name efs-cfn \\ --template-file ecs.yml --parameter-overrides App = ${ YOUR_APP } Env = ${ YOUR_ENV } This will create an EFS file system and the mount targets your tasks need using outputs from the Copilot environment stack. To get the EFS filesystem ID, you can run a describe-stacks call: $ aws cloudformation describe-stacks --stack-name efs-cfn | \\ jq -r '.Stacks[] | .Outputs[] | .OutputValue' Then, in the manifest of the service which you would like to have access to the EFS filesystem, add the following configuration. storage : volumes : copilotVolume : # This is a variable key and can be set to arbitrary strings. path : '/etc/mount1' read_only : true # Set to false if your service needs write access. efs : id : <your filesystem ID> Finally, run copilot svc deploy to reconfigure your service to mount the filesystem at /etc/mount1 .","title":"CloudFormation"},{"location":"docs/developing/storage#cleanup_1","text":"To clean this up, remove the storage configuration from the manifest and redeploy the service: $ copilot svc deploy Then, delete the stack. $ aws cloudformation delete-stack --stack-name efs-cfn","title":"Cleanup"},{"location":"docs/getting-started/first-app-tutorial","text":"AWS Copilot makes it easy to deploy your containers to AWS in just a few steps. In this tutorial we\u2019re going to do just that - we\u2019re going to deploy a sample front end service that you can visit in your browser. While we\u2019ll be using a sample static website in this example, you can use AWS Copilot to build and deploy any container app with a Dockerfile. After we get your service all set up, we\u2019ll show you how to delete the resources Copilot created to avoid charges. Sound fun? Let\u2019s do it! Step 1: Download & Configure AWS Copilot You\u2019ll need a few things to use AWS Copilot - the AWS Copilot binary, AWS CLI, Docker Desktop and AWS credentials. Follow our instructions here on how to set up and configure all these tools. Make sure that you have a default profile! Run aws configure to set one up! Step 2: Download some code to deploy In this example, we\u2019ll be using a sample app that\u2019s just a simple static website - but if you already have something you\u2019d like to deploy, just open your terminal and cd into your Dockerfile\u2019s directory. Otherwise you can just clone our sample repository. In your terminal, copy and paste this code. This will clone our sample app and change directories to it. $ git clone https://github.com/aws-samples/aws-copilot-sample-service example $ cd example Step 3: Set up our app Now this is where the fun starts! We have our service code and our Dockerfile and we want to get it deployed to AWS. Let\u2019s have AWS Copilot help us do just that! From within your code directory run: $ copilot init Step 4: Answer a few questions The next thing we\u2019re going to do is answer a few questions from Copilot. Copilot will use these questions to help us choose the best AWS infrastructure for your service. There\u2019s only a few so let\u2019s go through them: \u201cWhat would you like to name your application\u201d - an application is a collection of services. In this example we\u2019ll only have one service in our app, but if you wanted to have a multi-service app, Copilot makes that easy. Let\u2019s call this app example-app . \u201cWhich service type best represents your service's architecture?\u201d - Copilot is asking us what we want our service to do - do we want it to service traffic? Do we want it to be a private backend service? For us, we want our app to be accessible from the web, so let's hit enter and select Load Balanced Web Service . \u201cWhat do you want to name this Load Balanced Web Service?\u201d - now what should we call our service in our app? Be as creative as you want - but I recommend naming this service front-end . \u201cWhich Dockerfile would you like to use for front-end?\u201d - go ahead and choose the default Dockerfile here. This is the service that Copilot will build and deploy for you. Once you choose your Dockerfile, Copilot will start setting up the AWS infrastructure to manage your service. Step 5: Deploy your service Once Copilot finishes setting up the infrastructure to manage your app, you\u2019ll be asked if you want to deploy your service to a test environment type yes. Now we can wait a few minutes \u23f3 while Copilot sets up all the resources needed to run your service. After all the infrastructure for your service is set up, Copilot will build your image and push it to Amazon ECR, and start deploying to Amazon ECS. After your deployment completes your service will be up and running on AWS Fargate and Copilot will print a link to the URL \ud83c\udf89! Step 6: Clean up Now that you've deployed your service, let's go ahead and run copilot app delete - this will delete all the resources Copilot set up for your application, including your ECS Service and the ECR Repository. To delete everything run: $ copilot app delete Congratulations! Congratulations! You have learned how to set up, deploy, and delete your container application to Amazon ECS using AWS Copilot. AWS Copilot is a command line tool that helps you develop, release and operate your container apps on AWS. We hope you had fun deploying your app. Ready to dive deeper into AWS Copilot and learn how to build and manage production ready container apps on AWS? Check out the Developing section in the sidebar.","title":"Deploy your first application"},{"location":"docs/getting-started/first-app-tutorial#step-1-download-configure-aws-copilot","text":"You\u2019ll need a few things to use AWS Copilot - the AWS Copilot binary, AWS CLI, Docker Desktop and AWS credentials. Follow our instructions here on how to set up and configure all these tools. Make sure that you have a default profile! Run aws configure to set one up!","title":"Step 1: Download &amp; Configure AWS Copilot"},{"location":"docs/getting-started/first-app-tutorial#step-2-download-some-code-to-deploy","text":"In this example, we\u2019ll be using a sample app that\u2019s just a simple static website - but if you already have something you\u2019d like to deploy, just open your terminal and cd into your Dockerfile\u2019s directory. Otherwise you can just clone our sample repository. In your terminal, copy and paste this code. This will clone our sample app and change directories to it. $ git clone https://github.com/aws-samples/aws-copilot-sample-service example $ cd example","title":"Step 2: Download some code to deploy"},{"location":"docs/getting-started/first-app-tutorial#step-3-set-up-our-app","text":"Now this is where the fun starts! We have our service code and our Dockerfile and we want to get it deployed to AWS. Let\u2019s have AWS Copilot help us do just that! From within your code directory run: $ copilot init","title":"Step 3: Set up our app"},{"location":"docs/getting-started/first-app-tutorial#step-4-answer-a-few-questions","text":"The next thing we\u2019re going to do is answer a few questions from Copilot. Copilot will use these questions to help us choose the best AWS infrastructure for your service. There\u2019s only a few so let\u2019s go through them: \u201cWhat would you like to name your application\u201d - an application is a collection of services. In this example we\u2019ll only have one service in our app, but if you wanted to have a multi-service app, Copilot makes that easy. Let\u2019s call this app example-app . \u201cWhich service type best represents your service's architecture?\u201d - Copilot is asking us what we want our service to do - do we want it to service traffic? Do we want it to be a private backend service? For us, we want our app to be accessible from the web, so let's hit enter and select Load Balanced Web Service . \u201cWhat do you want to name this Load Balanced Web Service?\u201d - now what should we call our service in our app? Be as creative as you want - but I recommend naming this service front-end . \u201cWhich Dockerfile would you like to use for front-end?\u201d - go ahead and choose the default Dockerfile here. This is the service that Copilot will build and deploy for you. Once you choose your Dockerfile, Copilot will start setting up the AWS infrastructure to manage your service.","title":"Step 4: Answer a few questions"},{"location":"docs/getting-started/first-app-tutorial#step-5-deploy-your-service","text":"Once Copilot finishes setting up the infrastructure to manage your app, you\u2019ll be asked if you want to deploy your service to a test environment type yes. Now we can wait a few minutes \u23f3 while Copilot sets up all the resources needed to run your service. After all the infrastructure for your service is set up, Copilot will build your image and push it to Amazon ECR, and start deploying to Amazon ECS. After your deployment completes your service will be up and running on AWS Fargate and Copilot will print a link to the URL \ud83c\udf89!","title":"Step 5: Deploy your service"},{"location":"docs/getting-started/first-app-tutorial#step-6-clean-up","text":"Now that you've deployed your service, let's go ahead and run copilot app delete - this will delete all the resources Copilot set up for your application, including your ECS Service and the ECR Repository. To delete everything run: $ copilot app delete","title":"Step 6: Clean up"},{"location":"docs/getting-started/first-app-tutorial#congratulations","text":"Congratulations! You have learned how to set up, deploy, and delete your container application to Amazon ECS using AWS Copilot. AWS Copilot is a command line tool that helps you develop, release and operate your container apps on AWS. We hope you had fun deploying your app. Ready to dive deeper into AWS Copilot and learn how to build and manage production ready container apps on AWS? Check out the Developing section in the sidebar.","title":"Congratulations!"},{"location":"docs/getting-started/install","text":"You can install AWS Copilot through Homebrew or by downloading the binaries directly. Homebrew \ud83c\udf7b brew install aws/tap/copilot-cli Manually Copy and paste the command into your terminal. macOS Command to install curl -Lo copilot https://github.com/aws/copilot-cli/releases/latest/download/copilot-darwin && chmod +x copilot && sudo mv copilot /usr/local/bin/copilot && copilot --help Linux x86 (64-bit) Command to install curl -Lo copilot https://github.com/aws/copilot-cli/releases/latest/download/copilot-linux && chmod +x copilot && sudo mv copilot /usr/local/bin/copilot && copilot --help Linux (ARM) Command to install curl -Lo copilot https://github.com/aws/copilot-cli/releases/latest/download/copilot-linux-arm64 && chmod +x copilot && sudo mv copilot /usr/local/bin/copilot && copilot --help Windows Command to install Invoke-WebRequest -OutFile 'C:\\Program Files\\copilot.exe' https://github.com/aws/copilot-cli/releases/latest/download/copilot-windows.exe Tip Please use the Windows Terminal to have the best user experience. If you encounter permissions issues, ensure that you are running your terminal as an administrator. Info To download a specific version, replace \"latest\" with the specific version. For example, to download v0.6.0 on macOS, type: curl -Lo copilot https://github.com/aws/copilot-cli/releases/download/v0.6.0/copilot-darwin && chmod +x copilot && sudo mv copilot /usr/local/bin/copilot && copilot --help","title":"Install Copilot"},{"location":"docs/getting-started/install#homebrew","text":"brew install aws/tap/copilot-cli","title":"Homebrew \ud83c\udf7b"},{"location":"docs/getting-started/install#manually","text":"Copy and paste the command into your terminal. macOS Command to install curl -Lo copilot https://github.com/aws/copilot-cli/releases/latest/download/copilot-darwin && chmod +x copilot && sudo mv copilot /usr/local/bin/copilot && copilot --help Linux x86 (64-bit) Command to install curl -Lo copilot https://github.com/aws/copilot-cli/releases/latest/download/copilot-linux && chmod +x copilot && sudo mv copilot /usr/local/bin/copilot && copilot --help Linux (ARM) Command to install curl -Lo copilot https://github.com/aws/copilot-cli/releases/latest/download/copilot-linux-arm64 && chmod +x copilot && sudo mv copilot /usr/local/bin/copilot && copilot --help Windows Command to install Invoke-WebRequest -OutFile 'C:\\Program Files\\copilot.exe' https://github.com/aws/copilot-cli/releases/latest/download/copilot-windows.exe Tip Please use the Windows Terminal to have the best user experience. If you encounter permissions issues, ensure that you are running your terminal as an administrator. Info To download a specific version, replace \"latest\" with the specific version. For example, to download v0.6.0 on macOS, type: curl -Lo copilot https://github.com/aws/copilot-cli/releases/download/v0.6.0/copilot-darwin && chmod +x copilot && sudo mv copilot /usr/local/bin/copilot && copilot --help","title":"Manually"},{"location":"docs/getting-started/verify","text":"The AWS Copilot CLI executables are cryptographically signed using PGP signatures. The PGP signatures can be used to verify the validity of the AWS Copilot CLI executable. Use the following steps to verify the signatures using the GnuPG tool. Download and install GnuPG. For more information, see the GnuPG website . For macOS, we recommend using Homebrew. Install Homebrew using the instructions from their website. For more information, see Homebrew . After Homebrew is installed, use the following command from your macOS terminal. brew install gnupg For Linux systems, install gpg using the package manager on your flavor of Linux. For Windows systems, download and use the Windows simple installer from the GnuPG website. For more information, see GnuPG Download . Retrieve the Amazon ECS PGP public key. You can use a command to do this or manually create the key and then import it. Option 1: Retrieve the key with the following command. gpg --keyserver hkp://keys.gnupg.net --recv BCE9D9A42D51784F Option 2: Create a file with the following contents of the Amazon ECS PGP public key and then import it. -----BEGIN PGP PUBLIC KEY BLOCK----- Version: GnuPG v2 mQINBFq1SasBEADliGcT1NVJ1ydfN8DqebYYe9ne3dt6jqKFmKowLmm6LLGJe7HU jGtqhCWRDkN+qPpHqdArRgDZAtn2pXY5fEipHgar4CP8QgRnRMO2fl74lmavr4Vg 7K/KH8VHlq2uRw32/B94XLEgRbGTMdWFdKuxoPCttBQaMj3LGn6Pe+6xVWRkChQu BoQAhjBQ+bEm0kNy0LjNgjNlnL3UMAG56t8E3LANIgGgEnpNsB1UwfWluPoGZoTx N+6pHBJrKIL/1v/ETU4FXpYw2zvhWNahxeNRnoYj3uycHkeliCrw4kj0+skizBgO 2K7oVX8Oc3j5+ZilhL/qDLXmUCb2az5cMM1mOoF8EKX5HaNuq1KfwJxqXE6NNIcO lFTrT7QwD5fMNld3FanLgv/ZnIrsSaqJOL6zRSq8O4LN1OWBVbndExk2Kr+5kFxn 5lBPgfPgRj5hQ+KTHMa9Y8Z7yUc64BJiN6F9Nl7FJuSsfqbdkvRLsQRbcBG9qxX3 rJAEhieJzVMEUNl+EgeCkxj5xuSkNU7zw2c3hQZqEcrADLV+hvFJktOz9Gm6xzbq lTnWWCz4xrIWtuEBA2qE+MlDheVd78a3gIsEaSTfQq0osYXaQbvlnSWOoc1y/5Zb zizHTJIhLtUyls9WisP2s0emeHZicVMfW61EgPrJAiupgc7kyZvFt4YwfwARAQAB tCRBbWF6b24gRUNTIDxlY3Mtc2VjdXJpdHlAYW1hem9uLmNvbT6JAhwEEAECAAYF AlrjL0YACgkQHivRXs0TaQrg1g/+JppwPqHnlVPmv7lessB8I5UqZeD6p6uVpHd7 Bs3pcPp8BV7BdRbs3sPLt5bV1+rkqOlw+0gZ4Q/ue/YbWtOAt4qY0OcEo0HgcnaX lsB827QIfZIVtGWMhuh94xzm/SJkvngml6KB3YJNnWP61A9qJ37/VbVVLzvcmazA McWB4HUMNrhd0JgBCo0gIpqCbpJEvUc02Bjn23eEJsS9kC7OUAHyQkVnx4d9UzXF 4OoISF6hmQKIBoLnRrAlj5Qvs3GhvHQ0ThYq0Grk/KMJJX2CSqt7tWJ8gk1n3H3Y SReRXJRnv7DsDDBwFgT6r5Q2HW1TBUvaoZy5hF6maD09nHcNnvBjqADzeT8Tr/Qu bBCLzkNSYqqkpgtwv7seoD2P4n1giRvDAOEfMZpVkUr+C252IaH1HZFEz+TvBVQM Y8OWWxmIJW+J6evjo3N1eO19UHv71jvoF8zljbI4bsL2c+QTJmOv7nRqzDQgCWyp Id/v2dUVVTk1j9omuLBBwNJzQCB+72LcIzJhYmaP1HC4LcKQG+/f41exuItenatK lEJQhYtyVXcBlh6Yn/wzNg2NWOwb3vqY/F7m6u9ixAwgtIMgPCDE4aJ86zrrXYFz N2HqkTSQh77Z8KPKmyGopsmN/reMuilPdINb249nA0dzoN+nj+tTFOYCIaLaFyjs Z0r1QAOJAjkEEwECACMFAlq1SasCGwMHCwkIBwMCAQYVCAIJCgsEFgIDAQIeAQIX gAAKCRC86dmkLVF4T9iFEACEnkm1dNXsWUx34R3c0vamHrPxvfkyI1FlEUen8D1h uX9xy6jCEROHWEp0rjGK4QDPgM93sWJ+s1UAKg214QRVzft0y9/DdR+twApA0fzy uavIthGd6+03jAAo6udYDE+cZC3P7XBbDiYEWk4XAF9I1JjB8hTZUgvXBL046JhG eM17+crgUyQeetkiOQemLbsbXQ40Bd9V7zf7XJraFd8VrwNUwNb+9KFtgAsc9rk+ YIT/PEf+YOPysgcxI4sTWghtyCulVnuGoskgDv4v73PALU0ieUrvvQVqWMRvhVx1 0X90J7cC1KOyhlEQQ1aFTgmQjmXexVTwIBm8LvysFK6YXM41KjOrlz3+6xBIm/qe bFyLUnf4WoiuOplAaJhK9pRY+XEnGNxdtN4D26Kd0F+PLkm3Tr3Hy3b1Ok34FlGr KVHUq1TZD7cvMnnNKEELTUcKX+1mV3an16nmAg/my1JSUt6BNK2rJpY1s/kkSGSE XQ4zuF2IGCpvBFhYAlt5Un5zwqkwwQR3/n2kwAoDzonJcehDw/C/cGos5D0aIU7I K2X2aTD3+pA7Mx3IMe2hqmYqRt9X42yF1PIEVRneBRJ3HDezAgJrNh0GQWRQkhIx gz6/cTR+ekr5TptVszS9few2GpI5bCgBKBisZIssT89aw7mAKWut0Gcm4qM9/yK6 1bkCDQRatUmrARAAxNPvVwreJ2yAiFcUpdRlVhsuOgnxvs1QgsIw3H7+Pacr9Hpe 8uftYZqdC82KeSKhpHq7c8gMTMucIINtH25x9BCc73E33EjCL9Lqov1TL7+QkgHe T+JIhZwdD8Mx2K+LVVVu/aWkNrfMuNwyDUciSI4D5QHa8T+F8fgN4OTpwYjirzel 5yoICMr9hVcbzDNv/ozKCxjx+XKgnFc3wrnDfJfntfDAT7ecwbUTL+viQKJ646s+ psiqXRYtVvYInEhLVrJ0aV6zHFoigE/Bils6/g7ru1Q6CEHqEw++APs5CcE8VzJu WAGSVHZgun5Y9N4quR/M9Vm+IPMhTxrAg7rOvyRN9cAXfeSMf77I+XTifigNna8x t/MOdjXr1fjF4pThEi5u6WsuRdFwjY2azEv3vevodTi4HoJReH6dFRa6y8c+UDgl 2iHiOKIpQqLbHEfQmHcDd2fix+AaJKMnPGNku9qCFEMbgSRJpXz6BfwnY1QuKE+I R6jA0frUNt2jhiGG/F8RceXzohaaC/Cx7LUCUFWc0n7z32C9/Dtj7I1PMOacdZzz bjJzRKO/ZDv+UN/c9dwAkllzAyPMwGBkUaY68EBstnIliW34aWm6IiHhxioVPKSp VJfyiXPO0EXqujtHLAeChfjcns3I12YshT1dv2PafG53fp33ZdzeUgsBo+EAEQEA AYkCHwQYAQIACQUCWrVJqwIbDAAKCRC86dmkLVF4T+ZdD/9x/8APzgNJF3o3STrF jvnV1ycyhWYGAeBJiu7wjsNWwzMFOv15tLjB7AqeVxZn+WKDD/mIOQ45OZvnYZuy X7DR0JszaH9wrYTxZLVruAu+t6UL0y/XQ4L1GZ9QR6+r+7t1Mvbfy7BlHbvX/gYt Rwe/uwdibI0CagEzyX+2D3kTOlHO5XThbXaNf8AN8zha91Jt2Q2UR2X5T6JcwtMz FBvZnl3LSmZyE0EQehS2iUurU4uWOpGppuqVnbi0jbCvCHKgDGrqZ0smKNAQng54 F365W3g8AfY48s8XQwzmcliowYX9bT8PZiEi0J4QmQh0aXkpqZyFefuWeOL2R94S XKzr+gRh3BAULoqF+qK+IUMxTip9KTPNvYDpiC66yBiT6gFDji5Ca9pGpJXrC3xe TXiKQ8DBWDhBPVPrruLIaenTtZEOsPc4I85yt5U9RoPTStcOr34s3w5yEaJagt6S Gc5r9ysjkfH6+6rbi1ujxMgROSqtqr+RyB+V9A5/OgtNZc8llK6u4UoOCde8jUUW vqWKvjJB/Kz3u4zaeNu2ZyyHaOqOuH+TETcW+jsY9IhbEzqN5yQYGi4pVmDkY5vu lXbJnbqPKpRXgM9BecV9AMbPgbDq/5LnHJJXg+G8YQOgp4lR/hC1TEFdIp5wM8AK CWsENyt2o1rjgMXiZOMF8A5oBLkCDQRatUuSARAAr77kj7j2QR2SZeOSlFBvV7oS mFeSNnz9xZssqrsm6bTwSHM6YLDwc7Sdf2esDdyzONETwqrVCg+FxgL8hmo9hS4c rR6tmrP0mOmptr+xLLsKcaP7ogIXsyZnrEAEsvW8PnfayoiPCdc3cMCR/lTnHFGA 7EuR/XLBmi7Qg9tByVYQ5Yj5wB9V4B2yeCt3XtzPqeLKvaxl7PNelaHGJQY/xo+m V0bndxf9IY+4oFJ4blD32WqvyxESo7vW6WBh7oqv3Zbm0yQrr8a6mDBpqLkvWwNI 3kpJR974tg5o5LfDu1BeeyHWPSGm4U/G4JB+JIG1ADy+RmoWEt4BqTCZ/knnoGvw D5sTCxbKdmuOmhGyTssoG+3OOcGYHV7pWYPhazKHMPm201xKCjH1RfzRULzGKjD+ yMLT1I3AXFmLmZJXikAOlvE3/wgMqCXscbycbLjLD/bXIuFWo3rzoezeXjgi/DJx jKBAyBTYO5nMcth1O9oaFd9d0HbsOUDkIMnsgGBE766Piro6MHo0T0rXl07Tp4pI rwuSOsc6XzCzdImj0Wc6axS/HeUKRXWdXJwno5awTwXKRJMXGfhCvSvbcbc2Wx+L IKvmB7EB4K3fmjFFE67yolmiw2qRcUBfygtH3eL5XZU28MiCpue8Y8GKJoBAUyvf KeM1rO8Jm3iRAc5a/D0AEQEAAYkEPgQYAQIACQUCWrVLkgIbAgIpCRC86dmkLVF4 T8FdIAQZAQIABgUCWrVLkgAKCRDePL1hra+LjtHYD/9MucxdFe6bXO1dQR4tKhhQ P0LRqy6zlBY9ILCLowNdGZdqorogUiUymgn3VhEhVtxTOoHcN7qOuM01PNsRnOeS EYjf8Xrb1clzkD6xULwmOclTb9bBxnBc/4PFvHAbZW3QzusaZniNgkuxt6BTfloS Of4inq71kjmGK+TlzQ6mUMQUg228NUQC+a84EPqYyAeY1sgvgB7hJBhYL0QAxhcW 6m20Rd8iEc6HyzJ3yCOCsKip/nRWAbf0OvfHfRBp0+m0ZwnJM8cPRFjOqqzFpKH9 HpDmTrC4wKP1+TL52LyEqNh4yZitXmZNV7giSRIkk0eDSko+bFy6VbMzKUMkUJK3 D3eHFAMkujmbfJmSMTJOPGn5SB1HyjCZNx6bhIIbQyEUB9gKCmUFaqXKwKpF6rj0 iQXAJxLR/shZ5Rk96VxzOphUl7T90m/PnUEEPwq8KsBhnMRgxa0RFidDP+n9fgtv HLmrOqX9zBCVXh0mdWYLrWvmzQFWzG7AoE55fkf8nAEPsalrCdtaNUBHRXA0OQxG AHMOdJQQvBsmqMvuAdjkDWpFu5y0My5ddU+hiUzUyQLjL5Hhd5LOUDdewlZgIw1j xrEAUzDKetnemM8GkHxDgg8koev5frmShJuce7vSjKpCNg3EIJSgqMOPFjJuLWtZ vjHeDNbJy6uNL65ckJy6WhGjEADS2WAW1D6Tfekkc21SsIXk/LqEpLMR/0g5OUif wcEN1rS9IJXBwIy8MelN9qr5KcKQLmfdfBNEyyceBhyVl0MDyHOKC+7PofMtkGBq 13QieRHv5GJ8LB3fclqHV8pwTTo3Bc8z2g0TjmUYAN/ixETdReDoKavWJYSE9yoM aaJu279ioVTrwpECse0XkiRyKToTjwOb73CGkBZZpJyqux/rmCV/fp4ALdSW8zbz FJVORaivhoWwzjpfQKhwcU9lABXi2UvVm14v0AfeI7oiJPSU1zM4fEny4oiIBXlR zhFNih1UjIu82X16mTm3BwbIga/s1fnQRGzyhqUIMii+mWra23EwjChaxpvjjcUH 5ilLc5Zq781aCYRygYQw+hu5nFkOH1R+Z50Ubxjd/aqUfnGIAX7kPMD3Lof4KldD Q8ppQriUvxVo+4nPV6rpTy/PyqCLWDjkguHpJsEFsMkwajrAz0QNSAU5CJ0G2Zu4 yxvYlumHCEl7nbFrm0vIiA75Sa8KnywTDsyZsu3XcOcf3g+g1xWTpjJqy2bYXlqz 9uDOWtArWHOis6bq8l9RE6xr1RBVXS6uqgQIZFBGyq66b0dIq4D2JdsUvgEMaHbc e7tBfeB1CMBdA64e9Rq7bFR7Tvt8gasCZYlNr3lydh+dFHIEkH53HzQe6l88HEic +0jVnLkCDQRa55wJARAAyLya2Lx6gyoWoJN1a6740q3o8e9d4KggQOfGMTCflmeq ivuzgN+3DZHN+9ty2KxXMtn0mhHBerZdbNJyjMNT1gAgrhPNB4HtXBXum2wS57WK DNmade914L7FWTPAWBG2Wn448OEHTqsClICXXWy9IICgclAEyIq0Yq5mAdTEgRJS Z8t4GpwtDL9gNQyFXaWQmDmkAsCygQMvhAlmu9xOIzQG5CxSnZFk7zcuL60k14Z3 Cmt49k4T/7ZU8goWi8tt+rU78/IL3J/fF9+1civ1OwuUidgfPCSvOUW1JojsdCQA L+RZJcoXq7lfOFj/eNjeOSstCTDPfTCL+kThE6E5neDtbQHBYkEX1BRiTedsV4+M ucgiTrdQFWKf89G72xdv8ut9AYYQ2BbEYU+JAYhUH8rYYui2dHKJIgjNvJscuUWb +QEqJIRleJRhrO+/CHgMs4fZAkWF1VFhKBkcKmEjLn1f7EJJUUW84ZhKXjO/AUPX 1CHsNjziRceuJCJYox1cwsoq6jTE50GiNzcIxTn9xUc0UMKFeggNAFys1K+TDTm3 Bzo8H5ucjCUEmUm9lhkGwqTZgOlRX5eqPX+JBoSaObqhgqCa5IPinKRa6MgoFPHK 6sYKqroYwBGgZm6Js5chpNchvJMs/3WXNOEVg0J3z3vP0DMhxqWm+r+n9zlW8qsA EQEAAYkEPgQYAQgACQUCWuecCQIbAgIpCRC86dmkLVF4T8FdIAQZAQgABgUCWuec CQAKCRBQ3szEcQ5hr+ykD/4tOLRHFHXuKUcxgGaubUcVtsFrwBKma1cYjqaPms8u 6Sk0wfGRI32G/GhOrp0Ts/MOkbObq6VLTh8N5Yc/53MEl8zQFw9Y5AmRoW4PZXER ujs5s7p4oR7xHMihMjCCBn1bvrR+34YPfgzTcgLiOEFHYT8UTxwnGmXOvNkMM7md xD3CV5q6VAte8WKBo/220II3fcQlc9r/oWX4kXXkb0v9hoGwKbDJ1tzqTPrp/xFt yohqnvImpnlz+Q9zXmbrWYL9/g8VCmW/NN2gju2G3Lu/TlFUWIT4v/5OPK6TdeNb VKJO4+S8bTayqSG9CML1S57KSgCo5HUhQWeSNHI+fpe5oX6FALPT9JLDce8OZz1i cZZ0MELP37mOOQun0AlmHm/hVzf0f311PtbzcqWaE51tJvgUR/nZFo6Ta3O5Ezhs 3VlEJNQ1Ijf/6DH87SxvAoRIARCuZd0qxBcDK0avpFzUtbJd24lRA3WJpkEiMqKv RDVZkE4b6TW61f0o+LaVfK6E8oLpixegS4fiqC16mFrOdyRk+RJJfIUyz0WTDVmt g0U1CO1ezokMSqkJ7724pyjr2xf/r9/sC6aOJwB/lKgZkJfC6NqL7TlxVA31dUga LEOvEJTTE4gl+tYtfsCDvALCtqL0jduSkUo+RXcBItmXhA+tShW0pbS2Rtx/ixua KohVD/0R4QxiSwQmICNtm9mw9ydIl1yjYXX5a9x4wMJracNY/LBybJPFnZnT4dYR z4XjqysDwvvYZByaWoIe3QxjX84V6MlI2IdAT/xImu8gbaCI8tmyfpIrLnPKiR9D VFYfGBXuAX7+HgPPSFtrHQONCALxxzlbNpS+zxt9r0MiLgcLyspWxSdmoYGZ6nQP RO5Nm/ZVS+u2imPCRzNUZEMa+dlE6kHx0rS0dPiuJ4O7NtPeYDKkoQtNagspsDvh cK7CSqAiKMq06UBTxqlTSRkm62eOCtcs3p3OeHu5GRZF1uzTET0ZxYkaPgdrQknx ozjP5mC7X+45lcCfmcVt94TFNL5HwEUVJpmOgmzILCI8yoDTWzloo+i+fPFsXX4f kynhE83mSEcr5VHFYrTY3mQXGmNJ3bCLuc/jq7ysGq69xiKmTlUeXFm+aojcRO5i zyShIRJZ0GZfuzDYFDbMV9amA/YQGygLw//zP5ju5SW26dNxlf3MdFQE5JJ86rn9 MgZ4gcpazHEVUsbZsgkLizRp9imUiH8ymLqAXnfRGlU/LpNSefnvDFTtEIRcpOHc bhayG0bk51Bd4mioOXnIsKy4j63nJXA27x5EVVHQ1sYRN8Ny4Fdr2tMAmj2O+X+J qX2yy/UX5nSPU492e2CdZ1UhoU0SRFY3bxKHKB7SDbVeav+K5g== =Gi5D -----END PGP PUBLIC KEY BLOCK----- The details of the Amazon ECS PGP public key for reference: Key ID: BCE9D9A42D51784F Type: RSA Size: 4096/4096 Expires: Never User ID: Amazon ECS Key fingerprint: F34C 3DDA E729 26B0 79BE AEC6 BCE9 D9A4 2D51 784F Import the Amazon ECS PGP public key with the following command. gpg --import <public_key_filename> Download the AWS Copilot CLI signatures. The signatures are ASCII detached PGP signatures stored in files with the extension .asc . The signatures file has the same name as its corresponding executable, with .asc appended. macOS For macOS systems, run the following command. sudo curl -Lo copilot.asc https://github.com/aws/copilot-cli/releases/latest/download/copilot-darwin.asc Linux For Linux systems, run the following command. sudo curl -Lo copilot.asc https://github.com/aws/copilot-cli/releases/latest/download/copilot-linux.asc Windows For Windows systems, run the following command. Invoke-WebRequest -OutFile ecs-cli.asc https://github.com/aws/copilot-cli/releases/latest/download/copilot-windows.exe Verify the signature with the following command. For macOS and Linux systems: gpg --verify copilot.asc /usr/local/bin/copilot For Windows systems: gpg --verify ecs-cli.asc 'C:\\Program Files\\copilot.exe' Expected output: gpg: Signature made Tue Apr 3 13:29:30 2018 PDT gpg: using RSA key DE3CBD61ADAF8B8E gpg: Good signature from \"Amazon ECS <ecs-security@amazon.com>\" [unknown] gpg: WARNING: This key is not certified with a trusted signature! gpg: There is no indication that the signature belongs to the owner. Primary key fingerprint: F34C 3DDA E729 26B0 79BE AEC6 BCE9 D9A4 2D51 784F Subkey fingerprint: EB3D F841 E2C9 212A 2BD4 2232 DE3C BD61 ADAF 8B8E Warning The warning in the output is expected and is not problematic. It occurs because there is not a chain of trust between your personal PGP key (if you have one) and the Amazon ECS PGP key. For more information, see Web of trust .","title":"(Optional) Verify the installation"},{"location":"docs/include/common-svc-fields/","text":"entrypoint String or Array of Strings Override the default entrypoint in the image. # String version. entrypoint : \"/bin/entrypoint --p1 --p2\" # Alteratively, as an array of strings. entrypoint : [ \"/bin/entrypoint\" , \"--p1\" , \"--p2\" ] command String or Array of Strings Override the default command in the image. # String version. command : ps au # Alteratively, as an array of strings. command : [ \"ps\" , \"au\" ] cpu Integer Number of CPU units for the task. See the Amazon ECS docs for valid CPU values. memory Integer Amount of memory in MiB used by the task. See the Amazon ECS docs for valid memory values. count Integer or Map If you specify a number: count : 5 The service will set the desired count to 5 and maintain 5 tasks in your service. count. spot Integer If you want to use Fargate Spot capacity to run your services, you can specify a number under the spot subfield: count : spot : 5 Alternatively, you can specify a map for setting up autoscaling: count : range : 1-10 cpu_percentage : 70 memory_percentage : 80 requests : 10000 response_time : 2s count. range String or Map You can specify a minimum and maximum bound for the number of tasks your service should maintain. count : range : n-m This will set up an Application Autoscaling Target with the MinCapacity of n and MaxCapacity of m . Alternatively, if you wish to scale your service onto Fargate Spot instances, specify min and max under range and then specify spot_from with the desired count you wish to start placing your services onto Spot capacity. For example: count : range : min : 1 max : 10 spot_from : 3 This will set your range as 1-10 as above, but will place the first two copies of your service on dedicated Fargate capacity. If your service scales to 3 or higher, the third and any additional copies will be placed on Spot until the maximum is reached. range. min Integer The minimum desired count for your service using autoscaling. range. max Integer The maximum desired count for your service using autoscaling. range. spot_from Integer The desired count at which you wish to start placing your service using Fargate Spot capacity providers. count. cpu_percentage Integer Scale up or down based on the average CPU your service should maintain. count. memory_percentage Integer Scale up or down based on the average memory your service should maintain. count. requests Integer Scale up or down based on the request count handled per tasks. count. response_time Duration Scale up or down based on the service average response time. exec Boolean Enable running commands in your container. The default is false . Required for $ copilot svc exec . network Map The network section contains parameters for connecting to AWS resources in a VPC. network. vpc Map Subnets and security groups attached to your tasks. network.vpc. placement String Must be one of 'public' or 'private' . Defaults to launching your tasks in public subnets. Info If you launch tasks in 'private' subnets and use a Copilot-generated VPC, Copilot will add NAT Gateways to your environment. Alternatively, you can import a VPC with NAT Gateways when running copilot env init for internet connectivity. network.vpc. security_groups Array of Strings Additional security group IDs associated with your tasks. Copilot always includes a security group so containers within your environment can communicate with each other. variables Map Key-value pairs that represent environment variables that will be passed to your service. Copilot will include a number of environment variables by default for you. secrets Map Key-value pairs that represent secret values from AWS Systems Manager Parameter Store that will be securely passed to your service as environment variables. storage Map The Storage section lets you specify external EFS volumes for your containers and sidecars to mount. This allows you to access persistent storage across availability zones in a region for data processing or CMS workloads. For more detail, see the storage page. You can also specify extensible ephemeral storage at the task level. storage. ephemeral Int Specify how much ephemeral task storage to provision in GiB. The default value and minimum is 20 GiB. The maximum size is 200 GiB. Sizes above 20 GiB incur additional charges. To create a shared filesystem context between an essential container and a sidecar, you can use an empty volume: storage : ephemeral : 100 volumes : scratch : path : /var/data read_only : false sidecars : mySidecar : image : public.ecr.aws/my-image:latest mount_points : - source_volume : scratch path : /var/data read_only : false This example will provision 100 GiB of storage to be shared between the sidecar and the task container. This can be useful for large datasets, or for using a sidecar to transfer data from EFS into task storage for workloads with high disk I/O requirements. storage. volumes Map Specify the name and configuration of any EFS volumes you would like to attach. The volumes field is specified as a map of the form: volumes : <volume name> : path : \"/etc/mountpath\" efs : ... storage.volumes. volume Map Specify the configuration of a volume. volume. path String Required. Specify the location in the container where you would like your volume to be mounted. Must be fewer than 242 characters and must consist only of the characters a-zA-Z0-9.-_/ . volume. read_only Boolean Optional. Defaults to true . Defines whether the volume is read-only or not. If false, the container is granted elasticfilesystem:ClientWrite permissions to the filesystem and the volume is writable. volume. efs Boolean or Map Specify more detailed EFS configuration. If specified as a boolean, or using only the uid and gid subfields, creates a managed EFS filesystem and dedicated Access Point for this workload. // Simple managed EFS efs : true // Managed EFS with custom POSIX info efs : uid : 10000 gid : 110000 volume.efs. id String Required. The ID of the filesystem you would like to mount. volume.efs. root_dir String Optional. Defaults to / . Specify the location in the EFS filesystem you would like to use as the root of your volume. Must be fewer than 255 characters and must consist only of the characters a-zA-Z0-9.-_/ . If using an access point, root_dir must be either empty or / and auth.iam must be true . volume.efs. uid Uint32 Optional. Must be specified with gid . Mutually exclusive with root_dir , auth , and id . The POSIX UID to use for the dedicated access point created for the managed EFS filesystem. volume.efs. gid Uint32 Optional. Must be specified with uid . Mutually exclusive with root_dir , auth , and id . The POSIX GID to use for the dedicated access point created for the managed EFS filesystem. volume.efs. auth Map Specify advanced authorization configuration for EFS. volume.efs.auth. iam Boolean Optional. Defaults to true . Whether or not to use IAM authorization to determine whether the volume is allowed to connect to EFS. volume.efs.auth. access_point_id String Optional. Defaults to \"\" . The ID of the EFS access point to connect to. If using an access point, root_dir must be either empty or / and auth.iam must be true . environments Map The environment section lets you override any value in your manifest based on the environment you're in. In the example manifest above, we're overriding the count parameter so that we can run 2 copies of our service in our prod environment.","title":"Common svc fields"},{"location":"docs/include/http-config/","text":"http Map The http section contains parameters related to integrating your service with an Application Load Balancer. http. path String Requests to this path will be forwarded to your service. Each Load Balanced Web Service should listen on a unique path. http. healthcheck String or Map If you specify a string, Copilot interprets it as the path exposed in your container to handle target group health check requests. The default is \"/\". http : healthcheck : '/' You can also specify healthcheck as a map: http : healthcheck : path : '/' success_codes : '200' healthy_threshold : 3 unhealthy_threshold : 2 interval : 15s timeout : 10s http.healthcheck. success_codes String The HTTP status codes that healthy targets must use when responding to an HTTP health check. You can specify values between 200 and 499. You can specify multiple values (for example, \"200,202\") or a range of values (for example, \"200-299\"). The default is 200. http.healthcheck. healthy_threshold Integer The number of consecutive health check successes required before considering an unhealthy target healthy. The default is 5. Range: 2-10. http.healthcheck. unhealthy_threshold Integer The number of consecutive health check failures required before considering a target unhealthy. The default is 2. Range: 2-10. http.healthcheck. interval Duration The approximate amount of time, in seconds, between health checks of an individual target. The default is 30s. Range: 5s\u2013300s. http.healthcheck. timeout Duration The amount of time, in seconds, during which no response from a target means a failed health check. The default is 5s. Range 5s-300s. http. target_container String A sidecar container that takes the place of a service container. http. stickiness Boolean Indicates whether sticky sessions are enabled. http. allowed_source_ips Array of Strings CIDR IP addresses permitted to access your service. http : allowed_source_ips : [ \"192.0.2.0/24\" , \"198.51.100.10/32\" ]","title":"Http config"},{"location":"docs/include/image-config/","text":"image Map The image section contains parameters relating to the Docker build configuration and exposed port. image. build String or Map If you specify a string, Copilot interprets it as the path to your Dockerfile. It will assume that the dirname of the string you specify should be the build context. The manifest: image : build : path/to/dockerfile will result in the following call to docker build: $ docker build --file path/to/dockerfile path/to You can also specify build as a map: image : build : dockerfile : path/to/dockerfile context : context/dir target : build-stage cache_from : - image:tag args : key : value In this case, Copilot will use the context directory you specified and convert the key-value pairs under args to --build-arg overrides. The equivalent docker build call will be: $ docker build --file path/to/dockerfile --target build-stage --cache-from image:tag --build-arg key=value context/dir . You can omit fields and Copilot will do its best to understand what you mean. For example, if you specify context but not dockerfile , Copilot will run Docker in the context directory and assume that your Dockerfile is named \"Dockerfile.\" If you specify dockerfile but no context , Copilot assumes you want to run Docker in the directory that contains dockerfile . All paths are relative to your workspace root. image. location String Instead of building a container from a Dockerfile, you can specify an existing image name. Mutually exclusive with image.build . The location field follows the same definition as the image parameter in the Amazon ECS task definition. image. port Integer The port exposed in your Dockerfile. Copilot should parse this value for you from your EXPOSE instruction. image. labels Map An optional key/value map of Docker labels to add to the container.","title":"Image config"},{"location":"docs/include/image-healthcheck/","text":"image. healthcheck Map Optional configuration for container health checks. image.healthcheck. command Array of Strings The command to run to determine if the container is healthy. The string array can start with CMD to execute the command arguments directly, or CMD-SHELL to run the command with the container's default shell. image.healthcheck. interval Duration Time period between health checks, in seconds. Default is 10s. image.healthcheck. retries Integer Number of times to retry before container is deemed unhealthy. Default is 2. image.healthcheck. timeout Duration How long to wait before considering the health check failed, in seconds. Default is 5s. image.healthcheck. start_period Duration Grace period within which to provide containers time to bootstrap before failed health checks count towards the maximum number of retries. Default is 0s.","title":"Image healthcheck"},{"location":"docs/manifest/backend-service","text":"List of all available properties for a 'Backend Service' manifest. To learn about Copilot services, see the Services concept page. Sample manifest for an api service # Your service name will be used in naming your resources like log groups, ECS services, etc. name : api type : Backend Service # Your service is reachable at \"http://api.${COPILOT_SERVICE_DISCOVERY_ENDPOINT}:8080\" but is not public. # Configuration for your containers and service. image : build : ./api/Dockerfile port : 8080 healthcheck : command : [ \"CMD-SHELL\" , \"curl -f http://localhost:8080 || exit 1\" ] interval : 10s retries : 2 timeout : 5s start_period : 0s cpu : 256 memory : 512 count : 1 exec : true storage : volumes : myEFSVolume : path : '/etc/mount1' read_only : true efs : id : fs-12345678 root_dir : '/' auth : iam : true access_point_id : fsap-12345678 network : vpc : placement : 'private' security_groups : [ 'sg-05d7cd12cceeb9a6e' ] variables : LOG_LEVEL : info secrets : GITHUB_TOKEN : GITHUB_TOKEN # You can override any of the values defined above by environment. environments : test : count : spot : 2 production : count : 2 name String The name of your service. type String The architecture type for your service. Backend Services are not reachable from the internet, but can be reached with service discovery from your other services. image Map The image section contains parameters relating to the Docker build configuration and exposed port. image. build String or Map If you specify a string, Copilot interprets it as the path to your Dockerfile. It will assume that the dirname of the string you specify should be the build context. The manifest: image : build : path/to/dockerfile will result in the following call to docker build: $ docker build --file path/to/dockerfile path/to You can also specify build as a map: image : build : dockerfile : path/to/dockerfile context : context/dir target : build-stage cache_from : - image:tag args : key : value In this case, Copilot will use the context directory you specified and convert the key-value pairs under args to --build-arg overrides. The equivalent docker build call will be: $ docker build --file path/to/dockerfile --target build-stage --cache-from image:tag --build-arg key=value context/dir . You can omit fields and Copilot will do its best to understand what you mean. For example, if you specify context but not dockerfile , Copilot will run Docker in the context directory and assume that your Dockerfile is named \"Dockerfile.\" If you specify dockerfile but no context , Copilot assumes you want to run Docker in the directory that contains dockerfile . All paths are relative to your workspace root. image. location String Instead of building a container from a Dockerfile, you can specify an existing image name. Mutually exclusive with image.build . The location field follows the same definition as the image parameter in the Amazon ECS task definition. image. port Integer The port exposed in your Dockerfile. Copilot should parse this value for you from your EXPOSE instruction. image. labels Map An optional key/value map of Docker labels to add to the container. image. healthcheck Map Optional configuration for container health checks. image.healthcheck. command Array of Strings The command to run to determine if the container is healthy. The string array can start with CMD to execute the command arguments directly, or CMD-SHELL to run the command with the container's default shell. image.healthcheck. interval Duration Time period between health checks, in seconds. Default is 10s. image.healthcheck. retries Integer Number of times to retry before container is deemed unhealthy. Default is 2. image.healthcheck. timeout Duration How long to wait before considering the health check failed, in seconds. Default is 5s. image.healthcheck. start_period Duration Grace period within which to provide containers time to bootstrap before failed health checks count towards the maximum number of retries. Default is 0s. entrypoint String or Array of Strings Override the default entrypoint in the image. # String version. entrypoint : \"/bin/entrypoint --p1 --p2\" # Alteratively, as an array of strings. entrypoint : [ \"/bin/entrypoint\" , \"--p1\" , \"--p2\" ] command String or Array of Strings Override the default command in the image. # String version. command : ps au # Alteratively, as an array of strings. command : [ \"ps\" , \"au\" ] cpu Integer Number of CPU units for the task. See the Amazon ECS docs for valid CPU values. memory Integer Amount of memory in MiB used by the task. See the Amazon ECS docs for valid memory values. count Integer or Map If you specify a number: count : 5 The service will set the desired count to 5 and maintain 5 tasks in your service. count. spot Integer If you want to use Fargate Spot capacity to run your services, you can specify a number under the spot subfield: count : spot : 5 Alternatively, you can specify a map for setting up autoscaling: count : range : 1-10 cpu_percentage : 70 memory_percentage : 80 requests : 10000 response_time : 2s count. range String or Map You can specify a minimum and maximum bound for the number of tasks your service should maintain. count : range : n-m This will set up an Application Autoscaling Target with the MinCapacity of n and MaxCapacity of m . Alternatively, if you wish to scale your service onto Fargate Spot instances, specify min and max under range and then specify spot_from with the desired count you wish to start placing your services onto Spot capacity. For example: count : range : min : 1 max : 10 spot_from : 3 This will set your range as 1-10 as above, but will place the first two copies of your service on dedicated Fargate capacity. If your service scales to 3 or higher, the third and any additional copies will be placed on Spot until the maximum is reached. range. min Integer The minimum desired count for your service using autoscaling. range. max Integer The maximum desired count for your service using autoscaling. range. spot_from Integer The desired count at which you wish to start placing your service using Fargate Spot capacity providers. count. cpu_percentage Integer Scale up or down based on the average CPU your service should maintain. count. memory_percentage Integer Scale up or down based on the average memory your service should maintain. count. requests Integer Scale up or down based on the request count handled per tasks. count. response_time Duration Scale up or down based on the service average response time. exec Boolean Enable running commands in your container. The default is false . Required for $ copilot svc exec . network Map The network section contains parameters for connecting to AWS resources in a VPC. network. vpc Map Subnets and security groups attached to your tasks. network.vpc. placement String Must be one of 'public' or 'private' . Defaults to launching your tasks in public subnets. Info If you launch tasks in 'private' subnets and use a Copilot-generated VPC, Copilot will add NAT Gateways to your environment. Alternatively, you can import a VPC with NAT Gateways when running copilot env init for internet connectivity. network.vpc. security_groups Array of Strings Additional security group IDs associated with your tasks. Copilot always includes a security group so containers within your environment can communicate with each other. variables Map Key-value pairs that represent environment variables that will be passed to your service. Copilot will include a number of environment variables by default for you. secrets Map Key-value pairs that represent secret values from AWS Systems Manager Parameter Store that will be securely passed to your service as environment variables. storage Map The Storage section lets you specify external EFS volumes for your containers and sidecars to mount. This allows you to access persistent storage across availability zones in a region for data processing or CMS workloads. For more detail, see the storage page. You can also specify extensible ephemeral storage at the task level. storage. ephemeral Int Specify how much ephemeral task storage to provision in GiB. The default value and minimum is 20 GiB. The maximum size is 200 GiB. Sizes above 20 GiB incur additional charges. To create a shared filesystem context between an essential container and a sidecar, you can use an empty volume: storage : ephemeral : 100 volumes : scratch : path : /var/data read_only : false sidecars : mySidecar : image : public.ecr.aws/my-image:latest mount_points : - source_volume : scratch path : /var/data read_only : false This example will provision 100 GiB of storage to be shared between the sidecar and the task container. This can be useful for large datasets, or for using a sidecar to transfer data from EFS into task storage for workloads with high disk I/O requirements. storage. volumes Map Specify the name and configuration of any EFS volumes you would like to attach. The volumes field is specified as a map of the form: volumes : <volume name> : path : \"/etc/mountpath\" efs : ... storage.volumes. volume Map Specify the configuration of a volume. volume. path String Required. Specify the location in the container where you would like your volume to be mounted. Must be fewer than 242 characters and must consist only of the characters a-zA-Z0-9.-_/ . volume. read_only Boolean Optional. Defaults to true . Defines whether the volume is read-only or not. If false, the container is granted elasticfilesystem:ClientWrite permissions to the filesystem and the volume is writable. volume. efs Boolean or Map Specify more detailed EFS configuration. If specified as a boolean, or using only the uid and gid subfields, creates a managed EFS filesystem and dedicated Access Point for this workload. // Simple managed EFS efs : true // Managed EFS with custom POSIX info efs : uid : 10000 gid : 110000 volume.efs. id String Required. The ID of the filesystem you would like to mount. volume.efs. root_dir String Optional. Defaults to / . Specify the location in the EFS filesystem you would like to use as the root of your volume. Must be fewer than 255 characters and must consist only of the characters a-zA-Z0-9.-_/ . If using an access point, root_dir must be either empty or / and auth.iam must be true . volume.efs. uid Uint32 Optional. Must be specified with gid . Mutually exclusive with root_dir , auth , and id . The POSIX UID to use for the dedicated access point created for the managed EFS filesystem. volume.efs. gid Uint32 Optional. Must be specified with uid . Mutually exclusive with root_dir , auth , and id . The POSIX GID to use for the dedicated access point created for the managed EFS filesystem. volume.efs. auth Map Specify advanced authorization configuration for EFS. volume.efs.auth. iam Boolean Optional. Defaults to true . Whether or not to use IAM authorization to determine whether the volume is allowed to connect to EFS. volume.efs.auth. access_point_id String Optional. Defaults to \"\" . The ID of the EFS access point to connect to. If using an access point, root_dir must be either empty or / and auth.iam must be true . environments Map The environment section lets you override any value in your manifest based on the environment you're in. In the example manifest above, we're overriding the count parameter so that we can run 2 copies of our service in our prod environment.","title":"Backend Service"},{"location":"docs/manifest/lb-web-service","text":"List of all available properties for a 'Load Balanced Web Service' manifest. To learn about Copilot services, see the Services concept page. Sample manifest for a frontend service # Your service name will be used in naming your resources like log groups, ECS services, etc. name : frontend type : Load Balanced Web Service # Distribute traffic to your service. http : path : '/' healthcheck : path : '/_healthcheck' success_codes : '200,301' healthy_threshold : 3 unhealthy_threshold : 2 interval : 15s timeout : 10s stickiness : false allowed_source_ips : [ \"10.24.34.0/23\" ] # Configuration for your containers and service. image : build : dockerfile : ./frontend/Dockerfile context : ./frontend port : 80 cpu : 256 memory : 512 count : range : 1-10 cpu_percentage : 70 memory_percentage : 80 requests : 10000 response_time : 2s exec : true variables : LOG_LEVEL : info secrets : GITHUB_TOKEN : GITHUB_TOKEN # You can override any of the values defined above by environment. environments : test : count : range : min : 1 max : 10 spot_from : 2 staging : count : spot : 2 production : count : 2 name String The name of your service. type String The architecture type for your service. A Load Balanced Web Service is an internet-facing service that's behind a load balancer, orchestrated by Amazon ECS on AWS Fargate. http Map The http section contains parameters related to integrating your service with an Application Load Balancer. http. path String Requests to this path will be forwarded to your service. Each Load Balanced Web Service should listen on a unique path. http. healthcheck String or Map If you specify a string, Copilot interprets it as the path exposed in your container to handle target group health check requests. The default is \"/\". http : healthcheck : '/' You can also specify healthcheck as a map: http : healthcheck : path : '/' success_codes : '200' healthy_threshold : 3 unhealthy_threshold : 2 interval : 15s timeout : 10s http.healthcheck. success_codes String The HTTP status codes that healthy targets must use when responding to an HTTP health check. You can specify values between 200 and 499. You can specify multiple values (for example, \"200,202\") or a range of values (for example, \"200-299\"). The default is 200. http.healthcheck. healthy_threshold Integer The number of consecutive health check successes required before considering an unhealthy target healthy. The default is 5. Range: 2-10. http.healthcheck. unhealthy_threshold Integer The number of consecutive health check failures required before considering a target unhealthy. The default is 2. Range: 2-10. http.healthcheck. interval Duration The approximate amount of time, in seconds, between health checks of an individual target. The default is 30s. Range: 5s\u2013300s. http.healthcheck. timeout Duration The amount of time, in seconds, during which no response from a target means a failed health check. The default is 5s. Range 5s-300s. http. target_container String A sidecar container that takes the place of a service container. http. stickiness Boolean Indicates whether sticky sessions are enabled. http. allowed_source_ips Array of Strings CIDR IP addresses permitted to access your service. http : allowed_source_ips : [ \"192.0.2.0/24\" , \"198.51.100.10/32\" ] image Map The image section contains parameters relating to the Docker build configuration and exposed port. image. build String or Map If you specify a string, Copilot interprets it as the path to your Dockerfile. It will assume that the dirname of the string you specify should be the build context. The manifest: image : build : path/to/dockerfile will result in the following call to docker build: $ docker build --file path/to/dockerfile path/to You can also specify build as a map: image : build : dockerfile : path/to/dockerfile context : context/dir target : build-stage cache_from : - image:tag args : key : value In this case, Copilot will use the context directory you specified and convert the key-value pairs under args to --build-arg overrides. The equivalent docker build call will be: $ docker build --file path/to/dockerfile --target build-stage --cache-from image:tag --build-arg key=value context/dir . You can omit fields and Copilot will do its best to understand what you mean. For example, if you specify context but not dockerfile , Copilot will run Docker in the context directory and assume that your Dockerfile is named \"Dockerfile.\" If you specify dockerfile but no context , Copilot assumes you want to run Docker in the directory that contains dockerfile . All paths are relative to your workspace root. image. location String Instead of building a container from a Dockerfile, you can specify an existing image name. Mutually exclusive with image.build . The location field follows the same definition as the image parameter in the Amazon ECS task definition. image. port Integer The port exposed in your Dockerfile. Copilot should parse this value for you from your EXPOSE instruction. image. labels Map An optional key/value map of Docker labels to add to the container. entrypoint String or Array of Strings Override the default entrypoint in the image. # String version. entrypoint : \"/bin/entrypoint --p1 --p2\" # Alteratively, as an array of strings. entrypoint : [ \"/bin/entrypoint\" , \"--p1\" , \"--p2\" ] command String or Array of Strings Override the default command in the image. # String version. command : ps au # Alteratively, as an array of strings. command : [ \"ps\" , \"au\" ] cpu Integer Number of CPU units for the task. See the Amazon ECS docs for valid CPU values. memory Integer Amount of memory in MiB used by the task. See the Amazon ECS docs for valid memory values. count Integer or Map If you specify a number: count : 5 The service will set the desired count to 5 and maintain 5 tasks in your service. count. spot Integer If you want to use Fargate Spot capacity to run your services, you can specify a number under the spot subfield: count : spot : 5 Alternatively, you can specify a map for setting up autoscaling: count : range : 1-10 cpu_percentage : 70 memory_percentage : 80 requests : 10000 response_time : 2s count. range String or Map You can specify a minimum and maximum bound for the number of tasks your service should maintain. count : range : n-m This will set up an Application Autoscaling Target with the MinCapacity of n and MaxCapacity of m . Alternatively, if you wish to scale your service onto Fargate Spot instances, specify min and max under range and then specify spot_from with the desired count you wish to start placing your services onto Spot capacity. For example: count : range : min : 1 max : 10 spot_from : 3 This will set your range as 1-10 as above, but will place the first two copies of your service on dedicated Fargate capacity. If your service scales to 3 or higher, the third and any additional copies will be placed on Spot until the maximum is reached. range. min Integer The minimum desired count for your service using autoscaling. range. max Integer The maximum desired count for your service using autoscaling. range. spot_from Integer The desired count at which you wish to start placing your service using Fargate Spot capacity providers. count. cpu_percentage Integer Scale up or down based on the average CPU your service should maintain. count. memory_percentage Integer Scale up or down based on the average memory your service should maintain. count. requests Integer Scale up or down based on the request count handled per tasks. count. response_time Duration Scale up or down based on the service average response time. exec Boolean Enable running commands in your container. The default is false . Required for $ copilot svc exec . network Map The network section contains parameters for connecting to AWS resources in a VPC. network. vpc Map Subnets and security groups attached to your tasks. network.vpc. placement String Must be one of 'public' or 'private' . Defaults to launching your tasks in public subnets. Info If you launch tasks in 'private' subnets and use a Copilot-generated VPC, Copilot will add NAT Gateways to your environment. Alternatively, you can import a VPC with NAT Gateways when running copilot env init for internet connectivity. network.vpc. security_groups Array of Strings Additional security group IDs associated with your tasks. Copilot always includes a security group so containers within your environment can communicate with each other. variables Map Key-value pairs that represent environment variables that will be passed to your service. Copilot will include a number of environment variables by default for you. secrets Map Key-value pairs that represent secret values from AWS Systems Manager Parameter Store that will be securely passed to your service as environment variables. storage Map The Storage section lets you specify external EFS volumes for your containers and sidecars to mount. This allows you to access persistent storage across availability zones in a region for data processing or CMS workloads. For more detail, see the storage page. You can also specify extensible ephemeral storage at the task level. storage. ephemeral Int Specify how much ephemeral task storage to provision in GiB. The default value and minimum is 20 GiB. The maximum size is 200 GiB. Sizes above 20 GiB incur additional charges. To create a shared filesystem context between an essential container and a sidecar, you can use an empty volume: storage : ephemeral : 100 volumes : scratch : path : /var/data read_only : false sidecars : mySidecar : image : public.ecr.aws/my-image:latest mount_points : - source_volume : scratch path : /var/data read_only : false This example will provision 100 GiB of storage to be shared between the sidecar and the task container. This can be useful for large datasets, or for using a sidecar to transfer data from EFS into task storage for workloads with high disk I/O requirements. storage. volumes Map Specify the name and configuration of any EFS volumes you would like to attach. The volumes field is specified as a map of the form: volumes : <volume name> : path : \"/etc/mountpath\" efs : ... storage.volumes. volume Map Specify the configuration of a volume. volume. path String Required. Specify the location in the container where you would like your volume to be mounted. Must be fewer than 242 characters and must consist only of the characters a-zA-Z0-9.-_/ . volume. read_only Boolean Optional. Defaults to true . Defines whether the volume is read-only or not. If false, the container is granted elasticfilesystem:ClientWrite permissions to the filesystem and the volume is writable. volume. efs Boolean or Map Specify more detailed EFS configuration. If specified as a boolean, or using only the uid and gid subfields, creates a managed EFS filesystem and dedicated Access Point for this workload. // Simple managed EFS efs : true // Managed EFS with custom POSIX info efs : uid : 10000 gid : 110000 volume.efs. id String Required. The ID of the filesystem you would like to mount. volume.efs. root_dir String Optional. Defaults to / . Specify the location in the EFS filesystem you would like to use as the root of your volume. Must be fewer than 255 characters and must consist only of the characters a-zA-Z0-9.-_/ . If using an access point, root_dir must be either empty or / and auth.iam must be true . volume.efs. uid Uint32 Optional. Must be specified with gid . Mutually exclusive with root_dir , auth , and id . The POSIX UID to use for the dedicated access point created for the managed EFS filesystem. volume.efs. gid Uint32 Optional. Must be specified with uid . Mutually exclusive with root_dir , auth , and id . The POSIX GID to use for the dedicated access point created for the managed EFS filesystem. volume.efs. auth Map Specify advanced authorization configuration for EFS. volume.efs.auth. iam Boolean Optional. Defaults to true . Whether or not to use IAM authorization to determine whether the volume is allowed to connect to EFS. volume.efs.auth. access_point_id String Optional. Defaults to \"\" . The ID of the EFS access point to connect to. If using an access point, root_dir must be either empty or / and auth.iam must be true . environments Map The environment section lets you override any value in your manifest based on the environment you're in. In the example manifest above, we're overriding the count parameter so that we can run 2 copies of our service in our prod environment.","title":"Load Balanced Web Service"},{"location":"docs/manifest/overview","text":"Manifest The AWS Copilot CLI manifest describes a service\u2019s or job's architecture as infrastructure-as-code. It is a file generated from copilot init , copilot svc init , or copilot job init that gets converted to an AWS CloudFormation template. Unlike raw CloudFormation templates, the manifest allows you to focus on the most common settings for the architecture of your service or job, and not the individual resources. Manifest files are stored under copilot/<your service or job name>/manifest.yml .","title":"Overview"},{"location":"docs/manifest/overview#manifest","text":"The AWS Copilot CLI manifest describes a service\u2019s or job's architecture as infrastructure-as-code. It is a file generated from copilot init , copilot svc init , or copilot job init that gets converted to an AWS CloudFormation template. Unlike raw CloudFormation templates, the manifest allows you to focus on the most common settings for the architecture of your service or job, and not the individual resources. Manifest files are stored under copilot/<your service or job name>/manifest.yml .","title":"Manifest"},{"location":"docs/manifest/pipeline","text":"List of all available properties for a Copilot pipeline manifest. To learn more about pipelines, see the Pipelines concept page. Sample manifest for a pipeline triggered from a GitHub repo name : pipeline-sample-app-frontend version : 1 source : provider : GitHub properties : branch : main repository : https://github.com/<user>/sample-app-frontend # Optional: specify the name of an existing CodeStar Connections connection. connection_name : a-connection build : image : aws/codebuild/amazonlinux2-x86_64-standard:3.0 stages : - name : test test_commands : - make test - echo \"woo! Tests passed\" - name : prod requires_approval : true name String The name of your pipeline. version String The schema version for the template. There is only one version, 1 , supported at the moment. source Map Configuration for how your pipeline is triggered. source. provider String The name of your provider. Currently, GitHub , Bitbucket , and CodeCommit are supported. source. properties Map Provider-specific configuration on how the pipeline is triggered. source.properties. access_token_secret String The name of AWS Secrets Manager secret that holds the GitHub access token to trigger the pipeline if your provider is GitHub and you created your pipeline with a personal access token. Info As of AWS Copilot v1.4.0, the access token is no longer needed for GitHub repository sources. Instead, Copilot will trigger the pipeline using AWS CodeStar connections . source.properties. branch String The name of the branch in your repository that triggers the pipeline. The default for GitHub is main ; the default for Bitbucket and CodeCommit is master . source.properties. repository String The URL of your repository. source.properties. connection_name String The name of an existing CodeStar Connections connection. If omitted, Copilot will generate a connection for you. build Map Configuration for CodeBuild project. build. image String The URI that identifies the Docker image to use for this build project. As of now, aws/codebuild/amazonlinux2-x86_64-standard:3.0 is used by default. stages Array of Maps Ordered list of environments that your pipeline will deploy to. stages. name String The name of an environment to deploy your services to. stages. requires_approval Boolean Indicates whether to add a manual approval step before the deployment. stages. test_commands Array of Strings Commands to run integration or end-to-end tests after deployment.","title":"Pipeline"},{"location":"docs/manifest/rd-web-service","text":"List of all available properties for a 'Request-Driven Web Service' manifest. Sample manifest for a frontend service # Your service name will be used in naming your resources like log groups, App Runner services, etc. name : frontend # The \"architecture\" of the service you're running. type : Request-Driven Web Service http : healthcheck : path : '/_healthcheck' healthy_threshold : 3 unhealthy_threshold : 5 interval : 10s timeout : 5s # Configuration for your containers and service. image : build : ./frontend/Dockerfile port : 80 cpu : 1024 memory : 2048 variables : LOG_LEVEL : info tags : owner : frontend-team environments : test : LOG_LEVEL : debug name String The name of your service. type String The architecture type for your service. A Request-Driven Web Service is an internet-facing service that is deployed on AWS App Runner. http Map The http section contains parameters related to integrating your service with an Application Load Balancer. http. path String Requests to this path will be forwarded to your service. Each Request-Driven Web Service should listen on a unique path. http. healthcheck String or Map If you specify a string, Copilot interprets it as the path exposed in your container to handle target group health check requests. The default is \"/\". http : healthcheck : '/' You can also specify healthcheck as a map: http : healthcheck : path : '/' healthy_threshold : 3 unhealthy_threshold : 2 interval : 15s timeout : 10s http.healthcheck. healthy_threshold Integer The number of consecutive health check successes required before considering an unhealthy target healthy. The default is 3. Range: 1-20. http.healthcheck. unhealthy_threshold Integer The number of consecutive health check failures required before considering a target unhealthy. The default is 3. Range: 1-20. http.healthcheck. interval Duration The approximate amount of time, in seconds, between health checks of an individual target. The default is 5s. Range: 1s\u201320s. http.healthcheck. timeout Duration The amount of time, in seconds, during which no response from a target means a failed health check. The default is 2s. Range 1s-20s. image Map The image section contains parameters relating to the Docker build configuration and exposed port. image. build String or Map If you specify a string, Copilot interprets it as the path to your Dockerfile. It will assume that the dirname of the string you specify should be the build context. The manifest: image : build : path/to/dockerfile will result in the following call to docker build: $ docker build --file path/to/dockerfile path/to You can also specify build as a map: image : build : dockerfile : path/to/dockerfile context : context/dir target : build-stage cache_from : - image:tag args : key : value In this case, Copilot will use the context directory you specified and convert the key-value pairs under args to --build-arg overrides. The equivalent docker build call will be: $ docker build --file path/to/dockerfile --target build-stage --cache-from image:tag --build-arg key=value context/dir . You can omit fields and Copilot will do its best to understand what you mean. For example, if you specify context but not dockerfile , Copilot will run Docker in the context directory and assume that your Dockerfile is named \"Dockerfile.\" If you specify dockerfile but no context , Copilot assumes you want to run Docker in the directory that contains dockerfile . All paths are relative to your workspace root. image. location String Instead of building a container from a Dockerfile, you can specify an existing image name. Mutually exclusive with image.build . The location field follows the same definition as the image parameter in the Amazon ECS task definition. image. port Integer The port exposed in your Dockerfile. Copilot should parse this value for you from your EXPOSE instruction. image. labels Map An optional key/value map of Docker labels to add to the container. cpu Integer Number of CPU units reserved for each instance of your service. See the AWS App Runner docs for valid CPU values. memory Integer Amount of memory in MiB reserved for each instance of your service. See the AWS App Runner docs for valid memory values. variables Map Key-value pairs that represent environment variables that will be passed to your service. Copilot will include a number of environment variables by default for you. environments Map The environment section lets you override any value in your manifest based on the environment you're in. In the example manifest above, we're overriding the count parameter so that we can run 2 copies of our service in our prod environment.","title":"Request-Driven Web Service"},{"location":"docs/manifest/scheduled-job","text":"List of all available properties for a 'Scheduled Job' manifest. To learn about Copilot jobs, see the Jobs concept page. Sample manifest for a report generator cronjob # Your job name will be used in naming your resources like log groups, ECS Tasks, etc. name : report-generator type : Scheduled Job on : schedule : @ daily cpu : 256 memory : 512 retries : 3 timeout : 1h image : # Path to your service's Dockerfile. build : ./Dockerfile variables : LOG_LEVEL : info secrets : GITHUB_TOKEN : GITHUB_TOKEN name String The name of your job. type String The architecture type for your job. Currently, Copilot only supports the \"Scheduled Job\" type for tasks that are triggered either on a fixed schedule or periodically. on Map The configuration for the event that triggers your job. on. schedule String You can specify a rate to periodically trigger your job. Supported rates: \"@yearly\" \"@monthly\" \"@weekly\" \"@daily\" \"@hourly\" \"@every {duration}\" (For example, \"1m\", \"5m\") \"rate({duration})\" based on CloudWatch's rate expressions Alternatively, you can specify a cron schedule if you'd like to trigger the job at a specific time: \"* * * * *\" based on the standard cron format . \"cron({fields})\" based on CloudWatch's cron expressions with six fields. image Map The image section contains parameters relating to the Docker build configuration. image. build String or Map If you specify a string, Copilot interprets it as the path to your Dockerfile. It will assume that the dirname of the string you specify should be the build context. The manifest: image : build : path/to/dockerfile will result in the following call to docker build: $ docker build --file path/to/dockerfile path/to You can also specify build as a map: image : build : dockerfile : path/to/dockerfile context : context/dir target : build-stage cache_from : - image:tag args : key : value In this case, Copilot will use the context directory you specified and convert the key-value pairs under args to --build-arg overrides. The equivalent docker build call will be: $ docker build --file path/to/dockerfile --target build-stage --cache-from image:tag --build-arg key=value context/dir . You can omit fields and Copilot will do its best to understand what you mean. For example, if you specify context but not dockerfile , Copilot will run Docker in the context directory and assume that your Dockerfile is named \"Dockerfile.\" If you specify dockerfile but no context , Copilot assumes you want to run Docker in the directory that contains dockerfile . All paths are relative to your workspace root. image. location String Instead of building a container from a Dockerfile, you can specify an existing image name. Mutually exclusive with image.build . The location field follows the same definition as the image parameter in the Amazon ECS task definition. image. labels Map An optional key/value map of Docker labels to add to the container. entrypoint String or Array of Strings Override the default entrypoint in the image. # String version. entrypoint : \"/bin/entrypoint --p1 --p2\" # Alteratively, as an array of strings. entrypoint : [ \"/bin/entrypoint\" , \"--p1\" , \"--p2\" ] command String or Array of Strings Override the default command in the image. # String version. command : ps au # Alteratively, as an array of strings. command : [ \"ps\" , \"au\" ] cpu Integer Number of CPU units for the task. See the Amazon ECS docs for valid CPU values. memory Integer Amount of memory in MiB used by the task. See the Amazon ECS docs for valid memory values. retries Integer The number of times to retry the job before failing. timeout Duration How long the job should run before it aborts and fails. You can use the units: h , m , or s . network Map The network section contains parameters for connecting to AWS resources in a VPC. network. vpc Map Subnets and security groups attached to your tasks. network.vpc. placement String Must be one of 'public' or 'private' . Defaults to launching your tasks in public subnets. Info Launching tasks in 'private' subnets that need internet connectivity is only supported if you imported a VPC with NAT Gateways when running copilot env init . See #1959 for tracking NAT Gateways support in Copilot-generated VPCs. network.vpc. security_groups Array of Strings Additional security group IDs associated with your tasks. Copilot always includes a security group so containers within your environment can communicate with each other. variables Map Key-value pairs that represent environment variables that will be passed to your job. Copilot will include a number of environment variables by default for you. secrets Map Key-value pairs that represent secret values from AWS Systems Manager Parameter Store that will be securely passed to your job as environment variables. storage Map The Storage section lets you specify external EFS volumes for your containers and sidecars to mount. This allows you to access persistent storage across regions for data processing or CMS workloads. For more detail, see the storage page. storage. volumes Map Specify the name and configuration of any EFS volumes you would like to attach. The volumes field is specified as a map of the form: volumes : <volume name> : path : \"/etc/mountpath\" efs : ... storage.volumes. volume Map Specify the configuration of a volume. volume. path String Required. Specify the location in the container where you would like your volume to be mounted. Must be fewer than 242 characters and must consist only of the characters a-zA-Z0-9.-_/ . volume. read_only Boolean Optional. Defaults to true . Defines whether the volume is read-only or not. If false, the container is granted elasticfilesystem:ClientWrite permissions to the filesystem and the volume is writable. volume. efs Map Specify more detailed EFS configuration. volume.efs. id String Required. The ID of the filesystem you would like to mount. volume.efs. root_dir String Optional. Defaults to / . Specify the location in the EFS filesystem you would like to use as the root of your volume. Must be fewer than 255 characters and must consist only of the characters a-zA-Z0-9.-_/ . If using an access point, root_dir must be either empty or / and auth.iam must be true . volume.efs. auth Map Specify advanced authorization configuration for EFS. volume.efs.auth. iam Boolean Optional. Defaults to true . Whether or not to use IAM authorization to determine whether the volume is allowed to connect to EFS. volume.efs.auth. access_point_id String Optional. Defaults to \"\" . The ID of the EFS access point to connect to. If using an access point, root_dir must be either empty or / and auth.iam must be true . environments Map The environment section lets you override any value in your manifest based on the environment you're in. In the example manifest above, we're overriding the CPU parameter so that our production container is more performant.","title":"Scheduled Job"}]}